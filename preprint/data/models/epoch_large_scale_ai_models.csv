Model,Domain,Task,Authors,Model accessibility,Link,Citations,Reference,Publication date,Organization,Parameters,Parameters notes,Training compute (FLOP),Training compute notes,Training dataset,Training dataset notes,Training dataset size (datapoints),Dataset size notes,Training time (hours),Training time notes,Training hardware,Confidence,Abstract,Country (of organization),Base model,Finetune compute (FLOP),Finetune compute notes,Hardware quantity,Training code accessibility,Accessibility notes,Organization categorization (from Organization)
Qwen3-235B-A22B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen3/,,"Qwen3: Think Deeper, Act Faster",2025-04-29,Alibaba,235000000000.0,"235 billion total parameters and 22 billion activated parameters

Number of Layers: 94
Number of Attention Heads (GQA): 64 for Q and 4 for KV
Number of Experts: 128
Number of Activated Experts: 8
Context Length: 32,768 natively and 131,072 tokens with YaRN.",4.752e+24,6 FLOP / parameter / token * 22*10^9 active parameters * 36000000000000 tokens = 4.752e+24 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,Likely,"Today, we are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.",China,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-235B-A22B",Industry
Qwen3-30B-A3B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen3/,,"Qwen3: Think Deeper, Act Faster",2025-04-29,Alibaba,30000000000.0,"30 billion total parameters and 3 billion activated parameters

Number of Layers: 48
Number of Attention Heads (GQA): 32 for Q and 4 for KV
Number of Experts: 128
Number of Activated Experts: 8
Context Length: 32,768",6.48e+23,6 FLOP / parameter / token * 3*10^9 active parameters * 36000000000000 tokens = 6.48e+23 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,Likely,"Today, we are excited to announce the release of Qwen3, the latest addition to the Qwen family of large language models. Our flagship model, Qwen3-235B-A22B, achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B, outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.",China,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-30B-A3B",Industry
Qwen3-32B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen3/,,"Qwen3: Think Deeper, Act Faster",2025-04-29,Alibaba,32800000000.0,"Number of Parameters: 32.8B
Number of Paramaters (Non-Embedding): 31.2B
Number of Layers: 64
Number of Attention Heads (GQA): 64 for Q and 8 for KV
Context Length: 32,768 natively and 131,072 tokens with YaRN.",7.0848e+24,6 FLOP / parameter / token * 36 * 10^12 tokens * 32.8  * 10^9 parameters = 7.0848e+24 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,Confident,"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:

Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.
Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.
Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.
Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.
Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.",China,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-32B-Base",Industry
Qwen3-14B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen3/,,"Qwen3: Think Deeper, Act Faster",2025-04-29,Alibaba,14800000000.0,"Number of Parameters: 14.8B
Number of Paramaters (Non-Embedding): 13.2B
Number of Layers: 40
Number of Attention Heads (GQA): 40 for Q and 8 for KV
Context Length: 32,768 natively and 131,072 tokens with YaRN.",3.1968e+24,6 FLOP / parameter / token * 36 * 10^12 tokens * 14.8  * 10^9 parameters = 3.1968e+24 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,Confident,"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:

Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.
Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.
Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.
Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.
Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.",China,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-14B-Base",Industry
Qwen3-8B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen3/,,"Qwen3: Think Deeper, Act Faster",2025-04-29,Alibaba,8200000000.0,"Number of Parameters: 8.2B
Number of Paramaters (Non-Embedding): 6.95B
Number of Layers: 36
Number of Attention Heads (GQA): 32 for Q and 8 for KV
Context Length: 32,768 natively and 131,072 tokens with YaRN.",1.7712e+24,6 FLOP / parameter / token * 36 * 10^12 tokens * 8.2  * 10^9 parameters = 1.7712e+24 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,Confident,"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:

Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.
Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.
Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.
Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.
Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.",China,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-8B-Base",Industry
Qwen3-4B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen3/,,"Qwen3: Think Deeper, Act Faster",2025-04-29,Alibaba,4000000000.0,"Number of Parameters: 4.0B
Number of Paramaters (Non-Embedding): 3.6B
Number of Layers: 36
Number of Attention Heads (GQA): 32 for Q and 8 for KV
Context Length: 32,768 natively and 131,072 tokens with YaRN.",8.64e+23,6 FLOP / parameter / token * 36 * 10^12 tokens * 4  * 10^9 parameters = 8.64e+23 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,Confident,"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:

Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.
Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.
Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.
Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.
Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.",China,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-4B-Base",Industry
Qwen3-1.7B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen3/,,"Qwen3: Think Deeper, Act Faster",2025-04-29,Alibaba,1700000000.0,"Number of Parameters: 1.7B
Number of Paramaters (Non-Embedding): 1.4B
Number of Layers: 28
Number of Attention Heads (GQA): 16 for Q and 8 for KV
Context Length: 32,768
",3.672e+23,6 FLOP / parameter / token * 36 * 10^12 tokens * 1.7  * 10^9 parameters = 3.672e+23 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,Confident,"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:

Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.
Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.
Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.
Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.
Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.",China,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-1.7B-Base",Industry
Qwen3-0.6B,Language,"Language modeling/generation,Question answering,Mathematical reasoning,Quantitative reasoning,Code generation,Translation",,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen3/,,"Qwen3: Think Deeper, Act Faster",2025-04-29,Alibaba,600000000.0,"Number of Parameters: 0.6B
Number of Paramaters (Non-Embedding): 0.44B
Number of Layers: 28
Number of Attention Heads (GQA): 16 for Q and 8 for KV
Context Length: 32,768",1.296e+23,6 FLOP / parameter / token * 36 * 10^12 tokens * 0.6 * 10^9 parameters = 1.296e+23 FLOP,Unspecified unreleased,"""While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets.

The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. """,36000000000000.0,36T,,,,Confident,"Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:

Uniquely support of seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose dialogue) within single model, ensuring optimal performance across various scenarios.
Significantly enhancement in its reasoning capabilities, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.
Superior human preference alignment, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.
Expertise in agent capabilities, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.
Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation.",China,,,,,Unreleased,"Apache 2.0

https://huggingface.co/Qwen/Qwen3-0.6B-Base",Industry
Nemotron-H 8B,Language,"Language modeling/generation,Question answering,Translation,Quantitative reasoning,Code generation","NVIDIA: Aaron Blakeman, Aarti Basant, Abhinav Khattar, Adithya Renduchintala, Akhiad Bercovich, Aleksander Ficek, Alexis Bjorlin, Ali Taghibakhshi, Amala Sanjay Deshmukh, Ameya Sunil Mahabaleshwarkar, Andrew Tao, Anna Shors, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Bobby Chen, Boris Ginsburg, Boxin Wang, Brandon Norick, Brian Butterfield, Bryan Catanzaro, Carlo del Mundo, Chengyu Dong, Christine Harvey, Christopher Parisien, Dan Su, Daniel Korzekwa, Danny Yin, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Denys Fridman, Dima Rekesh, Ding Ma, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Dusan Stosic, Eileen Long, Elad Segal, Ellie Evans, Eric Chung, Erick Galinkin, Evelina Bakhturina, Ewa Dobrowolska, Fei Jia, Fuxiao Liu, Gargi Prasad, Gerald Shen, Guilin Liu, Guo Chen, Haifeng Qian, Helen Ngo, Hongbin Liu, Hui Li, Igor Gitman, Ilia Karmanov, Ivan Moshkov, Izik Golan, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jarno Seppanen, Jason Lu, Jason Sewall, Jiaqi Zeng, Jiaxuan You, Jimmy Zhang, Jing Zhang, Jining Huang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jon Barker, Jonathan Cohen, Joseph Jennings, Jupinder Parmar, Karan Sapra, Kari Briski, Kateryna Chumachenko, Katherine Luna, Keshav Santhanam, Kezhi Kong, Kirthi Sivamani, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Lawrence McAfee, Leon Derczynski, Lindsey Pavao, Luis Vega, Lukas Voegtle, Maciej Bala, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Markus Kliegl et al. (100 additional authors not shown)",Open weights (non-commercial),https://arxiv.org/abs/2504.03624,,Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models,2025-04-14,NVIDIA,8000000000.0,"Model Architecture
Architecture Type: Hybrid Mamba-Transformer
Network Architecture: Nemotron-H
This model has 8B model parameters.",7.2e+23,6 FLOP / parameter / token * 8 * 10^9 parameters * 15 * 10^12 tokens = 7.2e+23 FLOP,"Common Crawl,Unspecified unreleased","""The training corpus for Nemotron-H-8B-Base-8K consists of English and multilingual text (German, Spanish, French, Italian, Korean, Portuguese, Russian, Japanese, Chinese and English), as well as code. Our sources cover a variety of document types such as: webpages, dialogue, articles, and other written materials. This model was also improved using synthetic data from Qwen (Built with Qwen). The corpus spans domains including legal, math, science, finance, and more. We also include a small portion of question-answering, and alignment style data to improve model accuracy.""",15000000000000.0,"""We trained Nemotron-H-8B-Base on a token horizon of 15 trillion tokens""",,,,Confident,"As inference-time scaling becomes critical for enhanced reasoning capabilities, it is increasingly becoming important to build models that are efficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid Mamba-Transformer models designed to reduce inference cost for a given accuracy level. To achieve this goal, we replace the majority of self-attention layers in the common Transformer model architecture with Mamba layers that perform constant computation and require constant memory per generated token. We show that Nemotron-H models offer either better or on-par accuracy compared to other similarly-sized state-of-the-art open-sourced Transformer models (e.g., Qwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3× faster at inference. To further increase inference speed and reduce the memory required at inference time, we created Nemotron-H-47B-Base from the 56B model using a new compression via pruning and distillation technique called MiniPuzzle. Nemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20% faster to infer. In addition, we introduce an FP8-based training recipe and show that it can achieve on par results with BF16-based training. This recipe is used to train the 56B model. We are releasing Nemotron-H base model checkpoints with support in Hugging Face and NeMo.",United States of America,,,,,Unreleased,"https://huggingface.co/nvidia/Nemotron-H-8B-Base-8K
nvidia-internal-scientific-research-and-development-model-license",Industry
Nemotron-H 56B,Language,"Language modeling/generation,Question answering,Translation,Quantitative reasoning,Code generation","NVIDIA: Aaron Blakeman, Aarti Basant, Abhinav Khattar, Adithya Renduchintala, Akhiad Bercovich, Aleksander Ficek, Alexis Bjorlin, Ali Taghibakhshi, Amala Sanjay Deshmukh, Ameya Sunil Mahabaleshwarkar, Andrew Tao, Anna Shors, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Bobby Chen, Boris Ginsburg, Boxin Wang, Brandon Norick, Brian Butterfield, Bryan Catanzaro, Carlo del Mundo, Chengyu Dong, Christine Harvey, Christopher Parisien, Dan Su, Daniel Korzekwa, Danny Yin, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Denys Fridman, Dima Rekesh, Ding Ma, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Dusan Stosic, Eileen Long, Elad Segal, Ellie Evans, Eric Chung, Erick Galinkin, Evelina Bakhturina, Ewa Dobrowolska, Fei Jia, Fuxiao Liu, Gargi Prasad, Gerald Shen, Guilin Liu, Guo Chen, Haifeng Qian, Helen Ngo, Hongbin Liu, Hui Li, Igor Gitman, Ilia Karmanov, Ivan Moshkov, Izik Golan, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jarno Seppanen, Jason Lu, Jason Sewall, Jiaqi Zeng, Jiaxuan You, Jimmy Zhang, Jing Zhang, Jining Huang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jon Barker, Jonathan Cohen, Joseph Jennings, Jupinder Parmar, Karan Sapra, Kari Briski, Kateryna Chumachenko, Katherine Luna, Keshav Santhanam, Kezhi Kong, Kirthi Sivamani, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Lawrence McAfee, Leon Derczynski, Lindsey Pavao, Luis Vega, Lukas Voegtle, Maciej Bala, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Markus Kliegl et al. (100 additional authors not shown)",Open weights (non-commercial),https://arxiv.org/abs/2504.03624,,Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models,2025-04-14,NVIDIA,56000000000.0,"Model Architecture
Architecture Type: Hybrid Mamba-Transformer
Network Architecture: Nemotron-H
This model has 56B model parameters.",6.72e+24,6 FLOP / parameter / token * 56 * 10^9 parameters * 20 * 10^12 tokens = 6.72e+24 FLOP,"Common Crawl,Unspecified unreleased",,20000000000000.0,"""We trained Nemotron-H-8B-Base on a token horizon of 15 trillion tokens and Nemotron-H-56B-Base on a token horizon of 20 trillion tokens. We used a sequence length of 8192 and global batch size of
768 (6291456 tokens per batch).""",,,NVIDIA H100 SXM5 80GB,Confident,"As inference-time scaling becomes critical for enhanced reasoning capabilities, it is increasingly becoming important to build models that are efficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid Mamba-Transformer models designed to reduce inference cost for a given accuracy level. To achieve this goal, we replace the majority of self-attention layers in the common Transformer model architecture with Mamba layers that perform constant computation and require constant memory per generated token. We show that Nemotron-H models offer either better or on-par accuracy compared to other similarly-sized state-of-the-art open-sourced Transformer models (e.g., Qwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3× faster at inference. To further increase inference speed and reduce the memory required at inference time, we created Nemotron-H-47B-Base from the 56B model using a new compression via pruning and distillation technique called MiniPuzzle. Nemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20% faster to infer. In addition, we introduce an FP8-based training recipe and show that it can achieve on par results with BF16-based training. This recipe is used to train the 56B model. We are releasing Nemotron-H base model checkpoints with support in Hugging Face and NeMo.",United States of America,,,,6144.0,Unreleased,"https://huggingface.co/nvidia/Nemotron-H-56B-Base-8K
nvidia-internal-scientific-research-and-development-model-license",Industry
Pangu Ultra,Language,"Code generation,Language modeling/generation","Yichun Yin, Wenyong Huang, Kaikai Song, Yehui Tang, Xueyu Wu, Wei Guo, Peng Guo, Yaoyuan Wang, Xiaojun Meng, Yasheng Wang, Dong Li, Can Chen, Dandan Tu, Yin Li, Fisher Yu, Ruiming Tang, Yunhe Wang, Baojun Wang, Bin Wang, Bo Wang, Boxiao Liu, Changzheng Zhang, Duyu Tang, Fei Mi, Hui Jin, Jiansheng Wei, Jiarui Qin, Jinpeng Li, Jun Zhao, Liqun Deng, Lin Li, Minghui Xu, Naifu Zhang, Nianzu Zheng, Qiang Li, Rongju Ruan, Shengjun Cheng, Tianyu Guo, Wei He, Wei Li, Weiwen Liu, Wulong Liu, Xinyi Dai, Yonghan Dong, Yu Pan, Yue Li, Yufei Wang, Yujun Li, Yunsheng Ni, Zhe Liu, Zhenhe Zhang, Zhicheng Liu",,https://arxiv.org/abs/2504.07866,0.0,"Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs
",2025-04-10,Huawei,135000000000.0,,1.0692e+25,"When compared to Llama 3.1 405B, Pangu Ultra achieves better scores on most of the challenging benchmarks, while utilizing only about 29% of the training FLOPs required by Llama 405B.

6*135000000000*13200000000000=1.069200e+25",,,13200000000000.0, 13.2 trillion tokens,,,Huawei Ascend 910B,Confident,"We present Pangu Ultra, a Large Language Model (LLM) with 135 billion parameters and dense Transformer modules trained on Ascend Neural Processing Units (NPUs). Although the field of LLM has been witnessing unprecedented advances in pushing the scale and capability of LLM in recent years, training such a large-scale model still involves significant optimization and system challenges. To stabilize the training process, we propose depth-scaled sandwich normalization, which effectively eliminates loss spikes during the training process of deep models. We pre-train our model on 13.2 trillion diverse and high-quality tokens and further enhance its reasoning capabilities during post-training. To perform such large-scale training efficiently, we utilize 8,192 Ascend NPUs with a series of system optimizations. Evaluations on multiple diverse benchmarks indicate that Pangu Ultra significantly advances the state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral Large 2, and even achieves competitive results with DeepSeek-R1, whose sparse model structure contains much more parameters. Our exploration demonstrates that Ascend NPUs are capable of efficiently and effectively training dense models with more than 100 billion parameters. Our model and system will be available for our commercial customers.",China,,,,8192.0,,,Industry
Llama 4 Scout,"Multimodal,Language","Chat,Code generation",,Open weights (restricted use),https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation,2025-04-05,Meta AI,109000000000.0,"""Our smaller model, Llama 4 Scout, is a general purpose model with 17 billion active parameters, 16 experts, and 109 billion total parameters that delivers state-of-the-art performance for its class.""",4.08e+24,"40T training tokens per model card:

https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md  

6 FLOP per token per parameter * 17B active parameters * 40T tokens = 4.08e24 FLOP


whereas Behemoth's training dataset is at least 30T tokens:

https://ai.meta.com/blog/llama-4-multimodal-intelligence/ ",,,30000000000000.0,"""The overall data mixture for training consisted of more than 30 trillion tokens, which is more than double the Llama 3 pre-training mixture and includes diverse text, image, and video datasets.""",,,,Likely,"We’re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.
Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks.
Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding—at less than half the active parameters. Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on LMArena.
These models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world’s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we’re excited to share more details about it even while it’s still in flight.
Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",United States of America,,,,,,Llama 4 license: https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE ,Industry
Llama 4 Maverick,"Multimodal,Language","Chat,Code generation",,Open weights (restricted use),https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation,2025-04-05,Meta AI,400000000000.0,"""Llama 4 Maverick models have 17B active parameters and 400B total parameters.""

https://ai.meta.com/blog/llama-4-multimodal-intelligence/",2.244000000001e+24,"22T training tokens per model card:

https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md   

Maverick was trained using co-distillation from Llama 4 Behemoth. It isn't 100% clear that all 22T tokens used distillation, but we assume this for the time being.

Direct compute = 6 FLOP per token per parameter * 17B active parameters * 22T tokens = 2.244e24 FLOP",,,30000000000000.0,"""The overall data mixture for training consisted of more than 30 trillion tokens, which is more than double the Llama 3 pre-training mixture and includes diverse text, image, and video datasets.""",,,,Likely,"We’re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.
Llama 4 Scout, a 17 billion active parameter model with 16 experts, is the best multimodal model in the world in its class and is more powerful than all previous generation Llama models, while fitting in a single NVIDIA H100 GPU. Additionally, Llama 4 Scout offers an industry-leading context window of 10M and delivers better results than Gemma 3, Gemini 2.0 Flash-Lite, and Mistral 3.1 across a broad range of widely reported benchmarks.
Llama 4 Maverick, a 17 billion active parameter model with 128 experts, is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and coding—at less than half the active parameters. Llama 4 Maverick offers a best-in-class performance to cost ratio with an experimental chat version scoring ELO of 1417 on LMArena.
These models are our best yet thanks to distillation from Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world’s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we’re excited to share more details about it even while it’s still in flight.
Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",United States of America,,,,,,Llama 4 license: https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE ,Industry
Llama 4 Behemoth,"Multimodal,Language","Chat,Code generation",,,https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation,2025-04-05,Meta AI,2000000000000.0,"""Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world’s smartest LLMs.""",5.18400000000001e+25,"Behemoth's training dataset is at least 30T tokens:

https://ai.meta.com/blog/llama-4-multimodal-intelligence/ ",,,30000000000000.0,"""The overall data mixture for training consisted of more than 30 trillion tokens, which is more than double the Llama 3 pre-training mixture and includes diverse text, image, and video datasets.""",,,,Likely,"We’re sharing the first models in the Llama 4 herd, which will enable people to build more personalized multimodal experiences.
...
Llama 4 Behemoth, a 288 billion active parameter model with 16 experts that is our most powerful yet and among the world’s smartest LLMs. Llama 4 Behemoth outperforms GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro on several STEM benchmarks. Llama 4 Behemoth is still training, and we’re excited to share more details about it even while it’s still in flight.
Download the Llama 4 Scout and Llama 4 Maverick models today on llama.com and Hugging Face. Try Meta AI built with Llama 4 in WhatsApp, Messenger, Instagram Direct, and on the web.",United States of America,,,,,,,Industry
GPT-4.5,"Language,Vision,Multimodal","Language modeling/generation,Question answering,Quantitative reasoning,Translation,Visual question answering,Code generation,Instruction interpretation","Foundational contributors
Alex Paino, Ali Kamali, Amin Tootoonchian, Andrew Tulloch, Ben Sokolowsky, Clemens Winter, Colin Wei, Daniel Kappler, Daniel Levy, Felipe Petroski Such, Geoff Salmon, Ian O’Connell, Jason Teplitz, Kai Chen, Nik Tezak, Prafulla Dhariwal, Rapha Gontijo Lopes, Sam Schoenholz, Youlong Cheng, Yujia Jin, Yunxing Dai

Research
Core contributors

Aiden Low, Alec Radford, Alex Carney, Alex Nichol, Alexis Conneau, Ananya Kumar, Ben Wang, Charlotte Cole , Elizabeth Yang, Gabriel Goh, Hadi Salman, Haitang Hu, Heewoo Jun, Ian Sohl, Ishaan Gulrajani, Jacob Coxon, James Betker, Jamie Kiros, Jessica Landon, Kyle Luther, Lia Guy, Lukas Kondraciuk, Lyric Doshi, Mikhail Pavlov, Qiming Yuan, Reimar Leike, Rowan Zellers, Sean Metzger, Shengjia Zhao, Spencer Papay, Tao Wang

Contributors

Adam Lerer, Aidan McLaughlin, Alexander Prokofiev, Alexandra Barr, Allan Jabri, Ananya Kumar, Andrew Gibiansky, Andrew Schmidt, Casey Chu, Chak Li, Chelsea Voss, Chris Hallacy, Chris Koch, Christine McLeavey, David Mely, Dimitris Tsipras, Eric Sigler, Erin Kavanaugh, Farzad Khorasani, Huiwen Chang, Ilya Kostrikov, Ishaan Singal, Ji Lin, Jiahui Yu, Jing Yu Zhang, John Rizzo, Jong Wook Kim, Joyce Lee, Juntang Zhuang, Leo Liu, Li Jing, Long Ouyang, Louis Feuvrier, Mo Bavarian, Nick Stathas, Nitish Keskar, Oleg Murk, Preston Bowman, Scottie Yan, SQ Mah, Tao Xu, Taylor Gordon, Valerie Qi, Wenda Zhou, Yu Zhang

Scaling
Core contributors

Adam Goucher, Alex Chow, Alex Renzin, Aleksandra Spyra, Avi Nayak, Ben Leimberger, Christopher Hesse, Duc Phong Nguyen, Dinghua Li, Eric Peterson, Francis Zhang, Gene Oden, Kai Fricke, Kai Hayashi, Larry Lv, Leqi Zou, Lin Yang, Madeleine Thompson, Michael Petrov, Miguel Castro, Natalia Gimelshein, Phil Tillet, Reza Zamani, Ryan Cheu Stanley Hsieh, Steve Lee, Stewart Hall, Thomas Raoux, Tianhao Zheng, Vishal Kuo, Yongjik Kim, Yuchen Zhang, Zhuoran Liu

Contributors

Alvin Wan, Andrew Cann, Antoine Pelisse, Anuj Kalia, Aaron Hurst, Avital Oliver, Brad Barnes, Brian Hsu, Chen Ding, Chen Shen, Cheng Chang, Christian Gibson, Duncan Findlay, Fan Wang, Fangyuan Li, Gianluca Borello, Heather Schmidt, Henrique Ponde de Oliveira Pinto, Ikai Lan, Jiayi Weng, James Crooks, Jos Kraaijeveld, Junru Shao, Kenny Hsu, Kenny Nguyen, Kevin King, Leah Burkhardt, Leo Chen, Linden Li, Lu Zhang, Mahmoud Eariby, Marat Dukhan, Mateusz Litwin, Miki Habryn, Natan LaFontaine, Pavel Belov, Peng Su, Prasad Chakka, Rachel Lim, Rajkumar Samuel, Renaud Gaubert, Rory Carmichael, Sarah Dong, Shantanu Jain, Stephen Logsdon, Todd Underwood, Weixing Zhang, Will Sheu, Weiyi Zheng, Yinghai Lu, Yunqiao Zhang

Safety Systems
Andrea Vallone, Andy Applebaum, Cameron Raymond, Chong Zhang, Dan Mossing, Elizabeth Proehl, Eric Wallace, Evan Mays, Grace Zhao, Ian Kivlichan, Irina Kofman, Joel Parish, Kevin Liu, Keren Gu-Lemberg, Kristen Ying, Lama Ahmad, Lilian Weng , Leon Maksin, Leyton Ho, Meghan Shah, Michael Lampe, Michele Wang, Miles Wang, Olivia Watkins, Phillip Guo, Samuel Miserendino, Sam Toizer, Sandhini Agarwal, Tejal Patwardhan, Tom Dupré la Tour, Tong Mu, Tyna Eloundou, Yunyun Wang

Deployment
Adam Brandon, Adam Perelman, Adele Li, Akshay Nathan, Alan Hayes, Alfred Xue, Alison Ben, Alec Gorge, Alex Guziel, Alex Iftimie, Ally Bennett, Andrew Chen, Andy Wang, Andy Wood, Angad Singh, Anoop Kotha, Antonia Woodford, Anuj Saharan, Ashley Tyra, Atty Eleti, Ben Schneider, Bessie Ji, Beth Hoover, Bill Chen, Blake Samic, Britney Smith, Brian Yu, Caleb Wang, Cary Bassin, Cary Hudson, Charlie Jatt, Chengdu Huang, Chris Beaumont, Christina Huang, Cristina Scheau, Dana Palmie, Daniel Levine, Daryl Neubieser, Dave Cummings, David Sasaki, Dibya Bhattacharjee, Dylan Hunn, Edwin Arbus, Elaine Ya Le, Enis Sert, Eric Kramer, Fred von Lohmann, Gaby Janatpour, Garrett McGrath, Garrett Ollinger, Gary Yang, Hao Sheng, Harold Hotelling, Janardhanan Vembunarayanan, Jeff Harris, Jeffrey Sabin Matsumoto, Jennifer Robinson, Jessica Liang, Jessica Shieh, Jiacheng Yang, Joel Morris, Joseph Florencio, Josh Kaplan, Kan Wu, Karan Sharma, Karen Li, Katie Pypes, Kendal Simon, Kendra Rimbach, Kevin Park, Kevin Rao, Laurance Fauconnet, Lauren Workman, Leher Pathak, Liang Wu, Liang Xiong, Lien Mamitsuka, Lindsay McCallum, Lukas Gross, Manoli Liodakis, Matt Nichols, Michelle Fradin, Minal Khan, Mingxuan Wang, Nacho Soto, Natalie Staudacher, Nikunj Handa, Niko Felix, Ning Liu, Olivier Godement, Oona Gleeson, Philip Pronin, Raymond Li, Reah Miyara, Rohan Nuttall, R.J. Marsan, Sara Culver, Scott Ethersmith, Sean Fitzgerald, Shamez Hemani, Sherwin Wu, Shiao Lee, Shuyang Cheng, Siyuan Fu, Spug Golden, Steve Coffey, Steven Heidel, Sundeep Tirumalareddy, Tabarak Khan, Thomas Degry, Thomas Dimson, Tom Stasi, Tomo Hiratsuka, Trevor Creech, Uzair Navid Iftikhar, Victoria Chernova, Victoria Spiegel, Wanning Jiang, Wenlei Xie, Yaming Lin, Yara Khakbaz, Yilei Qian, Yilong Qin, Yo Shavit, Zhi Bie

Executive Leadership
Bob McGrew, Greg Brockman, Hannah Wong, Jakub Pachocki, Johannes Heidecke, Joanne Jang, Kate Rouch, Kevin Weil, Lauren Itow, Liam Fedus, Mark Chen, Mia Glaese, Mira Murati, Nick Ryder, Sam Altman, Srinivas Narayanan, Tal Broda",API access,https://openai.com/index/introducing-gpt-4-5/,,Introducing GPT-4.5,2025-02-27,OpenAI,,,,,Unspecified unreleased,"""GPT-4.5 was pre-trained and post-trained on diverse datasets, including a mix of publicly available
data, proprietary data from data partnerships, and custom datasets developed in-house, which
collectively contribute to the model’s robust conversational capabilities and world knowledge.""",,,,,,Unknown,"We advance AI capabilities by scaling two complementary paradigms: unsupervised learning and reasoning. These represent two axes of intelligence.

Unsupervised learning increases world model accuracy and intuition. Models like GPT‑3.5, GPT‑4, and GPT‑4.5 advance this paradigm.
Scaling reasoning⁠, on the other hand, teaches models to think and produce a chain of thought before they respond, allowing them to tackle complex STEM or logic problems. Models like OpenAI o1 and OpenAI o3‑mini advance this paradigm.
GPT‑4.5 is an example of scaling unsupervised learning by scaling up compute and data, along with architecture and optimization innovations. GPT‑4.5 was trained on Microsoft Azure AI supercomputers. The result is a model that has broader knowledge and a deeper understanding of the world, leading to reduced hallucinations and more reliability across a wide range of topics.",United States of America,,,,,Unreleased,,Industry
Claude 3.7 Sonnet,"Language,Vision,Multimodal","Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Translation,Instruction interpretation,Visual question answering",,API access,https://www.anthropic.com/news/claude-3-7-sonnet,,Claude 3.7 Sonnet,2025-02-24,Anthropic,,,3.3499999999999998e+25,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,Unspecified unreleased,"""Claude 3.7 Sonnet is trained on a proprietary mix of publicly available information on the Internet, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we
generate internally. While trained on publicly available information on the internet through November 2024, Claude 3.7 Sonnet’s knowledge cut-off date is the end of October 2024. This means the model’s knowledge base is most extensive and reliable on information and events up to October 2024.""",,,,,,Likely,"Today, we’re announcing Claude 3.7 Sonnet1, our most intelligent model to date and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. API users also have fine-grained control over how long the model can think for.

Claude 3.7 Sonnet shows particularly strong improvements in coding and front-end web development. ",United States of America,,,,,Unreleased,,Industry
Evo 2 40B,Biology,Protein or nucleotide language model (pLM/nLM),"Garyk Brixi, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, Samuel H. King, David B. Li, Aditi T. Merchant, Mohsen Naghipourfar, Eric Nguyen, Chiara Ricci-Tam, David W. Romero, Gwanggyu Sun, Ali Taghibakshi, Anton Vorontsov, Brandon Yang, Myra Deng, Liv Gorton, Nam Nguyen, Nicholas K. Wang, Etowah Adams, Stephen A. Baccus, Steven Dillmann, Stefano Ermon, Daniel Guo, Rajesh Ilango, Ken Janik, Amy X. Lu, Reshma Mehta, Mohammad R.K. Mofrad, Madelena Y. Ng, Jaspreet Pannu, Christopher Ré, Jonathan C. Schmok, John St. John, Jeremy Sullivan, Kevin Zhu, Greg Zynda, Daniel Balsam, Patrick Collison, Anthony B. Costa, Tina Hernandez-Boussard, Eric Ho, Ming-Yu Liu, Thomas McGrath, Kimberly Powell, Dave P. Burke, Hani Goodarzi, Patrick D. Hsu, Brian L. Hie",Open weights (unrestricted),https://arcinstitute.org/manuscripts/Evo2,,Genome modeling and design across all domains of life with Evo 2,2025-02-19,"Arc Institute,Stanford University,NVIDIA,Liquid,University of California (UC) Berkeley,Goodfire,Columbia University,University of California San Francisco",40300000000.0,Table 1 lists 40.3B paramters as model size.,2.25e+24,"40.3e9 parameters * 9.3e12 training datapoints * 6 = 2.25e24.
Same FLOPS estimate given by authors in Table 1.",OpenGenome 2,,9300000000000.0,"""We trained two versions of Evo 2: a smaller version at 7B parameters trained on 2.4 trillion tokens and a full version at 40B parameters trained on 9.3 trillion tokens.""",,,,Unverified,"All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.","United States of America,United States of America,United States of America,United States of America,United States of America,United States of America",,,,,,,"Academia,Industry,Industry,Academia,Academia,Academia"
Evo 2 7B,Biology,Protein or nucleotide language model (pLM/nLM),"Garyk Brixi, Matthew G. Durrant, Jerome Ku, Michael Poli, Greg Brockman, Daniel Chang, Gabriel A. Gonzalez, Samuel H. King, David B. Li, Aditi T. Merchant, Mohsen Naghipourfar, Eric Nguyen, Chiara Ricci-Tam, David W. Romero, Gwanggyu Sun, Ali Taghibakshi, Anton Vorontsov, Brandon Yang, Myra Deng, Liv Gorton, Nam Nguyen, Nicholas K. Wang, Etowah Adams, Stephen A. Baccus, Steven Dillmann, Stefano Ermon, Daniel Guo, Rajesh Ilango, Ken Janik, Amy X. Lu, Reshma Mehta, Mohammad R.K. Mofrad, Madelena Y. Ng, Jaspreet Pannu, Christopher Ré, Jonathan C. Schmok, John St. John, Jeremy Sullivan, Kevin Zhu, Greg Zynda, Daniel Balsam, Patrick Collison, Anthony B. Costa, Tina Hernandez-Boussard, Eric Ho, Ming-Yu Liu, Thomas McGrath, Kimberly Powell, Dave P. Burke, Hani Goodarzi, Patrick D. Hsu, Brian L. Hie",Open weights (unrestricted),https://arcinstitute.org/manuscripts/Evo2,,Genome modeling and design across all domains of life with Evo 2,2025-02-19,"Arc Institute,Stanford University,NVIDIA,Liquid,University of California (UC) Berkeley,Goodfire,Columbia University,University of California San Francisco",7000000000.0,,1.008e+23,7e9 parameters *2.4e12 training datapoints*6=1.008e23,OpenGenome 2,,2400000000000.0,"""We trained two versions of Evo 2: a smaller version at 7B parameters trained on 2.4 trillion tokens and a full version at 40B parameters trained on 9.3 trillion tokens.""",,,,Unverified,"All of life encodes information with DNA. While tools for sequencing, synthesis, and editing of genomic code have transformed biological research, intelligently composing new biological systems would also require a deep understanding of the immense complexity encoded by genomes. We introduce Evo 2, a biological foundation model trained on 9.3 trillion DNA base pairs from a highly curated genomic atlas spanning all domains of life. We train Evo 2 with 7B and 40B parameters to have an unprecedented 1 million token context window with single-nucleotide resolution. Evo 2 learns from DNA sequence alone to accurately predict the functional impacts of genetic variation—from noncoding pathogenic mutations to clinically significant BRCA1 variants—without task-specific finetuning. Applying mechanistic interpretability analyses, we reveal that Evo 2 autonomously learns a breadth of biological features, including exon–intron boundaries, transcription factor binding sites, protein structural elements, and prophage genomic regions. Beyond its predictive capabilities, Evo 2 generates mitochondrial, prokaryotic, and eukaryotic sequences at genome scale with greater naturalness and coherence than previous methods. Guiding Evo 2 via inference-time search enables controllable generation of epigenomic structure, for which we demonstrate the first inference-time scaling results in biology. We make Evo 2 fully open, including model parameters, training code, inference code, and the OpenGenome2 dataset, to accelerate the exploration and design of biological complexity.","United States of America,United States of America,United States of America,United States of America,United States of America,United States of America",,,,,,,"Academia,Industry,Industry,Academia,Academia,Academia"
Grok-3,"Language,Vision,Multimodal","Chat,Language modeling/generation,Question answering,Code generation,Visual question answering",,Hosted access (no API),https://x.ai/blog/grok-3,,Grok 3 Beta — The Age of Reasoning Agents,2025-02-17,xAI,,,4.64e+26,"Estimate based on training time for a cluster of 100,000 H100s, and xAI's statement that Grok 2 was trained on more compute than GPT-4 (2.1e25) and that Grok 3 was trained on around 15 times more compute than Grok 2. 

Full estimate here: https://docs.google.com/document/d/1C_dABuZrAqYE_ui4_GZ4bRLtq3TBjIGoBSktaPElhEU/edit?usp=sharing",Unspecified unreleased,,,,2400.0,Estimated to be between 3 and 4 months. We use 100 days in our estimate,NVIDIA H100 SXM5 80GB,Confident,,United States of America,,,,100000.0,Unreleased,,Industry
o3-mini,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation","Training
Brian Zhang, Eric Mitchell, Hongyu Ren, Kevin Lu, Max Schwarzer, Michelle Pokrass, Shengjia Zhao, Ted Sanders

Eval
Adam Kalai, Alex Tachard Passos, Ben Sokolowsky, Elaine Ya Le, Erik Ritter, Hao Sheng, Hanson Wang, Ilya Kostrikov, James Lee, Johannes Ferstad, Michael Lampe, Prashanth Radhakrishnan, Sean Fitzgerald, Sebastien Bubeck, Yann Dubois, Yu Bai

Frontier Evals & Preparedness
Andy Applebaum, Elizabeth Proehl, Evan Mays, Joel Parish, Kevin Liu, Leon Maksin, Leyton Ho, Miles Wang, Michele Wang, Olivia Watkins, Patrick Chao, Samuel Miserendino, Tejal Patwardhan

Engineering
Adam Walker, Akshay Nathan, Alyssa Huang, Andy Wang, Ankit Gohel, Ben Eggers, Brian Yu, Bryan Ashley, Chengdu Huang, Christian Hoareau, Davin Bogan, Emily Sokolova, Eric Horacek, Eric Jiang, Felipe Petroski Such, Jonah Cohen, Josh Gross, Justin Becker, Kan Wu, Kevin Whinnery, Larry Lv, Lee Byron, Manoli Liodakis, Max Johnson, Mike Trpcic, Murat Yesildal, Rasmus Rygaard, RJ Marsan, Rohit Ramchandani, Rohan Kshirsagar, Roman Huet, Sara Conlon, Shuaiqi (Tony) Xia, Siyuan Fu, Srinivas Narayanan, Sulman Choudhry, Tomer Kaftan, Trevor Creech

Search
Adam Fry, Adam Perelman, Brandon Wang, Cristina Scheau, Philip Pronin, Sundeep Tirumalareddy, Will Ellsworth, Zewei Chu

Product
Antonia Woodford, Beth Hoover, Jake Brill, Kelly Stirman, Minnia Feng, Neel Ajjarapu, Nick Turley, Nikunj Handa, Olivier Godement

Safety
Andrea Vallone, Andrew Duberstein, Enis Sert, Eric Wallace, Grace Zhao, Irina Kofman, Jieqi Yu, Joaquin Quinonero Candela, Madelaine Boyd, Mehmet Yatbaz, Mike McClay, Mingxuan Wang, Saachi Jain, Sandhini Agarwal, Sam Toizer, Santiago Hernández, Steve Mostovoy, Young Cha, Tao Li, Yunyun Wang

External Redteaming
Lama Ahmad, Troy Peterson


Research Program Managers
Carpus Chang, Kristen Ying

Leadership
Aidan Clark, Dane Stuckey, Jerry Tworek, Jakub Pachocki, Johannes Heidecke, Kevin Weil, Liam Fedus, Mark Chen, Sam Altman, Wojciech Zaremba",API access,https://openai.com/index/openai-o3-mini/,,Pushing the frontier of cost-effective reasoning.,2025-01-31,OpenAI,,,,,Unspecified unreleased,,,,,,,Unknown,"We’re releasing OpenAI o3-mini, the newest, most cost-efficient model in our reasoning series, available in both ChatGPT and the API today. Previewed in December 2024⁠, this powerful and fast model advances the boundaries of what small models can achieve, delivering exceptional STEM capabilities—with particular strength in science, math, and coding—all while maintaining the low cost and reduced latency of OpenAI o1-mini.

OpenAI o3-mini is our first small reasoning model that supports highly requested developer features including function calling⁠(opens in a new window), Structured Outputs⁠(opens in a new window), and developer messages⁠(opens in a new window), making it production-ready out of the gate. Like OpenAI o1-mini and OpenAI o1-preview, o3-mini will support streaming⁠(opens in a new window). Also, developers can choose between three reasoning effort⁠(opens in a new window) options—low, medium, and high—to optimize for their specific use cases. This flexibility allows o3-mini to “think harder” when tackling complex challenges or prioritize speed when latency is a concern. o3-mini does not support vision capabilities, so developers should continue using OpenAI o1 for visual reasoning tasks. o3-mini is rolling out in the Chat Completions API, Assistants API, and Batch API starting today to select developers in API usage tiers 3-5⁠",United States of America,,,,,Unreleased,,Industry
Mistral Small 3,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Translation",,Open weights (unrestricted),https://mistral.ai/news/mistral-small-3/,,"Mistral Small 3, a latency-optimized 24B-parameter model released under the Apache 2.0 license.",2025-01-30,Mistral AI,24000000000.0,24B,1.152e+24,6ND = 6*8T tokens * 24B parameters = 1.152e+24 FLOP,Unspecified unreleased,"""Notably, Mistral Small 3 was developed without reinforcement learning or synthetic training data, techniques commonly used by competitors. Lample said this “raw” approach helps avoid embedding unwanted biases that could be difficult to detect later.""",8000000000000.0,"8 trillion tokens

Source: https://venturebeat.com/ai/mistral-small-3-brings-open-source-ai-to-the-masses-smaller-faster-and-cheaper/",,,,Confident,"Mistral Small 3 is competitive with larger models such as Llama 3.3 70B or Qwen 32B, and is an excellent open replacement for opaque proprietary models like GPT4o-mini. Mistral Small 3 is on par with Llama 3.3 70B instruct, while being more than 3x faster on the same hardware.

Mistral Small 3 is a pre-trained and instructed model catered to the ‘80%’ of generative AI tasks—those that require robust language and instruction following performance, with very low latency.

We designed this new model to saturate performance at a size suitable for local deployment. Particularly, Mistral Small 3 has far fewer layers than competing models, substantially reducing the time per forward pass. At over 81% accuracy on MMLU and 150 tokens/s latency, Mistral Small is currently the most efficient model of its category.

We’re releasing both a pretrained and instruction-tuned checkpoint under Apache 2.0. The checkpoints can serve as a powerful base for accelerating progress. Note that Mistral Small 3 is neither trained with RL nor synthetic data, so is earlier in the model production pipeline than models like Deepseek R1 (a great and complementary piece of open-source technology!). It can serve as a great base model for building accrued reasoning capacities. We look forward to seeing how the open-source community adopts and customizes it.",France,,,,,Unreleased,"Apache 2.0

https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501",Industry
Qwen2.5-Max,Language,"Language modeling/generation,Question answering,Code generation",Qwen Team,API access,https://qwenlm.github.io/blog/qwen2.5-max/,,Qwen2.5-Max: Exploring the Intelligence of Large-scale MoE Model,2025-01-28,Alibaba,,,,,Unspecified unreleased,,20000000000000.0,"""Concurrently, we are developing Qwen2.5-Max, a large-scale MoE model that has been pretrained on over 20 trillion tokens and further post-trained with curated Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methodologies. """,,,,Unknown,"It is widely recognized that continuously scaling both data size and model size can lead to significant improvements in model intelligence. However, the research and industry community has limited experience in effectively scaling extremely large models, whether they are dense or Mixture-of-Expert (MoE) models. Many critical details regarding this scaling process were only disclosed with the recent release of DeepSeek V3. Concurrently, we are developing Qwen2.5-Max, a large-scale MoE model that has been pretrained on over 20 trillion tokens and further post-trained with curated Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) methodologies. Today, we are excited to share the performance results of Qwen2.5-Max and announce the availability of its API through Alibaba Cloud. We also invite you to explore Qwen2.5-Max on Qwen Chat!",China,,,,,Unreleased,,Industry
Doubao-1.5-pro,Language,Language generation,,Hosted access (no API),https://team.doubao.com/zh/special/doubao_1_5_pro,,Doubao-1.5-pro,2025-01-22,ByteDance,,"Not directly reported. We are told it is a MoE model, and that it matches the performance of a dense model trained on the same data, while using 1/7th of the activated parameters. Additionally they say ""The number of parameters of the Doubao dense model is also much smaller than that of Llama3.1-405B"", which suggests that the number of activated parameters on the forward pass is ""much less"" than 405B/7 = 58B parameters.",,"The model appears to have been trained on 9T tokens; since we believe the MoE model uses ""much less"" than 58B parameters (see parameter notes), training compute is likely to be less than 6 * 9T * 58B = 3.132e24

It is possible the 9T token training run was for comparison sake against the dense model (they label it as ""doubao-MoE"", not doubao-1.5-pro), and that they continued training beyond this. They would need to train for at least 29T tokens to ",,,9000000000000.0,9T tokens,,,,Unknown,The model uses the MoE architecture and explores the ultimate balance between model performance and reasoning performance through integrated training-thinking design. Doubao-1.5-pro can use only a smaller activation parameter to exceed the performance of a first-class super-large pre-training model and achieve excellent results on multiple evaluation benchmarks.,China,,,,,Unreleased,,Industry
"Cosmos-1.0-
Diffusion-14B Video2World","Robotics,Vision,Video","Robotic manipulation,Self-driving car,Video generation","NVIDIA: Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely Klár, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, Wei-Cheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, Artur Zolkowski",Open weights (restricted use),https://arxiv.org/abs/2501.03575,,Cosmos World Foundation Model Platform for Physical AI,2025-01-07,NVIDIA,14000000000.0,14B,6.1554816e+24,"""We train all of the WFM models reported in the paper using a cluster of 10,000 NVIDIA H100 GPUs in a time span of three months.""

989500000000000 FLOP / sec / GPU * 0.4 [assumed utilization] * 10000 GPUs * 3600 sec / hour * 3 months * 30 days / month * 24 hours / day  = 3.0777408e+25 FLOP

(total training compute)

assuming this model is 1/5 of it:

3.0777408e+25 / 5 = 6.1554816e+24 (Likely confidence)",Unspecified unreleased,,9000000000000000.0,"""Suite of first-generation video models trained on 9,000 trillion tokens, including 20 million hours of robotics and driving data - generating high-quality videos from multimodal inputs like images, text, or video."" - https://www.nvidia.com/en-us/ai/cosmos/ ",,"""We train all of the WFM models reported in the paper using a cluster of 10,000 NVIDIA H100 GPUs in a time span of three months.""",NVIDIA H100 SXM5 80GB,Likely,"Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and a digital twin of the world, the world model. In this paper, we present the Cosmos World Foundation Model Platform to help developers build customized world models for their Physical AI setups. We position a world foundation model as a general-purpose world model that can be fine-tuned into customized world models for downstream applications. Our platform covers a video curation pipeline, pre-trained world foundation models, examples of post-training of pre-trained world foundation models, and video tokenizers. To help Physical AI builders solve the most critical problems of our society, we make our platform open-source and our models open-weight with permissive licenses available via this https URL.",United States of America,,,,10000.0,,"NVIDIA Open Model License Agreement
Under the NVIDIA Open Model License, NVIDIA confirms:

Models are commercially usable.
You are free to create and distribute Derivative Models.
NVIDIA does not claim ownership to any outputs generated using the Models or Derivative Models.

Important Note: If you bypass, disable, reduce the efficacy of, or circumvent any technical limitation, safety guardrail or associated safety guardrail hyperparameter, encryption, security, digital rights management, or authentication mechanism contained in the Model, your rights under NVIDIA Open Model License Agreement will automatically terminate.",Industry
OLMo 2 Furious 7B,Language,"Language modeling/generation,Question answering","Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, Hannaneh Hajishirzi",Open weights (unrestricted),https://arxiv.org/abs/2501.00656,,2 OLMo 2 Furious,2024-12-31,"Allen Institute for AI,University of Washington,New York University (NYU)",7000000000.0,7B,1.8e+23,1.8*10^23 FLOPs (Table 6 - developers calculated using 6ND formula),"OLMo-Mix-1124,Dolmino-Mix-1124,Tulu 3",,4000000000000.0,"Pretraining Stage 1
(OLMo-Mix-1124)	4 trillion tokens (= 1 epoch)	

Pretraining Stage 2
(Dolmino-Mix-1124)	50B tokens (3 runs)
merged	

Post-training
(Tulu 3 SFT OLMo mix)	SFT + DPO + PPO
(preference mix)	",,,NVIDIA H100 SXM5 80GB,Confident,"We present OLMo 2, the next generation of our fully open language models. OLMo 2 includes dense autoregressive models with improved architecture and training recipe, pretraining data mixtures, and instruction tuning recipes. Our modified model architecture and training recipe achieve both better training stability and improved per-token efficiency. Our updated pretraining data mixture introduces a new, specialized data mix called Dolmino Mix 1124, which significantly improves model capabilities across many downstream task benchmarks when introduced via late-stage curriculum training (i.e. specialized data during the annealing phase of pretraining). Finally, we incorporate best practices from Tülu 3 to develop OLMo 2-Instruct, focusing on permissive data and extending our final-stage reinforcement learning with verifiable rewards (RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to compute, often matching or outperforming open-weight only models like Llama 3.1 and Qwen 2.5 while using fewer FLOPs and with fully transparent training data, code, and recipe. Our fully open OLMo 2-Instruct models are competitive with or surpassing open-weight only models of comparable size, including Qwen 2.5, Llama 3.1 and Gemma 2. We release all OLMo 2 artifacts openly -- models at 7B and 13B scales, both pretrained and post-trained, including their full training data, training code and recipes, training logs and thousands of intermediate checkpoints. The final instruction model is available on the Ai2 Playground as a free research demo.","United States of America,United States of America,United States of America",,,,,Open source,"apache 2
https://huggingface.co/allenai/OLMo-2-1124-7B
https://github.com/allenai/OLMo","Research collective,Academia,Academia"
OLMo 2 Furious 13B,Language,"Language modeling/generation,Question answering","Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, Hannaneh Hajishirzi",Open weights (unrestricted),https://arxiv.org/abs/2501.00656,,2 OLMo 2 Furious,2024-12-31,"Allen Institute for AI,University of Washington,New York University (NYU)",13000000000.0,13B,4.6e+23,"4.6*10^23 FLOPs (Table 6 - developers calculated using 6ND formula)
","OLMo-Mix-1124,Dolmino-Mix-1124,Tulu 3",,4000000000000.0,"Pretraining Stage 1
(OLMo-Mix-1124)	5 trillion tokens ( = 1.2 epochs)
Pretraining Stage 2
(Dolmino-Mix-1124) 100B tokens (3 runs)
300B tokens (1 run)
merged
Post-training
(Tulu 3 SFT OLMo mix)	SFT + DPO + PPO
(preference mix)	",,,NVIDIA H100 SXM5 80GB,Confident,"We present OLMo 2, the next generation of our fully open language models. OLMo 2 includes dense autoregressive models with improved architecture and training recipe, pretraining data mixtures, and instruction tuning recipes. Our modified model architecture and training recipe achieve both better training stability and improved per-token efficiency. Our updated pretraining data mixture introduces a new, specialized data mix called Dolmino Mix 1124, which significantly improves model capabilities across many downstream task benchmarks when introduced via late-stage curriculum training (i.e. specialized data during the annealing phase of pretraining). Finally, we incorporate best practices from Tülu 3 to develop OLMo 2-Instruct, focusing on permissive data and extending our final-stage reinforcement learning with verifiable rewards (RLVR). Our OLMo 2 base models sit at the Pareto frontier of performance to compute, often matching or outperforming open-weight only models like Llama 3.1 and Qwen 2.5 while using fewer FLOPs and with fully transparent training data, code, and recipe. Our fully open OLMo 2-Instruct models are competitive with or surpassing open-weight only models of comparable size, including Qwen 2.5, Llama 3.1 and Gemma 2. We release all OLMo 2 artifacts openly -- models at 7B and 13B scales, both pretrained and post-trained, including their full training data, training code and recipes, training logs and thousands of intermediate checkpoints. The final instruction model is available on the Ai2 Playground as a free research demo.","United States of America,United States of America,United States of America",,,,,Open source,"apache 2
https://huggingface.co/allenai/OLMo-2-1124-13B
https://github.com/allenai/OLMo","Research collective,Academia,Academia"
DeepSeek-V3,Language,"Language modeling/generation,Code generation,Quantitative reasoning,Question answering",,Open weights (restricted use),https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf,,DeepSeek-V3 Technical Report,2024-12-24,DeepSeek,671000000000.0,Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.,3.4078e+24,"""At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.""

6 * 37B (active params) * 14.8T = 3.2856e24 for pretraining.

We know they trained in FP8. H800s get 1.513e15 FLOP/s in FP8:
2.688M * 3600 * 1.513e15 * MFU = 3.2856e24

Suggests a MFU of 0.2244 in pre-training. If we assume MFU was the same in post-training, that adds an additional:

0.1M * 3600 * 1.513e15 * 0.2244 = 1.222e23 FLOP from post-training

Total: 3.2856e24 + 1.222e23 = 3.4078e24 FLOP",,,14800000000000.0,"""We pre-train DeepSeek-V3 on 14.8 trillion diverse and
high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities""",,"""DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training""",NVIDIA H800 SXM5,Confident,"We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.",China,,,,2048.0,,"MIT and deepseek license
https://github.com/deepseek-ai/DeepSeek-V3?tab=readme-ov-file",Industry
o3,"Language,Vision,Multimodal","Language modeling/generation,Question answering,Quantitative reasoning,Code generation,Visual question answering,Search,Instruction interpretation,Visual puzzles",,API access,https://openai.com/index/introducing-o3-and-o4-mini/,,"Our most powerful reasoning model with leading performance on coding, math, science, and vision",2024-12-20,OpenAI,,,,,Unspecified unreleased,"""The OpenAI o-series models are trained with large-scale reinforcement learning on chains of thought.""

""OpenAI o3 and o4-mini were trained on diverse datasets,
including information that is publicly available on the internet, information that we partner with third parties to access, and information that our users or human trainers and researchers provide or generate. Our data processing pipeline includes rigorous filtering to maintain data quality and mitigate potential risks. We use advanced data filtering processes to reduce personal information
from training data. We also employ a combination of our Moderation API and safety classifiers to help prevent the use of harmful or sensitive content, including explicit materials such as sexual content involving a minor.""",,,,,,Unknown,"Today, we’re releasing OpenAI o3 and o4-mini, the latest in our o-series of models trained to think for longer before responding. These are the smartest models we’ve released to date, representing a step change in ChatGPT's capabilities for everyone from curious users to advanced researchers. For the first time, our reasoning models can agentically use and combine every tool within ChatGPT—this includes searching the web, analyzing uploaded files and other data with Python, reasoning deeply about visual inputs, and even generating images. Critically, these models are trained to reason about when and how to use tools to produce detailed and thoughtful answers in the right output formats, typically in under a minute, to solve more complex problems. This allows them to tackle multi-faceted questions more effectively, a step toward a more agentic ChatGPT that can independently execute tasks on your behalf. The combined power of state-of-the-art reasoning with full tool access translates into significantly stronger performance across academic benchmarks and real-world tasks, setting a new standard in both intelligence and usefulness.
<..>
OpenAI o3 is our most powerful reasoning model that pushes the frontier across coding, math, science, visual perception, and more. It sets a new SOTA on benchmarks including Codeforces, SWE-bench (without building a custom model-specific scaffold), and MMMU. It’s ideal for complex queries requiring multi-faceted analysis and whose answers may not be immediately obvious. It performs especially strongly at visual tasks like analyzing images, charts, and graphics. In evaluations by external experts, o3 makes 20 percent fewer major errors than OpenAI o1 on difficult, real-world tasks—especially excelling in areas like programming, business/consulting, and creative ideation. Early testers highlighted its analytical rigor as a thought partner and emphasized its ability to generate and critically evaluate novel hypotheses—particularly within biology, math, and engineering contexts.

________
model was announced 2024/12/20 
from ARS technica: ""On Friday, during Day 12 of its ""12 days of OpenAI,"" OpenAI CEO Sam Altman announced its latest AI ""reasoning"" models, o3 and o3-mini, which build upon the o1 models launched earlier this year. The company is not releasing them yet but will make these models available for public safety testing and research access today.""

https://arstechnica.com/information-technology/2024/12/openai-announces-o3-and-o3-mini-its-next-simulated-reasoning-models/

model was released 2025/04/16",United States of America,,,,,Unreleased,"""Both o3 and o4-mini are also available to developers today via the Chat Completions API and Responses API (some developers will need to verify their organizations⁠(opens in a new window) to access these models)""",Industry
Granite 3.1 2B,Language,"Language modeling/generation,Question answering,Translation",,Open weights (unrestricted),https://huggingface.co/ibm-granite/granite-3.1-2b-base,,,2024-12-18,IBM,2500000000.0,"2.5B
Model Architecture: Granite-3.1-2B-Base is based on a decoder-only dense transformer architecture. Core components of this architecture are: GQA and RoPE, MLP with SwiGLU, RMSNorm, and shared input/output embeddings.",1.8e+23,6 FLOP / token / parameter * 2.5 * 10^9 parameters * 12*10^12 tokens = 1.8e+23 FLOP,Unspecified unreleased,"Training Data: This model is trained on a mix of open source and proprietary data following a three-stage training strategy.

Stage 1 data: The data for stage 1 is sourced from diverse domains, such as: web, code, academic sources, books, and math data.
Stage 2 data: The data for stage 2 comprises a curated mix of high-quality data from the same domains, plus multilingual and instruction data. The goal of this second training phase is to enhance the model’s performance on specific tasks.
Stage 3 data: The data for stage 3 consists of original stage-2 pretraining data with additional synthetic long-context data in form of QA/summary pairs where the answer contains a recitation of the related paragraph before the answer.",12000000000000.0,12T,,,NVIDIA H100 SXM5 80GB,Confident,Model Summary: Granite-3.1-2B-Base extends the context length of Granite-3.0-2B-Base from 4K to 128K using a progressive training strategy by increasing the supported context length in increments while adjusting RoPE theta until the model has successfully adapted to desired length of 128K. This long-context pre-training stage was performed using approximately 500B tokens.,United States of America,,,,,Unreleased,"https://huggingface.co/ibm-granite/granite-3.1-2b-base
Apache 2.0",Industry
Granite 3.1 8B,Language,"Language modeling/generation,Question answering,Translation",,Open weights (unrestricted),https://huggingface.co/ibm-granite/granite-3.1-8b-base,,,2024-12-18,IBM,8100000000.0,"8.1B
Model Architecture: Granite-3.1-8B-Base is based on a decoder-only dense transformer architecture. Core components of this architecture are: GQA and RoPE, MLP with SwiGLU, RMSNorm, and shared input/output embeddings.",5.832e+23,6ND = 6 FLOP / parameter / token * 8.1*10^9 parameters * 12*10^12 tokens = 5.832e+23 FLOP,Unspecified unreleased,"**Training Data:** This model is trained on a mix of open source and proprietary data following a three-stage training strategy. * Stage 1 data: The data for stage 1 is sourced from diverse domains, such as: web, code, academic sources, books, and math data. * Stage 2 data: The data for stage 2 comprises a curated mix of high-quality data from the same domains, plus multilingual and instruction data. The goal of this second training phase is to enhance the model’s performance on specific tasks. * Stage 3 data: The data for stage 3 consists of original stage-2 pretraining data with additional synthetic long-context data in form of QA/summary pairs where the answer contains a recitation of the related paragraph before the answer.",12000000000000.0,12T,,,NVIDIA H100 SXM5 80GB,Confident,Model Summary: Granite-3.1-8B-Base extends the context length of Granite-3.0-8B-Base from 4K to 128K using a progressive training strategy by increasing the supported context length in increments while adjusting RoPE theta until the model has successfully adapted to desired length of 128K. This long-context pre-training stage was performed using approximately 500B tokens.,United States of America,,,,,Unreleased,"https://huggingface.co/ibm-granite/granite-3.1-8b-base
Apache 2.0",Industry
Phi-4,Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning","Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, Yi Zhang",Open weights (unrestricted),https://arxiv.org/abs/2412.08905,,Phi-4 Technical Report,2024-12-12,Microsoft Research,14000000000.0,"14B parameters, dense decoder-only Transformer model",9.3202015e+23,"6ND = 6* 14*10^9 parameters * 10*10^12 tokens = 8.4e+23 FLOP

989500000000000 FLOP / sec [assumed bf16 precision] * 1920 GPUs * 504 hours * 3600 sec / hour * 0.3 [assumed utilization] = 1.0341209e+24 FLOP

geometric mean
sqrt(8.4e+23 * 1.0341209e+24) = 9.3202015e+23",Unspecified unreleased,"""The pretraining phase of phi-4 relies heavily on synthetic datasets generated through a variety of techniques. ""
""We collected a wide variety of high-quality organic data sources
for phi-4, prioritizing reasoning-dense and nuanced material (e.g., academic papers, educational forums,
and programming tutorials).""
""Our post-training data is composed of:
• Supervised Fine-Tuning (SFT) Datasets
• Direct Preference Optimization (DPO)",10000000000000.0,"""The model was pretrained for approximately 10T tokens using linear warm-up and decay schedules with peak learning rate of 0.0003, constant weight decay of 0.1, and global batch size of 5760. ""

Table 5:
Web 15% 1.3T unique tokens 1.2 epochs
Web rewrites 15% 290B unique tokens 5.2 epochs
Synthetic 40% 290B unique tokens 13.8 epochs
Code data 20% 820B unique tokens 2.4 epochs
Acquired sources 10% 580B unique tokens  1.7 epochs
",504.0,"https://huggingface.co/microsoft/phi-4
21 days * 24 hours / day = 504 hours",NVIDIA H100 SXM5 80GB,Confident,"We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process. While previous models in the Phi family largely distill the capabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model on STEM-focused QA capabilities, giving evidence that our data-generation and post-training techniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong performance relative to its size -- especially on reasoning-focused benchmarks -- due to improved data, training curriculum, and innovations in the post-training scheme.","United States of America,United Kingdom of Great Britain and Northern Ireland",,,,1920.0,Unreleased,"""Phi-4 is currently available on Azure AI Foundry under a Microsoft Research License Agreement (MSRLA) and will be available on Hugging Face next week.  ""
Hugging Face: MIT license

https://huggingface.co/microsoft/phi-4",Industry
Gemini 2.0 Flash,"Language,Vision,Audio,Speech,Video,Multimodal","Language modeling/generation,Question answering,Speech synthesis,Text-to-image,Visual question answering,Speech recognition,Code generation,Quantitative reasoning,Video description,Translation,Chat,Table tasks",Gemini Team,API access,https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message,,Introducing Gemini 2.0: our new AI model for the agentic era,2024-12-11,"Google DeepMind,Google",,,,"""We used Trillium TPUs to train the new Gemini 2.0, Google’s most capable AI model yet"" according to https://cloud.google.com/blog/products/compute/trillium-tpu-is-ga",Unspecified unreleased,,,,,,Google TPU v6e Trillium,Unknown,"Gemini 2.0 Flash builds on the success of 1.5 Flash, our most popular model yet for developers, with enhanced performance at similarly fast response times. Notably, 2.0 Flash even outperforms 1.5 Pro on key benchmarks, at twice the speed. 2.0 Flash also comes with new capabilities. In addition to supporting multimodal inputs like images, video and audio, 2.0 Flash now supports multimodal output like natively generated images mixed with text and steerable text-to-speech (TTS) multilingual audio. It can also natively call tools like Google Search, code execution as well as third-party user-defined functions.","United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France,United States of America",,,,,Unreleased,,"Industry,Industry"
EXAONE 3.5 7.8B,Language,"Language modeling/generation,Question answering,Translation","Soyoung An, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Yountae Jung, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Woohyung Lim, Sangha Park, Sooyoun Park, Yongmin Park, Sihoon Yang, Heuiyeen Yeen, Hyeongu Yun",Open weights (non-commercial),https://arxiv.org/abs/2412.04862,,EXAONE 3.5: Series of Large Language Models for Real-world Use Cases,2024-12-09,LG AI Research,7800000000.0,7.8B,4.21e+23,4.21 × 10^23 (Table 2),Unspecified unreleased,,9000000000000.0,9T tokens (Table 2),,,,Confident,"This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8B, and 2.4B. These models feature several standout capabilities: 1) exceptional instruction following capabilities in real-world scenarios, achieving the highest scores across seven benchmarks, 2) outstanding long-context comprehension, attaining the top performance in four benchmarks, and 3) competitive results compared to state-of-the-art open models of similar sizes across nine general benchmarks. The EXAONE 3.5 language models are open to anyone for research purposes and can be downloaded from this https URL. For commercial use, please reach out to the official contact point of LG AI Research: contact_us@lgresearch.ai.",Korea (Republic of),,,,,Unreleased,Exaone license (allows only non-commercial usage),Industry
EXAONE 3.5 32B,Language,"Language modeling/generation,Question answering,Translation","Soyoung An, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Yountae Jung, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Woohyung Lim, Sangha Park, Sooyoun Park, Yongmin Park, Sihoon Yang, Heuiyeen Yeen, Hyeongu Yun",Open weights (non-commercial),https://arxiv.org/abs/2412.04862,,EXAONE 3.5: Series of Large Language Models for Real-world Use Cases,2024-12-09,LG AI Research,32000000000.0,32B,1.25e+24,1.25 × 10^24 (Table 2) ,Unspecified unreleased,,6500000000000.0,6.5T tokens (Table 2),,,,Confident,"This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8B, and 2.4B. These models feature several standout capabilities: 1) exceptional instruction following capabilities in real-world scenarios, achieving the highest scores across seven benchmarks, 2) outstanding long-context comprehension, attaining the top performance in four benchmarks, and 3) competitive results compared to state-of-the-art open models of similar sizes across nine general benchmarks. The EXAONE 3.5 language models are open to anyone for research purposes and can be downloaded from this https URL. For commercial use, please reach out to the official contact point of LG AI Research: contact_us@lgresearch.ai.",Korea (Republic of),,,,,Unreleased,Exaone license (allows only non-commercial usage),Industry
Llama 3.3 70B,Language,"Language modeling/generation,Question answering,Translation,Code generation",,Open weights (restricted use),https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/,,Meta Llama 3.3 multilingual large language model (LLM) ,2024-12-06,Meta AI,70000000000.0,70B,6.8649768e+24,"6ND = 6 FLOP / parameter / token * 70*10^9 parameters * 15*10^12 tokens = 6.3e+24 FLOP

7000000 GPU-hours * 3600 sec / hour * 989500000000000 FLOP / second * 0.3 [assumed utilization]= 7.48062e+24 FLOP

sqrt(7.48062e+24*6.3e+24) = 6.8649768e+24",Unspecified unreleased,"""A new mix of publicly available online data.""",15000000000000.0,"""Overview: Llama 3.3 was pretrained on ~15 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples.

Data Freshness: The pretraining data has a cutoff of December 2023.""",,"""Training utilized a cumulative of 39.3M GPU hours of computation on H100-80GB (TDP of 700W) type hardware, per the table below. Training time is the total GPU time required for training each model and power consumption is the peak power capacity per GPU device used, adjusted for power usage efficiency.""

Llama 3.3 70B: Training Time (GPU hours): 7M
",NVIDIA H100 SXM5 80GB,Confident,"The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.

Model developer: Meta

Model Architecture: Llama 3.3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.",United States of America,,,,,Unreleased,"License A custom commercial license, the Llama 3.3 Community License Agreement, is available at: https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE

""Llama 3.3 is intended for commercial and research use in multiple languages.""",Industry
o1,"Language,Mathematics,Multimodal","Code generation,Language modeling/generation,Quantitative reasoning,Chat,Question answering,Translation",,API access,https://openai.com/index/introducing-chatgpt-pro/,,Introducing ChatGPT Pro: Broadening usage of frontier AI.,2024-12-05,OpenAI,,,,,Unspecified unreleased,"""Both models were trained on a variety of publicly available datasets, including web data and open-source datasets. Key components include reasoning data and scientific literature. <..> To further enhance the capabilities of o1-preview and o1-mini, we formed partnerships to access high-value non-public datasets. These proprietary data sources include paywalled content, specialized archives, and other domain-specific datasets that provide deeper insights into industry-specific knowledge and use cases.""",,,,,,Unknown,"We've developed a new series of AI models designed to spend more time thinking before they respond. They can reason through complex tasks and solve harder problems than previous models in science, coding, and math.

Today, we are releasing the first of this series in ChatGPT and our API. This is a preview and we expect regular updates and improvements. Alongside this release, we’re also including evaluations for the next update, currently in development.",United States of America,,,,,Unreleased,,Industry
Amazon Nova Pro,"Multimodal,Language,Video","Language modeling/generation,Retrieval-augmented generation,Video generation",,API access,https://aws.amazon.com/es/blogs/aws/introducing-amazon-nova-frontier-intelligence-and-industry-leading-price-performance/,,Introducing Amazon Nova foundation models: Frontier intelligence and industry leading price performance,2024-12-03,Amazon,,,6.000010000000001e+24,"""probably just below 1e25 stemming from the Llama 70B serving speed.  If Llama 70B is trained proportionally to 405B, then it's at ~ 6.6e24. Nova Pro is served at 100tk/s, while Llama 70B is served at 70tk/s on average, and 100tk/s by together.ai at FP8. So Nova Pro would be >1e25 if they roughly 2x the amount of training compared to Llama 70B which [seems unlikely]""",,,,,,,,Speculative,,United States of America,,,,,,,Industry
Hunyuan Video,Video,Video generation,"Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Daquan Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, Caesar Zhong",Open weights (restricted use),https://www.arxiv.org/abs/2412.03603,,HunyuanVideo: A Systematic Framework For Large Video Generative Models,2024-12-03,Tencent,13000000000.0,13b,1.4814815e+23,"from Figure 10:

the optimal model has 13b parameters, 5.8e+07PF (image training) + 7.0e+07PF (video training) of compute and 740B (image tokens) + 928B (video tokens) 

5.8e+07PF + 7.0e+07PF = 12.8e+07PF = 12.8*10^7*10^20/(24*3600) = 1.4814815e+23 FLOPs

6ND = 6*13*10^9*(740+928)*10^9 = 1.30104e+23
",Unspecified unreleased,"""We employ various filters for data filtering and progressively increase their thresholds to build 4 training datasets, i.e., 256p, 360p, 540p, and
720p, while the final SFT dataset is built through manual annotation.""",,,,,,Confident,"Recent advancements in video generation have significantly impacted daily life for both individuals and industries. However, the leading video generation models remain closed-source, resulting in a notable performance gap between industry capabilities and those available to the public. In this report, we introduce HunyuanVideo, an innovative open-source video foundation model that demonstrates performance in video generation comparable to, or even surpassing, that of leading closed-source models. HunyuanVideo encompasses a comprehensive framework that integrates several key elements, including data curation, advanced architectural design, progressive model scaling and training, and an efficient infrastructure tailored for large-scale model training and inference. As a result, we successfully trained a video generative model with over 13 billion parameters, making it the largest among all open-source models. We conducted extensive experiments and implemented a series of targeted designs to ensure high visual quality, motion dynamics, text-video alignment, and advanced filming techniques. According to evaluations by professionals, HunyuanVideo outperforms previous state-of-the-art models, including Runway Gen-3, Luma 1.6, and three top-performing Chinese video generative models. By releasing the code for the foundation model and its applications, we aim to bridge the gap between closed-source and open-source communities. This initiative will empower individuals within the community to experiment with their ideas, fostering a more dynamic and vibrant video generation ecosystem. The code is publicly available at this https URL. https://github.com/Tencent/HunyuanVideo.",China,,,,,Unreleased,"""THIS LICENSE AGREEMENT DOES NOT APPLY IN THE EUROPEAN UNION, UNITED KINGDOM AND SOUTH KOREA"" 

also requires additional licensing in case of massive commercial use

https://huggingface.co/tencent/HunyuanVideo/blob/main/LICENSE

the code seems to be just inference code not training code
",Industry
abab7,Language,Language modeling/generation,,API access,https://www.minimaxi.com/en/news/abab65-series,,,2024-11-29,MiniMax,,,,,Unspecified unreleased,,,,,,,Unverified,,China,,,,,Unreleased,,Industry
bailing-pro-1120,Multimodal,Language modeling/generation,Ant Group,Unreleased,,,,2024-11-15,Ant Group,,,,"Trained on ""trillions of tokens"".",Unspecified unreleased,,,,,,,Unverified,,China,,,,,,,Industry
Qwen2.5-Coder (32B),Language,"Language modeling/generation,Code generation","Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, Junyang Lin",Open weights (unrestricted),https://arxiv.org/abs/2409.12186,,Qwen2.5-Coder Technical Report,2024-11-12,Alibaba,32500000000.0,32.5B (31B - non emb),1.0725e+24,"Assuming 1 epoch

6ND = 6*32.5 parameters *10^9*5.5*10^12 tokens = 1.0725e+24","GitHub,Common Crawl,Unspecified unreleased","""We collected public repositories from GitHub created before February 2024""

""We curated a large-scale and high-quality text-code mixed
dataset from Common Crawl, which includes code-related documentation, tutorials, blogs,
and more""

""We used CodeQwen1.5, the predecessor of Qwen2.5-Coder, to generate large-scale synthetic datasets.""",5500000000000.0,"""As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens.""",,,,Confident,"In this report, we introduce the Qwen2.5-Coder series, a significant upgrade from its predecessor, CodeQwen1.5. This series includes six models: Qwen2.5-Coder-(0.5B/1.5B/3B/7B/14B/32B). As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning, scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder demonstrates impressive code generation capabilities while retaining general and math skills. These models have been evaluated on a wide range of code-related tasks, achieving state-of-the-art (SOTA) performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair, consistently outperforming larger models of the same model size. We believe that the release of the Qwen2.5-Coder series will advance research in code intelligence and, with its permissive licensing, support wider adoption by developers in real-world applications.",China,,,,,Unreleased,"Apache 2.0
https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct

though they have apache 2.0 github repository it seems to be inference code rather than training code",Industry
Hunyuan-Large,Language,"Language modeling/generation,Question answering,Code generation,Translation","Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, Jiahao Bu, Zhongzhi Chen, Xuemeng Huang, Fengzong Lian,
Saiyong Yang, Jianfeng Yan, Yuyuan Zeng, Xiaoqin Ren, Chao Yu, Lulu Wu, Yue Mao, Jun Xia, Tao Yang, Suncong Zheng, Kan Wu, Dian Jiao, Jinbao Xue, Xipeng Zhang, Decheng Wu, Kai Liu, Dengpeng Wu, Guanghui Xu, Shaohua Chen, Shuang Chen, Xiao Feng, Yigeng Hong, Junqiang Zheng, Chengcheng Xu, Zongwei Li, Xiong Kuang, Jianglu Hu, Yiqi Chen, Yuchi Deng, Guiyang Li, Ao Liu, Chenchen Zhang, Shihui Hu, Zilong Zhao, Zifan Wu, Yao Ding, Weichao Wang, Han Liu, Roberts Wang, Hao Fei, Peijie Yu, Ze Zhao, Xun Cao, Hai Wang, Fusheng Xiang, Mengyuan Huang, Zhiyuan Xiong, Bin Hu, Xuebin Hou, Lei Jiang, Jianqiang Ma, Jiajia Wu, Yaping Deng, Yi Shen, Qian Wang, Weijie Liu, Jie Liu, Meng Chen, Liang Dong, Weiwen Jia, Hu Chen, Feifei Liu, Rui Yuan, Huilin Xu, Zhenxiang Yan, Tengfei Cao, Zhichao Hu, Xinhua Feng, Dong Du, Tinghao Yu, Yangyu Tao, Feng Zhang, Jianchen Zhu, Chengzhong Xu, Xirui Li, Chong Zha, Wen Ouyang, Yinben Xia, Xiang Li, Zekun He, Rongpeng Chen, Jiawei Song, Ruibin Chen, Fan Jiang, Chongqing Zhao, Bo Wang, Hao Gong, Rong Gan, Winston Hu, Zhanhui Kang, Yong Yang, Yuhong Liu, Di Wang, and Jie Jiang.
",Open weights (restricted use),https://arxiv.org/abs/2411.02265,,Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent,2024-11-06,Tencent,389000000000.0,"""a total of 389 billion parameters and 52 billion activation parameters""",3.49237e+24,"52B activated parameters

6ND = 6*52*10^9*7*10^12 = 2.184 × 10^24

They also suggest more precise formula to calculate MoE compute budget:

9.59ND + 2.3 × 10^8D = 9.59*52*10^9*7*10^12 + 2.3 × 10^8 ×  7*10^12 = 3.49237×10^24

which seems closer to projected compute on Figure 3",Unspecified unreleased,,7000000000000.0,"""# Trained Tokens 7T""  Table 1",,,,Confident,"In this paper, we introduce Hunyuan-Large, which is currently the largest open-source Transformer-based mixture of experts model, with a total of 389 billion parameters and 52 billion activation parameters, capable of handling up to 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior performance across various benchmarks including language understanding and generation, logical reasoning, mathematical problem-solving, coding, long-context, and aggregated tasks, where it outperforms LLama3.1-70B and exhibits comparable performance when compared to the significantly larger LLama3.1-405B model. Key practice of Hunyuan-Large include large-scale synthetic data that is orders larger than in previous literature, a mixed expert routing strategy, a key-value cache compression technique, and an expert-specific learning rate strategy. Additionally, we also investigate the scaling laws and learning rate schedule of mixture of experts models, providing valuable insights and guidances for future model development and optimization. The code and checkpoints of Hunyuan-Large are released to facilitate future innovations and applications.",China,,,,,Open (restricted use),"the license doesn't regulate usage in the EU
also requires additional licensing in case of massive commercial use",Industry
Doubao-pro,Language,"Language modeling/generation,Question answering,Text summarization,Text classification",,API access,https://www.volcengine.com/docs/6360/1264663,,Doubao General Model Pro (Doubao-pro),2024-10-28,ByteDance,500000000000.0,"[Speculative] Doubao's large language model has scaled up from 35 billion parameters to 800 billion, with 500 billion and 800 billion parameter models currently under training.
https://xueqiu.com/9637001584/309910396?md5__1038=7qmx2DyDuie4cDBqDTQEWqDtMvO4iTphD
",2.505e+25,6ND = 6 * 500*10^9 * 8350*10^9 = 2.505e+25,Unspecified unreleased,"Doubao's data sources primarily rely on proprietary business data, accounting for 50-60%; externally sourced data comprises 15-20%; and synthetic data has been used since June of this year, although Doubao is cautious in feeding synthetic data due to its uncertain quality. ",8350000000000.0,"[Speculative] Doubao's pre-training data volume is approximately 500TB, with only about 10% of this data actually used for training. The current version employs a non-Mixture-of-Experts (MoE) architecture. In the future, MoE architecture may be introduced to increase parameter count and performance, while also integrating multimodal data solutions.

So this model is dense, and the training data is probably all text tokens, not multimodal.

50TB * 167M tokens/GB ~= 8.35 trillion tokens
",,,,Speculative,"A professional-grade, self-developed LLM supporting up to 128k tokens, enabling fine-tuning across the entire series. ",China,,,,,Unreleased,,Industry
Granite 3.0 8B,Language,"Language modeling/generation,Question answering,Translation,Text summarization,Text classification,Code generation",Granite Team IBM,Open weights (unrestricted),https://github.com/ibm-granite/granite-3.0-language-models/tree/main,,Granite 3.0 Language Models,2024-10-21,IBM,8100000000.0,8.1B,5.832e+23,"6ND = 6 FLOP / parameter / token * 8.1*10^9 parameters * 12*10^12 tokens = 5.832e+23 FLOP

""All our Granite 3.0 models are trained using a
compute budget of 8.35 × 10^23 FLOPS.""

8.35 × 10^23 * 757.0 (model's power consumption) / (174.6+757.0+64.5+121.2) = 5.6573436e+23

hardware estimation:
832102 GPU-hours * 3600 sec / hour * 989500000000000 FLOP / GPU / sec * 0.3 [assumed utilization] = 8.8923412e+23",Unspecified unreleased,"Granite 3.0 language models are trained using data from various sources such as unstructured natural language text and code data from theWeb curated by IBM, a collection of synthetic datasets generated by IBM, and publicly available high-quality datasets with permissible licenses.",12000000000000.0,12T tokens,,Table 7,NVIDIA H100 SXM5 80GB,Confident,"This report presents Granite 3.0, a new set of lightweight, state-of-the-art, open foundation models ranging in scale from 400 million to 8 billion active parameters.
Equipped with native support of multilingual, coding, function 
calling, and strong safety performance, these models target enterprise use cases, including on-premise and on-device settings. Evaluations on a comprehensive set of tasks demonstrate that our models consistently reach state-of-the-art performance for their size (as shown in Figure 1 and 2). This report also discloses technical details of pre-training and post-training that may help the research community accelerate the collective efforts to develop open foundation models. We publicly release pre-trained and post-trained versions of all our Granite 3.0 models under a standard permissive Apache 2.0 license allowing both research and commercial use. With support from the open source community, the Granite 3.0 models have been integrated with a
range of existing tools for quantization, fine-tuning, and deployment.",United States of America,,,,256.0,Unreleased,"Apache 2.0 license
https://huggingface.co/ibm-granite/granite-3.0-8b-instruct",Industry
Granite 3.0 2B,Language,"Language modeling/generation,Question answering,Translation,Text summarization,Text classification,Code generation",Granite Team IBM,Open weights (unrestricted),https://github.com/ibm-granite/granite-3.0-language-models/tree/main,,Granite 3.0 Language Models,2024-10-21,IBM,2500000000.0,2.5B,1.8e+23,"6ND = 6 FLOP / token / parameter * 2.5*10^9 parameters * 12*10^12 tokens = 1.8e+23 FLOP

""""All our Granite 3.0 models are trained using a
compute budget of 8.35 × 10^23 FLOPS.""

8.35 × 10^23 * 174.6 (model's power consumption) / (174.6+757.0+64.5+121.2) =1.304851e+23

hardware estimation:
192030 GPU-hours * 3600 sec / hour *989500000000000 FLOP / GPU / sec * 0.3 [assumed utilization] = 2.0521478e+23 FLOP",Unspecified unreleased,"Granite 3.0 language models are trained using data from various sources such as unstructured natural language text and code data from theWeb curated by IBM, a collection of synthetic datasets generated by IBM, and publicly available high-quality datasets with permissible licenses.",12000000000000.0,12T tokens,,Table 7,NVIDIA H100 SXM5 80GB,Confident,"This report presents Granite 3.0, a new set of lightweight, state-of-the-art, open foundation models ranging in scale from 400 million to 8 billion active parameters.
Equipped with native support of multilingual, coding, function 
calling, and strong safety performance, these models target enterprise use cases, including on-premise and on-device settings. Evaluations on a comprehensive set of tasks demonstrate that our models consistently reach state-of-the-art performance for their size (as shown in Figure 1 and 2). This report also discloses technical details of pre-training and post-training that may help the research community accelerate the collective efforts to develop open foundation models. We publicly release pre-trained and post-trained versions of all our Granite 3.0 models under a standard permissive Apache 2.0 license allowing both research and commercial use. With support from the open source community, the Granite 3.0 models have been integrated with a
range of existing tools for quantization, fine-tuning, and deployment.",United States of America,,,,768.0,Unreleased,"Apache 2.0 license
https://huggingface.co/ibm-granite/granite-3.0-2b-instruct",Industry
Yi-Lightning,Language,Language modeling/generation,,API access,https://www.lingyiwanwu.com/en https://platform.lingyiwanwu.com/,,Yi-Lightning,2024-10-18,01.AI,,,1.5e+24,"The CEO of 01.AI tweeted that Yi-Lightning was trained for 1 month on 2000 H100s: https://x.com/kaifulee/status/1846310645849047524
Assuming this is accurate:
(9.9e14 * 2000) FLOP/s * 1 month * 30.5 days/month * 24hr/day * 3600 s/hr * 0.3 utilization assumption = 1.565e24",Unspecified unreleased,,,,720.0,"https://x.com/kaifulee/status/1846310645849047524
""it was trained on 2000 H100s for 1 month""",NVIDIA H100 SXM5 80GB,Confident,,China,,,,2000.0,Unreleased,https://platform.lingyiwanwu.com/,Industry
Firefly Video,Video,Video generation,Adobe,Hosted access (no API),https://news.adobe.com/news/2024/10/101424-adobe-launches-firefly-video-model,,"Adobe Launches Firefly Video Model and Enhances Image, Vector and Design Models",2024-10-14,Adobe,,,,,,,,,,,,Unknown,,United States of America,,,,,,,Industry
CogVideoX,"Video,Language,Multimodal",Video generation,"Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, Jie Tang",Open weights (non-commercial),https://arxiv.org/abs/2408.06072,,CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer,2024-10-08,"Zhipu AI,Tsinghua University",5000000000.0,5 billion,,,"LAION,COYO-700M","""We construct a collection of relatively high-quality video clips with text descriptions with video filters and recaption models. After filtering, approximately 35M single-shot clips remain, with each clip averaging about 6 seconds. We additionally use 2B images filtered with aesthetics score from LAION-5B (Schuhmann et al., 2022) and COYO-700M (Byeon et al., 2022) datasets to assist training.""",,"""35M single-shot clips remain, with each clip averaging about 6 seconds""
""During training, we first train a 3D VAE at 256 × 256 resolution and 17 frames to save computation. Randomly select 8 or 16 fps to enhance the model’s robustness""
""we conduct a two-stage training process by first training on a video of 17 frames and finetuning by context parallel on videos of 161 frames.""

They provide many training details for smaller models but not this one",,,,Confident,"We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 * 1360 pixels. Previous video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. First, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, CogVideoX is adept at producing coherent, long-duration, different shape videos characterized by significant motions. In addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. Results show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. The model weight of both 3D Causal VAE, Video caption model and CogVideoX are publicly available at this https URL.","China,China",,,,,Open source,"https://huggingface.co/THUDM/CogVideoX-5b

https://github.com/THUDM/CogVideo
Apache 2.0 for code","Industry,Academia"
Movie Gen Video,Video,Video generation,"Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen
Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan
Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat
Singh, Mary Williamson, Matt Le, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit
Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen,
Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Tao Xu, Tingbo Hou, Wei-Ning
Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval
Kirstain, Zecheng He, Zijian He",Unreleased,https://ai.meta.com/static-resource/movie-gen-research-paper,,Movie Gen: A Cast of Media Foundation Models,2024-10-04,Meta AI,30000000000.0,30B,1.65e+24,"Model size = 30B
Broken down by training stage (table 3):
256px T2I: samples seen = 1.94E9; sample token length = 256; flops = 6ND = 8.94E22
256px T2I/V: samples seen = 3.95E8; sample token length = 8192; flops = 6ND = 5.82E23
768px T2I/V: samples seen = 7.38E7; sample token length = 73,728; flops = 6ND = 9.79E23
Total flops = 1.65E24",,,26600000000.0,"O(1B) images
O(100M) videos, each with 256 frames ~= 25M images",331.0,"54 hours for 256px T2I
128 hours for 256px T2I/V
149 hours for 768px T2I/V",NVIDIA H100 SXM5 80GB,Confident,"We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos
with different aspect ratios and synchronized audio. We also show additional capabilities such as
precise instruction-based video editing and generation of personalized videos based on a user’s image.
Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization,
video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation
model is a 30B parameter transformer trained with a maximum context length of 73K video tokens,
corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical
innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data
curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to
reap the benefits of scaling pre-training data, model size, and training compute for training large scale
media generation models. We hope this paper helps the research community to accelerate progress
and innovation in media generation models.
All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos.",United States of America,,,,6144.0,,,Industry
Movie Gen Audio,Audio,Audio generation,"Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen
Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan
Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat
Singh, Mary Williamson, Matt Le, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit
Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen,
Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Tao Xu, Tingbo Hou, Wei-Ning
Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval
Kirstain, Zecheng He, Zijian He",Unreleased,https://ai.meta.com/static-resource/movie-gen-research-paper,,Movie Gen: A Cast of Media Foundation Models,2024-10-04,Meta AI,13000000000.0,13B,1.4e+23,Pre trained for 14 days on 384 H100s. I assumed a 0.3 utilization rate,,,,It was trained on O(1k) hours of audio,360.0,"14 days of pretraining, 1 day of fine tuning",NVIDIA H100 SXM5 80GB,Confident,"We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos
with different aspect ratios and synchronized audio. We also show additional capabilities such as
precise instruction-based video editing and generation of personalized videos based on a user’s image.
Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization,
video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation
model is a 30B parameter transformer trained with a maximum context length of 73K video tokens,
corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical
innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data
curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to
reap the benefits of scaling pre-training data, model size, and training compute for training large scale
media generation models. We hope this paper helps the research community to accelerate progress
and innovation in media generation models.
All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos.",United States of America,,,,384.0,,,Industry
Llama 3.2 3B,Language,"Language modeling/generation,Text summarization,Question answering,Quantitative reasoning,Translation",,Open weights (restricted use),https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,,"Llama 3.2: Revolutionizing edge AI and vision with open, customizable models",2024-09-24,Meta AI,3210000000.0,https://huggingface.co/meta-llama/Llama-3.2-1B,1.7334e+23,"6ND = 6*3210000000.00*9000000000000 = 1.7334e+23

460000 hours * 3600 s * 133800000000000 FLOPS/s* 0.3 = 6.647184e+22",Unspecified unreleased,,9000000000000.0,"""Llama 3.2 was pretrained on up to 9 trillion tokens of data from publicly available sources.""",,https://huggingface.co/meta-llama/Llama-3.2-1B,NVIDIA H100 SXM5 80GB,Confident,"Today, we’re releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B), and lightweight, text-only models (1B and 3B) that fit onto edge and mobile devices, including pre-trained and instruction-tuned versions.
The Llama 3.2 1B and 3B models support context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge. These models are enabled on day one for Qualcomm and MediaTek hardware and optimized for Arm processors.
We’re sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety.
We’ve been working closely with partners like AWS, Databricks, Dell Technologies, Fireworks, Infosys, and Together AI to build Llama Stack distributions for their downstream enterprise clients. On-device distribution is via PyTorch ExecuTorch, and single-node distribution is via Ollama.
We continue to share our work because we believe openness drives innovation and is good for developers, Meta, and the world. Llama is already leading the way on openness, modifiability, and cost efficiency—enabling more people to have creative, useful, and life-changing breakthroughs using generative AI.
We’re making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, Snowflake, and more.",United States of America,,,,,Unreleased,"LLAMA 3.2 COMMUNITY LICENSE AGREEMENT

https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE",Industry
Spark 4.0,Language,Language modeling,iFlytek,Unreleased,https://www.iflytek.com/en/news-events/news/218.html,,"""SPARK"" Ignites a New Engine for Manufacturing Development: iFLYTEK Shines at 2024 World Manufacturing Convention",2024-09-23,iFlytek,,,,,,,,,,,,Unknown,"From September 20 to 23, the 2024 World Manufacturing Convention, themed ""Intelligent Manufacturing for a Better Future"", was held at the Hefei Binhu International Convention & Exhibition Center. iFLYTEK SPARK Large Model V4.0 (hereinafter referred to as "" iFLYTEK SPARK""), along with humanoid robot equipped with iFLYTEK SPARK, the Anhui Provincial Imaging Cloud Platform, and other products and solutions, made a grand appearance at the exhibition. Antelope Industrial Internet Co., Ltd. unveiled the Antelope Industrial Large Model V2.0 and a series of industrial application scenarios, demonstrating in a comprehensive manner the diverse ecosystem of general artificial intelligence empowering various industries.",China,,,,,,,Industry
Telechat2-115B,Language,Language modeling,Zihan Wang and Xinzhang Liu and Shixuan Liu and Yitong Yao and Yuyao Huang and Zhongjiang He and Xuelong Li and Yongxiang Li and Zhonghao Che and Zhaoxi Zhang and Yan Wang and Xin Wang and Luwen Pu and Huihan Xu and Ruiyu Fang and Yu Zhao and Jie Zhang and Xiaomeng Huang and Zhilong Lu and Jiaxin Peng and Wenjun Zheng and Shiquan Wang and Bingkai Yang and Xuewei he and Zhuoru Jiang and Qiyi Xie and Yanhan Zhang and Zhongqiu Li and Lingling Shi and Weiwei Fu and Yin Zhang and Zilu Huang and Sishi Xiong and Yuxiang Zhang and Chao Wang and Shuangyong Song,Open weights (restricted use),https://huggingface.co/Tele-AI/TeleChat2-115B,,TeleChat Technical Report,2024-09-20,China Telecom,115000000000.0,,6.899999999999999e+24,6ND: 6 * 115B * 10T = 6.9e24,,,10000000000000.0,The open source TeleChat2-115B model is trained using 10 trillion tokens of high-quality Chinese and English corpus,,,,Unverified,,China,,,,,,,Industry
Qwen2.5-72B,Language,"Language modeling/generation,Question answering,Quantitative reasoning",Qwen Team,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen2.5/,,Qwen2.5: A Party of Foundation Models!,2024-09-19,Alibaba,72700000000.0,72.7B,7.8e+24,"Training dataset size was 18 trillion

6ND = 6 * 72.7 billion parameters * 18 trillion tokens = 7.8e24",Unspecified unreleased,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens.""",18000000000000.0,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens""",,,,Confident,"In the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let’s get the party started!",China,,,,,Unreleased,"license: allows commercial. weights only
https://huggingface.co/Qwen/Qwen2.5-72B/blob/main/LICENSE ",Industry
Qwen2.5-3B,Language,"Language modeling/generation,Question answering,Quantitative reasoning",Qwen Team,Open weights (non-commercial),https://qwenlm.github.io/blog/qwen2.5-llm/,,Qwen2.5: A Party of Foundation Models!,2024-09-19,Alibaba,3090000000.0,3.09B,3.3372e+23,"Training dataset size was 18 trillion

6ND = 6 * 3.09 billion parameters * 18 trillion tokens = 3.3372e+23",Unspecified unreleased,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens.""",18000000000000.0,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens""",,,,Confident,"In the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let’s get the party started!",China,,,,,Unreleased,Qwen Research license,Industry
Qwen2.5-7B,Language,"Language modeling/generation,Question answering,Quantitative reasoning",Qwen Team,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen2.5/,,Qwen2.5: A Party of Foundation Models!,2024-09-19,Alibaba,7610000000.0,7.61B,8.2188e+23,"Training dataset size was 18 trillion

6ND = 6 * 7.61 billion parameters * 18 trillion tokens = 8.2188e+23",Unspecified unreleased,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens.""",18000000000000.0,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens""",,,,Confident,"In the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let’s get the party started!

The Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU, 49.8 on math challenges such as MATH, and 57.9 on coding tasks like HumanEval.",China,,,,,Unreleased,Apache 2.0,Industry
Qwen2.5-1.5B,Language,"Language modeling/generation,Question answering,Quantitative reasoning",Qwen Team,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen2.5-llm/,,Qwen2.5-LLM: Extending the boundary of LLMs,2024-09-19,Alibaba,1540000000.0,1.54B,1.6632e+23,"Training dataset size was 18 trillion

6ND = 6 * 1.54B billion parameters * 18 trillion tokens = 1.6632e+23",Unspecified unreleased,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens.""",18000000000000.0,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens""",,,,Confident,"In the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let’s get the party started!",China,,,,,Unreleased,Apache 2.0,Industry
Qwen2.5-14B,Language,"Language modeling/generation,Question answering,Quantitative reasoning",Qwen Team,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen2.5/,,Qwen2.5: A Party of Foundation Models!,2024-09-19,Alibaba,14700000000.0,"14.7B according to the model card
https://qwenlm.github.io/blog/qwen2.5-llm/",1.58760000000001e+24,"Training dataset size was 18 trillion

6ND = 6 * 14.7 billion parameters * 18 trillion tokens = 1.59e24",Unspecified unreleased,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens.""",18000000000000.0,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens""",,,,Confident,"In the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let’s get the party started!

The Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU, 49.8 on math challenges such as MATH, and 57.9 on coding tasks like HumanEval.",China,,,,,Unreleased,Apache 2.0,Industry
Qwen2-VL-72B,"Language,Vision,Multimodal","Visual question answering,Video description,Language modeling/generation,Translation,Question answering,Character recognition,Quantitative reasoning","Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, Junyang Lin",Open weights (unrestricted),https://arxiv.org/abs/2409.12191,,Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution,2024-09-18,Alibaba,72000000000.0,72 billion (language model) and 675M (vision encoder),6.048e+23,6ND = 6 FLOP / token / parameter × 1.4×10^12 tokens × 7.2×10^10 parameters = 6.048e+23 FLOP,Unspecified unreleased,"""The model is pre-trained on a diverse dataset that includes image-text pairs, optical character recognition (OCR) data, interleaved image-text articles, visual question answering datasets, video dialogues, and image knowledge datasets. Our data sources primarily comprise cleaned web pages, open-source datasets, and synthetic data. The cutoff date for our data knowledge is June 2023.""",1400000000000.0,"""Throughout the pre-training stages, Qwen2-VL processes a cumulative total of 1.4 trillion tokens. Specifically, these tokens encompass not only text tokens but also image tokens""",,,,Likely,"We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at \url{this https URL}.",China,,,,,,https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct,Industry
Qwen2.5-Coder (7B),Language,"Code generation,Code autocompletion,Quantitative reasoning,Question answering,Language modeling/generation","Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An Yang, Rui Men, Fei Huang, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, Junyang Lin",Open weights (unrestricted),https://arxiv.org/abs/2409.12186,,Qwen2.5-Coder Technical Report,2024-09-18,Alibaba,7610000000.0,Number of Parameters: 7.61B,2.5113e+23,6ND = 6*7610000000 parameters *5.5T tokens =2.5113e+23,"GitHub,Common Crawl","""we constructed a dataset named Qwen2.5-Coder-Data. This dataset comprises five key data types: Source Code Data, Text-Code Grounding Data, Synthetic Data, Math Data, and Text Data.""",5500000000000.0,,,,,Confident,"In this report, we introduce the Qwen2.5-Coder series, a significant upgrade from its predecessor, CodeQwen1.5. This series includes two models: Qwen2.5-Coder-1.5B and Qwen2.5-Coder-7B. As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning, scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder demonstrates impressive code generation capabilities while retaining general versatility. The model has been evaluated on a wide range of code-related tasks, achieving state-of-the-art (SOTA) performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair, consistently outperforming larger models of the same model size. We believe that the release of the Qwen2.5-Coder series will not only push the boundaries of research in code intelligence but also, through its permissive licensing, encourage broader adoption by developers in real-world applications.",China,,,,,,"Apache 2.0 
https://huggingface.co/Qwen/Qwen2.5-Coder-7B",Industry
Qwen2.5-32B,Language,"Language modeling/generation,Question answering,Quantitative reasoning",Qwen Team,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen2.5/ ,,Qwen2.5: A Party of Foundation Models!,2024-09-17,Alibaba,32500000000.0,32.5B,3.51e+24,6 * 32.5B parameters * 18 trillion tokens = 3.51 × 10^24,Unspecified unreleased,,18000000000000.0,"""In terms of Qwen2.5, the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens""",,,,Confident,"In the past three months since Qwen2’s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5. We are announcing what might be the largest opensource release in history! Let’s get the party started!

The Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks, despite having fewer non-embedding parameters. It demonstrates significant improvements across various tasks, achieving 74.2 on general benchmarks like MMLU, 49.8 on math challenges such as MATH, and 57.9 on coding tasks like HumanEval.",China,,,,,Unreleased,"Apache 2.0
https://huggingface.co/Qwen/Qwen2.5-32B",Industry
o1-mini,Language,"Code generation,Language modeling/generation,Quantitative reasoning,Chat,Question answering,Translation",,API access,https://openai.com/index/learning-to-reason-with-llms/,,Learning to reason with LLMs,2024-09-12,OpenAI,,,,,Unspecified unreleased,"""Both models were trained on a variety of publicly available datasets, including web data and open-source datasets. Key components include reasoning data and scientific literature. <..> To further enhance the capabilities of o1-preview and o1-mini, we formed partnerships to access high-value non-public datasets. These proprietary data sources include paywalled content, specialized archives, and other domain-specific datasets that provide deeper insights into industry-specific knowledge and use cases.""",,,,,,Unknown,"We've developed a new series of AI models designed to spend more time thinking before they respond. They can reason through complex tasks and solve harder problems than previous models in science, coding, and math.

...

We’re also releasing OpenAI o1-mini, a faster, cheaper reasoning model that is particularly effective at coding. As a smaller model, o1-mini is 80% cheaper than o1-preview, making it a powerful, cost-effective model for applications that require reasoning but not broad world knowledge.",United States of America,,,,,Unreleased,,Industry
o1-preview,"Language,Mathematics,Biology","Code generation,Language modeling/generation,Quantitative reasoning,Chat,Question answering,Translation",,API access,https://openai.com/index/introducing-openai-o1-preview/,,A new series of reasoning models for solving hard problems.,2024-09-12,OpenAI,,,,,Unspecified unreleased,"""Both models were trained on a variety of publicly available datasets, including web data and open-source datasets. Key components include reasoning data and scientific literature. <..> To further enhance the capabilities of o1-preview and o1-mini, we formed partnerships to access high-value non-public datasets. These proprietary data sources include paywalled content, specialized archives, and other domain-specific datasets that provide deeper insights into industry-specific knowledge and use cases.""",,,,,,Unknown,"We've developed a new series of AI models designed to spend more time thinking before they respond. They can reason through complex tasks and solve harder problems than previous models in science, coding, and math.

Today, we are releasing the first of this series in ChatGPT and our API. This is a preview and we expect regular updates and improvements. Alongside this release, we’re also including evaluations for the next update, currently in development.",United States of America,,,,,Unreleased,,Industry
DeepSeek-V2.5,Language,"Language modeling/generation,Chat,Code generation","DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J.L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R.J. Chen, R.L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S.S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W.L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X.Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun",Open weights (restricted use),https://huggingface.co/deepseek-ai/DeepSeek-V2.5,,DeepSeek-V2.5,2024-09-06,DeepSeek,236000000000.0,"21B active params, 236B total",1.7892e+24,"V2.5 is a merge of V2-coder and V2-chat
V2-coder is trained for 6T additional tokens from an intermediate checkpoint of V2, which had been trained for 4.2T tokens. Total: 10.2T
V2-chat is fine-tuned from V2, saw 8.2T tokens in pre-training
Unique steps: 8.2T + 6T = 14.2T
FLOPs: 6 * 21B * 14.2T = 1.7892e24","GitHub,Common Crawl",,,"The original V2 had a dataset of 8.1T unique tokens, and coder-V2 added an additional 1.391T unique tokens of code and math. But it appears no additional training was done to combine them into this model.",,,,Confident,,China,,,,,Unreleased,,Industry
Moonshot-v1,Language,Language modeling/generation,Moonshot,,,,,2024-09-01,Moonshot,,,,,,,,,,,,Unverified,,China,,,,,,,Industry
GLM-4-Plus,Language,Language modeling,Zhipu AI,API access,https://bigmodel.cn/dev/howuse/glm-4,,GLM-4-Plus,2024-08-29,Zhipu AI,,,3.5999999999999997e+25,Estimated using benchmark imputation,,,,,,,,Unknown,"At the KDD International Conference on Data Mining and Knowledge Discovery, the Zhipu GLM team unveiled the new generation of base large model—GLM-4-Plus. As the latest version of Zhipu’s fully self-developed GLM large model, GLM-4-Plus signifies Zhipu AI’s continuous dedication in the field of general artificial intelligence, advancing the independent and autonomous innovation of large model technology.",China,,,,,,,Industry
Pharia-1-LLM-7B,Language,"Language modeling/generation,Translation,Question answering",,Open weights (non-commercial),https://huggingface.co/Aleph-Alpha/Pharia-1-LLM-7B-control,,Introducing Pharia-1-LLM: transparent and compliant,2024-08-26,Aleph Alpha,7041544704.0,,4.43e+23,"reported by the authors: 2.75*10^23 + 1.68*10^23 = 4.43*10^23 FLOP

https://huggingface.co/Aleph-Alpha/Pharia-1-LLM-7B-control#compute--training-efficiency",Common Crawl,"The training data of our models comprises two components: web-crawled data and structured datasets with a total size of 7.7T, with a cutoff date 04/2023. We performed some additional web scraping to augment these datasets.

Web-crawled data was obtained by filtering and deduplicating data available in public datasets, derived from Common Crawl, in the following languages: English, French, German, Italian, Spanish, Dutch, Portuguese.

To deduplicate the data, we applied a Bloomfilter for exact document deduplication in English, French, German, Italian and Spanish. Portuguese and Dutch data was deduplicated using both URLs and fuzzy-deduplication with MinHashLSH.",7700000000000.0,4.7T + 3T = 7.7T tokens,,,"NVIDIA A100 SXM4 80 GB,NVIDIA H100 SXM5 80GB",Confident,"We are pleased to announce our new foundation model family that includes Pharia-1-LLM-7B-control and Pharia-1-LLM-7B-control-aligned, now publicly available under the Open Aleph License, which explicitly allows for non-commercial research and educational use. Pharia-1-LLM-7B-control is engineered to deliver concise, length-controlled responses that match the performance of leading open-source models in the 7B to 8B parameter range and is culturally and linguistically optimized for German, French, and Spanish by being trained on a multilingual base corpus. Pharia-1-LLM-7B-control is trained on carefully curated data in compliance with applicable EU and national regulations, including copyright and data privacy laws. With improved token efficiency, Pharia-1-LLM-7B-control excels in domain-specific applications, particularly in the automotive and engineering industries, and can be aligned to user preferences, making it suitable for critical applications without the risk of shutdown behavior. As such, it serves as a valuable addition to the community’s selection of weight-available foundation models. Pharia-1-LLM-7B-control-aligned has been added with additional safety guardrails via alignment methods.",Germany,,,,256.0,,,Industry
Grok-2,"Language,Vision,Multimodal","Chat,Language modeling/generation,Question answering,Code generation,Visual question answering",,Hosted access (no API),https://x.ai/blog/grok-2,,Grok-2 Beta Release,2024-08-13,xAI,,,2.9599999999999996e+25,Estimate based on xAI statements comparing Grok-2 compute to GPT-4 and Grok-3. Full estimate here: https://docs.google.com/document/d/1C_dABuZrAqYE_ui4_GZ4bRLtq3TBjIGoBSktaPElhEU/edit?usp=sharing,Unspecified unreleased,,,,,,NVIDIA H100 SXM5 80GB,Confident,Grok-2 is our frontier language model with state-of-the-art reasoning capabilities. This release includes two members of the Grok family: Grok-2 and Grok-2 mini. Both models are now being released to Grok users on the 𝕏 platform.,United States of America,,,,,Unreleased,,Industry
EXAONE 3.0,Language,"Language modeling/generation,Code generation,Question answering","LG AI Research: Soyoung An, Kyunghoon Bae, Eunbi Choi, Stanley Jungkyu Choi, Yemuk Choi, Seokhee Hong, Yeonjung Hong, Junwon Hwang, Hyojin Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Yountae Jung, Euisoon Kim, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Moontae Lee, Seungjun Lee, Woohyung Lim, Sangha Park, Sooyoun Park, Yongmin Park, Boseong Seo, Sihoon Yang, Heuiyeen Yeen, Kyungjae Yoo, Hyeongu Yun",Open weights (non-commercial),https://arxiv.org/abs/2408.03541,,EXAONE 3.0 7.8B Instruction Tuned Language Model,2024-08-07,LG AI Research,7820000000.0,7.8B,4e+23,"6ND = 6*7.8B parameters *8T tokens  = 3.744e+23 FLOP

""EXAONE language models were trained using Google Cloud Platform and a cluster powered by NVIDIA H100 GPUs and NVIDIA NeMo Framework. Then, they were optimized by NVIDIA TensorRT-LLM. The total amount of computation used for model training was about 4 × 1023 FLOPS""",Unspecified unreleased,"8T training data (tokens)

the token per word ration is 2.46 and given in the paper",8000000000000.0,,,,NVIDIA H100 SXM5 80GB,Confident,"We introduce EXAONE 3.0 instruction-tuned language model, the first open model in the family of Large Language Models (LLMs) developed by LG AI Research. Among different model sizes, we publicly release the 7.8B instruction-tuned model to promote open research and innovations. Through extensive evaluations across a wide range of public and in-house benchmarks, EXAONE 3.0 demonstrates highly competitive real-world performance with instruction-following capability against other state-of-the-art open models of similar size. Our comparative analysis shows that EXAONE 3.0 excels particularly in Korean, while achieving compelling performance across general tasks and complex reasoning. With its strong real-world effectiveness and bilingual proficiency, we hope that EXAONE keeps contributing to advancements in Expert AI. Our EXAONE 3.0 instruction-tuned model is available at this https URL",Korea (Republic of),,,,,,"https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct

Exaone license (allows only non-commercial usage)",Industry
Midjourney V6.1,Image generation,Image generation,,Hosted access (no API),https://updates.midjourney.com/version-6-1/,,,2024-07-30,Midjourney,,,,,Unspecified unreleased,,,,,,,Unknown,"More coherent images (arms, legs, hands, bodies, plants, animals, etc)
Much better image quality (reduced pixel artifacts, enhanced textures, skin, etc)
More precise, detailed, and correct small image features (eyes, small faces, far away hands, etc)
New 2x upscalers with much better image / texture quality
Roughly 25% faster for standard image jobs
Improved text accuracy (when drawing words via “quotations” in prompts)
A new personalization model with improved nuance, surprise, and accuracy
Personalization code versioning (use any personalization code from old jobs to use the personalization model and data from that job)
A new --q 2 mode which takes 25% longer to (sometimes) add more texture at the cost of reduced image coherence
Things should look “generally more beautiful” across the board",United States of America,,,,,Unreleased,,Industry
AFM-on-device,Language,Language modeling/generation,"Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chong Wang, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Ruoming Pang, Sam Wiseman, Syd Evans, Tao Lei, Tom Gunter, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Zirui Wang, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann Anupama, Areeba
Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg,
Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd,
Fangping Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah
Gillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Ke Ye, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Mark Lee, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xiang Kong, Xin Wang, Xin Zheng, Walker Cheng, Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yunsong Meng, Zhao Tang Luo, Zhi Ouyang, Zhiyun Lu, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy Narayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Chong Wang, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Guoli Yin, Irina Belousova, Jianyu Wang, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao, Mahyar Najibi , Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qibin Chen, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar, Vivek Rathod, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xiang Kong, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, Zhongzheng Ren",Hosted access (no API),https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models,,Apple Intelligence Foundation Language Models,2024-07-29,Apple,2730000000.0,"Table 1, sum of non-embedding and embedding parameters",4.5126e+23,"Model was initialized from a pruned version of a 6.4B parameter model trained using the same recipe as AFM-server. Assuming ""same recipe"" involves training for the full 6.3T tokens, this implies 6 * 6.3T * 6.4B = 2.42e23 FLOP. 

The pruning masks are learned by training over 188B tokens, which suggests 6 * 188B * 6.4B = 7.22e21 FLOPs.

Pretraining is then run over 6.3T tokens; however, labels are a convex combination of true labels and the predicted labels from the unpruned 6.4B model. Since this involves running the 6.3T tokens forward through both the 6.4B and the 2.73B model, but only calculating gradients for the smaller model, FLOPs here are equal to (6 * 6.3T * 2.73B) + (2 * 6.3T * 6.4B) = 1.84e23. 

Finally, there is a 1T ""continuation"" pretraining stage without distillation loss, for 6 * 1T * 2.73B = 1.64e22 FLOP, and a 100B context-lengthening stage for another 6 * 100B * 2.73B = 1.64e21 FLOP

In total: 2.42e23 + 7.22e21 + 1.84e23 + 1.64e22 + 1.64e21 = 4.51e23 FLOP",,"188B of tokens are used to train a pruning mask to reduce a 6.4B model to the 2.73B used for AFM-on-device. Main pre-training data is 6.3T tokens of web text, code, and math, plus another 1T in the second pre-training stage and 100B in the third. See section 3.1 for details. Post-training details do not give details on dataset size.",7588000000000.0,"Not explicitly mentioned, but I assume the 7.588T tokens do not involve multiple epochs.",,"Trained on ""one slice of 2048 TPUv5p chips""; wall-time not given.",Google TPU v5p,Confident,"We present foundation language models developed to power Apple Intelligence features, including a ∼3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute [Apple, 2024b]. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",United States of America,,,,2048.0,Unreleased,,Industry
AFM-server,Language,Language modeling/generation,"Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chong Wang, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Ruoming Pang, Sam Wiseman, Syd Evans, Tao Lei, Tom Gunter, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Zirui Wang, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann Anupama, Areeba
Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg,
Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd,
Fangping Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah
Gillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Ke Ye, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Mark Lee, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xiang Kong, Xin Wang, Xin Zheng, Walker Cheng, Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yunsong Meng, Zhao Tang Luo, Zhi Ouyang, Zhiyun Lu, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy Narayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Chong Wang, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Guoli Yin, Irina Belousova, Jianyu Wang, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao, Mahyar Najibi , Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qibin Chen, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar, Vivek Rathod, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xiang Kong, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, Zhongzheng Ren",Hosted access (no API),https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models,,Apple Intelligence Foundation Language Models,2024-07-29,Apple,,,4.3e+24,"""The AFM base models are dense decoder-only models that build on the
Transformer architecture""

""We train AFM-server from scratch for 6.3T tokens on 8192
TPUv4 chips, using a sequence length of 4096 and a batch-size of 4096 sequences.""

""For both models we perform continued pre-training at a sequence length of
8192, with another 1T tokens from a mixture that upweights math and code,
and down-weights the bulk web-crawl.""

""The sustained model-flop-utilization (MFU) for this training run was approximately 52%.""

Parameter count is not specified other than it being ""larger"" than 3 billion.

Counting FLOP: Chinchilla scaling laws would suggest 7.3T / 20 = 365B parameters. 

365B parameters * 7.3T tokens * 6 ~= 1.6e25 FLOP.

However, the attention to inference optimization in the technical report suggests a smaller size, even for this ""server"" model. One point of reference is Llama 3 70B being overtrained by a factor of 10. If this is true of AFM-server, the parameter count would be ~37B and training compute would be 1.6e24 FLOP.

GPU-time: assume a wall-clock training time of 30 days based on the current trend value for notable models.

8192 chips * 275e12 FLOP/s per chip * 0.52 utilization * 30 * 24 * 60 * 60 s ~= 3.0e24 FLOP

The geometric mean of these three estimates is 4.3e24 FLOP.",,"6.3T tokens of web text, code, and math, plus another 1T in the second stage and 100B in the third. See section 3.1 for details.",7400000000000.0,"Not explicitly mentioned, but I assume the 7.4T tokens do not involve multiple epochs.",,,Google TPU v4,Likely,"We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.",United States of America,,,,8192.0,Unreleased,,Industry
Mistral Large 2,Language,"Language modeling/generation,Translation,Code generation","Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Alok Kothari, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Bam4d, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Carole Rambaud, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Diogo Costa, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gaspard Blanchet, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Hichem Sattouf, Ian Mack, Jean-Malo Delignon, Jessica Chudnovsky, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, Mickaël Seznec, Nicolas Schuhl, Niklas Muhs, Olivier de Garrigues, Patrick von Platen, Paul Jacob, Pauline Buche, Pavan Kumar Reddy, Perry Savas, Pierre Stock, Romain Sauvestre, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibault Schueller, Thibaut Lavril, Thomas Wang, Théophile Gervet, Timothée Lacroix, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",Open weights (non-commercial),https://mistral.ai/news/mistral-large-2407/,,"Top-tier reasoning for high-complexity tasks, for your most sophisticated needs.",2024-07-24,Mistral AI,123000000000.0,,2.13e+25,"Details are sparse, but we can hazard a guess based on evidence about the training cluster they may have used, the scale up in compute they likely would have used relative to Mistral Large 1, and from the model's MMLU score. Extended reasoning given here: https://docs.google.com/document/d/1I2ZWBLFMpRZYcdMMUfKAGZFJrOJpduNDS9ZeVFIHnd8/edit?usp=sharing",Unspecified unreleased,,,,,,,Likely,"Today, we are announcing Mistral Large 2, the new generation of our flagship model. Compared to its predecessor, Mistral Large 2 is significantly more capable in code generation, mathematics, and reasoning. It also provides a much stronger multilingual support, and advanced function calling capabilities.",France,,,,,Unreleased,"""We are releasing Mistral Large 2 under the Mistral Research License, that allows usage and modification for research and non-commercial usages. For commercial usage of Mistral Large 2 requiring self-deployment, a Mistral Commercial License must be acquired by contacting us.""",Industry
Llama 3.1-405B,Language,Language modeling/generation,"Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,
Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie
Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen
Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux,
Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang
Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle
Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino,
Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip
Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire
Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,
Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert,
Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong,
Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe
Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala,
Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer,
Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence
Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat,
Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas,
Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar
Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji,
Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng,
Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong,
Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta
Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor,
Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun
Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan,
Shruti Bhosale, Shun Zhang, Simon Vandenhende, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin
Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas
Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta,
Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei
Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan,
Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen
Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, and Zoe
Papakipos.
(core contributors)",Open weights (restricted use),https://ai.meta.com/research/publications/the-llama-3-herd-of-models/,,The Llama 3 Herd of Models,2024-07-23,Meta AI,405000000000.0,405B,3.8e+25,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",Llama 3 dataset,,15600000000000.0,15.6T tokens,2142.0,"Trained on 30.84M GPU hours (https://huggingface.co/blog/llama31) and used ""up to 16K H100 GPU[s]"" so training took at least
30.84M / 16k = 1927.5 hours or ~80 days. 

Section 3.3.4 gives reliability details over a 54 day period during training, for which they had ""higher than 90% effective training time""
1927.5 / 0.9 = 2142 hours

Probably, full training time is somewhat longer, since it sounds like there were periods where not all 16k H100s were running.",NVIDIA H100 SXM5 80GB,Confident,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",United States of America,,,,16384.0,Open (restricted use),"Llama 3.1 model license:

https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/blob/main/LICENSE 

must seek separate license if over 700m monthly users, acceptable use restrictions

training code here: https://github.com/meta-llama/llama-recipes/blob/main/src/llama_recipes/utils/train_utils.py#L70 
",Industry
Llama 3.1-70B,Language,Language modeling/generation,"Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,
Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie
Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen
Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux,
Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang
Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle
Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino,
Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip
Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire
Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,
Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert,
Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong,
Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe
Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala,
Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer,
Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence
Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat,
Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas,
Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar
Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji,
Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng,
Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong,
Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta
Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor,
Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun
Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan,
Shruti Bhosale, Shun Zhang, Simon Vandenhende, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin
Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas
Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta,
Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei
Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan,
Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen
Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, and Zoe
Papakipos.
(core contributors)",Open weights (restricted use),https://ai.meta.com/research/publications/the-llama-3-herd-of-models/,,The Llama 3 Herd of Models,2024-07-23,Meta AI,70000000000.0,70B,7.929e+24,"Huggingface page says 3.1-70B used 7.0M H100 hours and trained over 15T tokens. https://huggingface.co/meta-llama/Llama-3.1-70B
The paper also says that 3.1-405B got MFU of between 38-43%; presumably 70B was around the same or a bit higher. I'll assume utilization of 40%

6ND:
6 * 15T * 70B = 6.3e24 FLOPs

Hardware:
7M * 9.9e14 * 3600 * 0.4 = 9.98e24 FLOPs

Geometric mean: sqrt(6.3e24 * 9.98e24) = 7.929e24

Note that Llama 3-70B also said it used 15T tokens, but only 6.4M H100 hours. This suggests 3.1 might have used a bit more than 15T tokens.

Training compute upper bound: 7M H100-hours * 989 TFLOPS * 50% utilization = 1.25e25 FLOP",Llama 3 dataset,,,,,,,Confident,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",United States of America,,,,,Open (restricted use),"Llama 3.1 license:

https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/blob/main/LICENSE 

must seek separate license if over 700m monthly users, acceptable use restrictions

code here: https://github.com/meta-llama/llama3/tree/main 
",Industry
Llama 3.1-8B,Language,Language modeling/generation,"Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,
Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie
Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen
Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux,
Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang
Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle
Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino,
Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip
Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire
Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,
Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert,
Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong,
Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe
Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala,
Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer,
Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence
Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat,
Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas,
Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar
Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji,
Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng,
Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong,
Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta
Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor,
Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun
Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan,
Shruti Bhosale, Shun Zhang, Simon Vandenhende, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin
Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas
Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta,
Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei
Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan,
Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen
Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, and Zoe
Papakipos.
(core contributors)",Open weights (restricted use),https://ai.meta.com/research/publications/the-llama-3-herd-of-models/,,The Llama 3 Herd of Models,2024-07-23,Meta AI,8000000000.0,8B,1.224e+24,"Huggingface page says 3.1-8B used 1.46M H100 hours and trained over 15T tokens. https://huggingface.co/meta-llama/Llama-3.1-70B
The paper also says that 3.1-405B got MFU of between 38-43%; presumably 8B was around the same or a bit higher. I'll assume utilization of 40%

6ND:
6 * 15T * 8B = 7.2e23 FLOPs

Hardware:
1.46M * 9.9e14 * 3600 * 0.4 = 2.08e24 FLOPs

Geometric mean: sqrt(7.2e23 * 2.08e24) = 1.224e24

Note that Llama 3-8B also said it used 15T tokens, but only 1.3M H100 hours. This suggests 3.1 might have used a bit more than 15T tokens.",Llama 3 dataset,,,,,,,Unverified,"Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",United States of America,,,,,Open (restricted use),"Llama 3.1 license:

https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/blob/main/LICENSE 

must seek separate license if over 700m monthly users, acceptable use restrictions

code here: https://github.com/meta-llama/llama3/tree/main 
",Industry
GPT-4o mini,"Language,Multimodal,Vision","Chat,Language modeling/generation,Code generation,Visual question answering","Pre-training leads
Aidan Clark, Alex Paino, Jacob Menick

Post-training leads
Liam Fedus, Luke Metz

Architecture leads
Clemens Winter, Lia Guy

Optimization leads
Sam Schoenholz, Daniel Levy

Long-context lead
Nitish Keskar

Pre-training Data leads
Alex Carney, Alex Paino, Ian Sohl, Qiming Yuan

Tokenizer lead
Reimar Leike

Human data leads
Arka Dhar, Brydon Eastman, Mia Glaese

Eval lead
Ben Sokolowsky

Data flywheel lead
Andrew Kondrich

Inference lead
Felipe Petroski Such

Inference Productionization lead
Henrique Ponde de Oliveira Pinto

Post-training infrastructure leads
Jiayi Weng, Randall Lin, Youlong Cheng

Pre-training organization lead
Nick Ryder

Pre-training program lead
Lauren Itow

Post-training organization leads
Barret Zoph, John Schulman

Post-training program lead
Mianna Chen

Core contributors
Adam Lerer, Adam P. Goucher, Adam Perelman, Akila Welihinda, Alec Radford, Alex Borzunov, Alex Carney, Alex Chow, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexi Christakis, Ali Kamali, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoonchian, Ananya Kumar, Andrej Karpathy, Andrey Mishchenko, Andrew Cann, Andrew Kondrich, Andrew Tulloch, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Avi Nayak, Avital Oliver, Behrooz Ghorbani, Ben Leimberger, Ben Wang, Beth Hoover, Blake Samic, Brian Guarraci, Brydon Eastman, Camillo Lugaresi, Chak Li, Charlotte Barette, Chelsea Voss, Chen Ding, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christine Choi, Christopher Hesse, Colin Wei, Daniel Kappler, Daniel Levin, Daniel Levy, David Farhi, David Mely, David Sasaki, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Peterson, Eric Sigler, Eugene Brevdo, Farzad Khorasani, Francis Zhang, Gene Oden, Geoff Salmon, Hadi Salman, Haiming Bao, Heather Schmidt, Hongyu Ren, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ibrahim Okuyucu, Ilya Kostrikov, Ingmar Kanitscheider, Jacob Coxon, James Crooks, James Lennon, Jane Park, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jiayi Weng, Jie Tang, Joanne Jang, Jonathan Ward, Jonathan McKay, Jong Wook Kim, Josh Gross, Josh Kaplan, Joy Jiao, Joyce Lee, Juntang Zhuang, Kai Fricke, Kavin Karthik, Kenny Hsu, Kiel Howe, Kyle Luther, Larry Kai, Lauren Itow, Leo Chen, Lia Guy, Lien Mamitsuka, Lilian Weng, Long Ouyang, Louis Feuvrier, Lukas Kondraciuk, Lukasz Kaiser, Lyric Doshi, Mada Aflak, Maddie Simens, Madeleine Thompson, Marat Dukhan, Marvin Zhang, Mateusz Litwin, Matthew Zeng, Max Johnson, Mayank Gupta, Mia Glaese, Michael Janner, Michael Petrov, Michael Wu, Michelle Fradin, Michelle Pokrass, Miguel Oom Temudo de Castro, Mikhail Pavlov, Minal Khan, Mo Bavarian, Murat Yesildal, Natalia Gimelshein, Natalie Staudacher, Nick Stathas, Nik Tezak, Nithanth Kudige, Noel Bundick, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivier Godement, Owen Campbell-Moore, Philip Pronin, Philippe Tillet, Rachel Lim, Rajan Troll, Randall Lin, Rapha gontijo lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Rob Honsby, Rohit Ramchandani, Rory Carmichael, Ruslan Nigmatullin, Ryan Cheu, Sara Culver, Scott Gray, Sean Grove, Sean Metzger, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shuaiqi (Tony) Xia, Sonia Phene, Spencer Papay, Steve Coffey, Steve Lee, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tarun Gogineni, Ted Sanders, Thomas Cunninghman, Thomas Dimson, Thomas Raoux, Tianhao Zheng, Christina Kim, Todd Underwood, Tristan Heywood, Valerie Qi, Vinnie Monaco, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wojciech Zaremba, Yash Patil, Yilei, Qian, Yongjik Kim, Youlong Cheng, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov",API access,https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/,,GPT-4o mini: advancing cost-efficient intelligence,2024-07-18,OpenAI,,,7.360010000000001e+24,"Training compute estimated from benchmark scores.

90% CI [3.23e+24, 2.05e+25]",Unspecified unreleased,,,,,,,Speculative,"OpenAI is committed to making intelligence as broadly accessible as possible. Today, we're announcing GPT-4o mini, our most cost-efficient small model. We expect GPT-4o mini will significantly expand the range of applications built with AI by making intelligence much more affordable. GPT-4o mini scores 82% on MMLU and currently outperforms GPT-41 on chat preferences in LMSYS leaderboard(opens in a new window). It is priced at 15 cents per million input tokens and 60 cents per million output tokens, an order of magnitude more affordable than previous frontier models and more than 60% cheaper than GPT-3.5 Turbo.",United States of America,,,,,Unreleased,,Industry
ESM3 (98B),Biology,Protein generation,"Thomas Hayes, Roshan Rao, Halil Akin, Nicholas James Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Quy Tran, Jonathan Deaton, Marius Wiggert, Rohil Badkundri, Irhum Shafkat, Jun Gong, Alexander Derry, Raul Santiago Molina, Neil Thomas, Yousuf Khan, Chetan Mishra, Carolyn Kim, Liam J Bartie, Patrick D Hsu, Tom Sercu, Salvatore Candido, Alexander Rives",Unreleased,https://www.evolutionaryscale.ai/blog/esm3-release ,,ESM3: Simulating 500 million years of evolution with a language model,2024-06-25,"EvolutionaryScale,University of California (UC) Berkeley",98500000000.0,98.5 billion (Table S1),1.07e+24,"""ESM3 at its largest scale was trained with 1.07×10^24 FLOPs on 2.78 billion proteins and 771 billion unique tokens, and has 98 billion parameters.""

per Table 1, trained 98B model on 1.8T training tokens. 98 billion * 1800 billion * 6 = 1.06e24. Likely some rounding, so will go with developer's reported count.",ESM3 Dataset,,771000000000.0, 771 billion tokens,,,,Confident,"More than three billion years of evolution have
produced an image of biology encoded into the
space of natural proteins. Here we show that language models trained on tokens generated by evolution can act as evolutionary simulators to generate functional proteins that are far away from
known proteins. We present ESM3, a frontier
multimodal generative language model that reasons over the sequence, structure, and function
of proteins. ESM3 can follow complex prompts
combining its modalities and is highly responsive
to biological alignment. We have prompted ESM3
to generate fluorescent proteins with a chain of
thought. Among the generations that we synthesized, we found a bright fluorescent protein at far
distance (58% identity) from known fluorescent
proteins. Similarly distant natural fluorescent proteins are separated by over five hundred million
years of evolution,

(from paper preview: https://evolutionaryscale-public.s3.us-east-2.amazonaws.com/research/esm3.pdf )","United States of America,United States of America",,,,,Unreleased,only small version released,"Industry,Academia"
Gemma 2 9B,Language,"Language modeling/generation,Chat,Code generation,Question answering,Quantitative reasoning","Gemma Team, Google DeepMind",Open weights (restricted use),https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools.",2024-06-24,Google DeepMind,9000000000.0,,4.32e+23,"""For the 9B model, we train on an 8x16x32 configuration of TPUv4, totaling 4096 chips""

6ND = 6 FLOP / token / parameter * 9000000000 parameters * 8000000000000 tokens = 4.32e+23 FLOP",Unspecified unreleased,"Web Documents: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. Primarily English-language content.
Code: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code or understand code-related questions.
Mathematics: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries.",8000000000000.0,"""the 9B model on 8 trillion tokens""",,,Google TPU v4,Confident,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.","United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,4096.0,Unreleased,"Gemma 2 is available under our 
commercially-friendly Gemma license, giving developers and researchers the ability to share and commercialize their innovations.",Industry
Gemma 2 27B,Language,"Language modeling/generation,Chat,Code generation,Question answering,Quantitative reasoning","Gemma Team, Google DeepMind",Open weights (restricted use),https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf,,"Gemma 2 offers best-in-class performance, runs at incredible speed across different hardware and easily integrates with other AI tools.",2024-06-24,Google DeepMind,27000000000.0,,2.106e+24,"""For the 27B model, we train on an 8x24x32 configuration of
TPUv5p, totaling 6144 chips""

trained on 13T tokens

6ND = 6*27000000000*13000000000000=2.106e+24",Unspecified unreleased,"Web Documents: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. Primarily English-language content.
Code: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code or understand code-related questions.
Mathematics: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries.",13000000000000.0,"""We train Gemma 2 27B on 13 trillion tokens of primarily-English data""",,,Google TPU v5p,Confident,"Now we’re officially releasing Gemma 2 to researchers and developers globally. Available in both 9 billion (9B) and 27 billion (27B) parameter sizes, Gemma 2 is higher-performing and more efficient at inference than the first generation, with significant safety advancements built in. In fact, at 27B, it offers competitive alternatives to models more than twice its size, delivering the kind of performance that was only possible with proprietary models as recently as December. And that’s now achievable on a single NVIDIA H100 Tensor Core GPU or TPU host, significantly reducing deployment costs.","United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,6144.0,Unreleased,"Gemma 2 is available under our 
commercially-friendly Gemma license, giving developers and researchers the ability to share and commercialize their innovations.",Industry
Claude 3.5 Sonnet,"Multimodal,Language,Vision","Chat,Image captioning,Code generation,Language modeling/generation",,API access,https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf,,Claude 3.5 Sonnet,2024-06-20,Anthropic,,,2.700000000000001e+25,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",Unspecified unreleased,Training data cutoff Apr 2024,,,,,,Speculative,,United States of America,,,,,Unreleased,,Industry
GLM-4V-9B,"Language,Multimodal,Vision","Language modeling/generation,Code generation,Question answering,Translation,Visual question answering","Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang",Open weights (non-commercial),https://arxiv.org/abs/2406.12793,,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools,2024-06-18,"Zhipu AI,Tsinghua University",9000000000.0,9B,,"6ND = 6*9000000000*10000000000000=54*10^(9+13)=5.4*10^23

""GLM-4-9B is pre-trained on approximately ten trillion tokens of multilingual corpus with a context length of 8192 (8K) and post-trained with the same pipeline and data used for GLM-4 (0520)"".
",,"""To date, the GLM-4 models are
pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage""",10000000000000.0,"""GLM-4-9B is pre-trained on approximately ten trillion tokens of multilingual corpus with a context length of 8192 (8K) and post-trained with the same pipeline and data used for GLM-4 (0520)"".",,,,Likely,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.","China,China",,,,,,https://github.com/THUDM/GLM-4/blob/main/README_en.md,"Industry,Academia"
Nemotron-4 340B,Language,"Language modeling/generation,Chat,Question answering","Bo Adler, Niket Agarwal, Ashwath Aithal, Dong H. Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, Sirshak Das, Ayush Dattagupta, Olivier Delalleau, Leon Derczynski, Yi Dong, Daniel Egert, Ellie Evans, Aleksander Ficek, Denys Fridman, Shaona Ghosh, Boris Ginsburg, Igor Gitman, Tomasz Grzegorzek, Robert Hero, Jining Huang, Vibhu Jawa, Joseph Jennings, Aastha Jhunjhunwala, John Kamalu, Sadaf Khan, Oleksii Kuchaiev, Patrick LeGresley, Hui Li, Jiwei Liu, Zihan Liu, Eileen Long, Ameya Sunil Mahabaleshwarkar, Somshubra Majumdar, James Maki, Miguel Martinez, Maer Rodrigues de Melo, Ivan Moshkov, Deepak Narayanan, Sean Narenthiran, Jesus Navarro, Phong Nguyen, Osvald Nitski, Vahid Noroozi, Guruprasad Nutheti, Christopher Parisien, Jupinder Parmar, Mostofa Patwary, Krzysztof Pawelec, Wei Ping, Shrimai Prabhumoye, Rajarshi Roy, Trisha Saar, Vasanth Rao Naik Sabavat, Sanjeev Satheesh, Jane Polak Scowcroft, Jason Sewall, Pavel Shamis, Gerald Shen, Mohammad Shoeybi, Dave Sizer, Misha Smelyanskiy, Felipe Soares, Makesh Narsimhan Sreedhar, Dan Su, Sandeep Subramanian, Shengyang Sun, Shubham Toshniwal, Hao Wang, Zhilin Wang, Jiaxuan You, Jiaqi Zeng, Jimmy Zhang, Jing Zhang, Vivienne Zhang, Yian Zhang, Chen Zhu",Open weights (unrestricted),https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/ ,,NVIDIA Releases Open Synthetic Data Generation Pipeline for Training Large Language Models,2024-06-14,NVIDIA,340000000000.0,340B,1.7999999999999999e+25,"9 trillion tokens for training
6 * 340B * 9T = 1.8E25

alternatively, can do a hardware estimate with a few extra steps:

According to the technical report, Nemotron-4 340B was trained using up to 6144 H100 GPUs. Helpfully, they also report the model FLOP utilization (MFU), which was 41-42% (Table 2). This is the ratio of the actual output of their GPUs, in FLOP used for training, relative to their theoretical max of 989 teraFLOP/s per GPU. 
Unfortunately, the report omits the last ingredient, which is the duration of the training run. However, in Table 2 they report some relevant data that we can use to infer the training time. 
Nemotron-4 was trained in several stages, but the largest stage used all 6144 GPUs with a batch size of 2304 and an iteration time (time per batch) of 8.0 seconds. This stage involved 7.6T tokens, so it makes up the majority of training. 
A batch size of 2304 means that each batch consists of 2304 sequences, and they report that the sequence length used for training was 4096 tokens. This means that each batch contained 4096 * 2304 = 9,437,184 tokens. 
So, during this stage, it took 8 seconds to train the model on 9.4m tokens. Extrapolating to the entire 9T token dataset, this implies the training run would have taken 7,659,574 seconds, or 89 days. (it actually took longer because they didn't use all their GPUs for the whole run) 
Multiplying 7,659,574 seconds by 41% MFU, 989 peak teraFLOP/s for each H100, and 6144 H100s, we get ~1.9e25 FLOP. This is very close to our first estimate. 
",Unspecified unreleased,"The technical report for the 340B model cites the report for the 15B version (https://arxiv.org/pdf/2402.16819 )

from that paper:

""We train Nemotron-4 15B on a pre-training dataset consisting of 8 trillion tokens. At a high-level,
the data blend is split into three different types of data: English natural language data (70%), multilingual
natural language data (15%), and source-code data (15%).
The English corpus consists of curated documents from a variety of sources and domains including web
documents, news articles, scientific papers, books, etc and the distribution used in our pre-training set is
highlighted in Figure 2. The code and multilingual data consists of a diverse set of natural and programming
languages. We find that appropriately sampling tokens from these languages is key to strong accuracies in
these domains. We share the distributions used for both code and multilingual tokens in our pre-training
dataset in Figure 3 and Figure 4 respectively.
In constructing the pre-training corpus, we remove any possible duplicates via document-level exact and
near-deduplication (Jennings et al., 2023). We additionally applied document-level quality filtering across
our corpus using a language-model based filtering approach similar to (Wenzek et al., 2019) in addition to a
series of heuristic filters as described in (Rae et al., 2022) and (Raffel et al., 2020).""",6750000000000.0,"9T training tokens.

They first train on an 8T token dataset and then an additional 1T tokens, it's slightly unclear if that's more data or a partial second epoch

6.75T words using 1 token = 0.75 words",2200.0,"see training compute notes, this is an inferred estimate",NVIDIA H100 SXM5 80GB,Confident,"We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base, Nemotron-4-
340B-Instruct, and Nemotron-4-340B-Reward. Our models are open access under the NVIDIA Open
Model License Agreement, a permissive model license that allows distribution, modification, and use of
the models and its outputs. These models perform competitively to open access models on a wide range
of evaluation benchmarks, and were sized to fit on a single DGX H100 with 8 GPUs when deployed in
FP8 precision. We believe that the community can benefit from these models in various research studies
and commercial applications, especially for generating synthetic data to train smaller language models.
Notably, over 98% of data used in our model alignment process is synthetically generated, showcasing
the effectiveness of these models in generating synthetic data. To further support open research and
facilitate model development, we are also open-sourcing the synthetic data generation pipeline used in
our model alignment process.

(from technical report: https://d1qx31qr3h6wln.cloudfront.net/publications/Nemotron_4_340B_8T_0.pdf )",United States of America,,,,6144.0,Unreleased,Permissive commercial license: https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf ,Industry
PLaMo-100B,Language,Language modeling/generation,Preferred Elements (PFE),API access,https://tech.preferred.jp/ja/blog/plamo-100b/,,"Pre-training of the proprietary LLM ""PLaMo-100B"" with 100 billion parameters",2024-06-14,Preferred Networks Inc,100000000000.0,,1.1999999999999999e+24,6*100B*2T=1.2e24,,,,"""The pre-trained model of PLaMo-100B developed this time was trained on a total of 2T tokens of both Japanese and English text data.""",,,,Unverified,"Preferred Elements (PFE), a subsidiary of Preferred Networks (PFN), has been developing a 100 billion (100B) parameter LLM called ""PLaMo-100B"" since February. The pre-training part of the development of PLaMo-100B was completed in May, so in this article we will introduce the pre-training part of this model.",Japan,,,,,,,Industry
Mamba2-Hybrid,Language,"Language modeling/generation,Question answering","Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, Mohammad Shoeybi, Bryan Catanzaro",Open weights (unrestricted),https://arxiv.org/abs/2406.07887,,An Empirical Study of Mamba-based Language Models,2024-06-12,NVIDIA,8660000000.0,Table 6,1.8186e+23,"6ND = 6*8660000000.00 parameters * 3500000000000 tokens = 1.8186 × 10^23

",Unspecified unreleased,"""We train the models discussed in this report on 1.1T and 3.5T token datasets. Both datasets are predecessors of the dataset used to train Nemotron-4 and are comprised of 70% English, 15% non-English, and 15% code.""

",3500000000000.0,,,,NVIDIA H100 SXM5 80GB,Likely,"Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets. Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project.",United States of America,,,,1024.0,Open source,"https://github.com/NVIDIA/Megatron-LM/tree/ssm/examples/mamba
Apache 2.0

train script: https://github.com/NVIDIA/Megatron-LM/blob/ssm/examples/mamba/train.sh ",Industry
Qwen2-72B,Language,"Chat,Language modeling/generation",Qwen Team,Open weights (unrestricted),"https://qwenlm.github.io/blog/qwen2/ 
https://arxiv.org/abs/2407.10671 ",,Hello Qwen2,2024-06-07,Alibaba,72710000000.0,"72.71B parameters in total, of which 70.21B are non-embedding parameters",3.02e+24,"72 billion params, 7 trillion tokens

6 * 72 billion * 7 trillion ~= 3.02e24",Unspecified unreleased,"""All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens,
covering a wide range of domains and languages. Compared to previous editions of Qwen, Qwen2
includes a broader spectrum of linguistic data, enhancing the quantity and quality of code and mathematics content. """,7000000000000.0,"""All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens, covering a wide range of domains and languages.""",,,,Confident,"After months of efforts, we are pleased to announce the evolution from Qwen1.5 to Qwen2. This time, we bring to you:
- Pretrained and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and Qwen2-72B;
- Having been trained on data in 27 additional languages besides English and Chinese;
- State-of-the-art performance in a large number of benchmark evaluations;
- Significantly improved performance in coding and mathematics;
- Extended context length support up to 128K tokens with Qwen2-7B-Instruct and Qwen2-72B-Instruct.

(Technical report to follow)",China,,,,,Unreleased,Apache 2.0,Industry
Qwen2-7B,Language,"Chat,Language modeling/generation",Qwen Team,Open weights (unrestricted),"https://qwenlm.github.io/blog/qwen2/ 
https://arxiv.org/abs/2407.10671 ",,Hello Qwen2,2024-06-07,Alibaba,7000000000.0,7B parameters (table 1),2.9400000000001e+23,"7 billion params, 7 trillion tokens

6 FLOP * 7 billion * 7 trillion ~= 2.94e23 FLOP",Unspecified unreleased,"""All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens, covering a wide range of domains and languages. Compared to previous editions of Qwen, Qwen2 includes a broader spectrum of linguistic data, enhancing the quantity and quality of code and mathematics content.""",7000000000000.0,"""All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens, covering a wide range of domains and languages.""",,,,Confident,"This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of
foundational and instruction-tuned language models, encompassing a parameter
range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts
model. Qwen2 surpasses most prior open-weight models, including its predecessor
Qwen1.5, and exhibits competitive performance relative to proprietary models
across diverse benchmarks on language understanding, generation, multilingual
proficiency, coding, mathematics, and reasoning.
The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on
MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as
a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains
9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover,
Qwen2 demonstrates robust multilingual capabilities, proficient in approximately
30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility
and global reach.
To foster community innovation and accessibility, we have made the Qwen2 model
weights openly available on Hugging Face1 and ModelScope2, and the supplementary materials including example code on GitHub3. These platforms also include
resources for quantization, fine-tuning, and deployment, facilitating a wide range
of applications and research endeavors.",China,,,,,Unreleased,Apache 2.0,Industry
Qwen2-57B-A14B,Language,"Chat,Language modeling/generation",Qwen Team,Open weights (unrestricted),"https://qwenlm.github.io/blog/qwen2/ 
https://arxiv.org/abs/2407.10671 ",,Hello Qwen2,2024-06-07,Alibaba,57000000000.0,57B parameters (table 1),3.7800000000001e+23,"""For MoE models, 57B-A14B denotes that the model has 57B parameters in total and for each token 14B parameters are active"" (page 5)

C ~= 6 FLOP * 14e9 * 4.5e12 = 3.78e23",Unspecified unreleased,"""All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens, covering a wide range of domains and languages. Compared to previous editions of Qwen, Qwen2 includes a broader spectrum of linguistic data, enhancing the quantity and quality of code and mathematics content.""",4500000000000.0,"""All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens, covering a wide range of domains and languages.""

57B-A14B model was trained with a 4.5T subset of the 7T overall dataset. (table 1)",,,,Confident,"This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of
foundational and instruction-tuned language models, encompassing a parameter
range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts
model. Qwen2 surpasses most prior open-weight models, including its predecessor
Qwen1.5, and exhibits competitive performance relative to proprietary models
across diverse benchmarks on language understanding, generation, multilingual
proficiency, coding, mathematics, and reasoning.
The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on
MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as
a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains
9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover,
Qwen2 demonstrates robust multilingual capabilities, proficient in approximately
30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility
and global reach.
To foster community innovation and accessibility, we have made the Qwen2 model
weights openly available on Hugging Face1 and ModelScope2, and the supplementary materials including example code on GitHub3. These platforms also include
resources for quantization, fine-tuning, and deployment, facilitating a wide range
of applications and research endeavors.",China,,,,,Unreleased,Apache 2.0,Industry
Granite 20B,Language,Language modeling/generation,IBM Research,Open weights (unrestricted),https://www.ibm.com/downloads/documents/us-en/10a99803c92fdb35,,Granite Foundation Models,2024-05-31,IBM Research,20000000000.0,,3.0000000000001e+23,6*2500000000000*20000000000=3e+23,"Stack Exchange,Common Crawl,Wikimedia","""For English and code, we used Wikimedia, Stack Exchange, and commoncrawl. For multilingual data, we used portions of commoncrawl.""",2500000000000.0,"For pre-training, we used 0.5 trillion English, 0.4 trillion multilingual (es, fr, de, pt), and 1.6 trillion code tokens.",,,,Confident,"We introduce the Granite series of decoder-only foundation models for generative artificial intelligence (AI) tasks that are ready for enterprise use. We report on the architecture, capabilities, underlying data and data governance, training algorithms, compute infrastructure, energy and carbon footprint, testing and evaluation, socio-technical harms and mitigations, and usage policies.","United States of America,Multinational",,,,,,,Industry
Codestral,Language,"Code generation,Code autocompletion","Albert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Henri Roussez, Jean-Malo Delignon, Jia Li, Justus Murke, Kartik Khandelwal, Lawrence Stewart, Louis Martin, Louis Ternon, Lucile Saulnier, Lélio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Marjorie Janiewicz, Mickael Seznec, Nicolas Schuhl, Patrick von Platen, Romain Sauvestre, Pierre Stock, Sandeep Subramanian, Saurabh Garg, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, Thibault Schueller, Timothée Lacroix, Théophile Gervet, Thomas Wang, Valera Nemychnikova, Wendy Shang, William El Sayed, William Marshall",Open weights (non-commercial),https://mistral.ai/news/codestral/,,,2024-05-29,Mistral AI,22200000000.0,22.2B from hugging face model card,,,Unspecified unreleased,"Codestral is trained on a diverse dataset of 80+ programming languages, including the most popular ones, such as Python, Java, C, C++, JavaScript, and Bash. It also performs well on more specific ones like Swift and Fortran. ",,,,,,Confident,"We introduce Codestral, our first-ever code model. Codestral is an open-weight generative AI model explicitly designed for code generation tasks. It helps developers write and interact with code through a shared instruction and completion API endpoint. As it masters code and English, it can be used to design advanced AI applications for software developers.

A model fluent in 80+ programming languages
Codestral is trained on a diverse dataset of 80+ programming languages, including the most popular ones, such as Python, Java, C, C++, JavaScript, and Bash. It also performs well on more specific ones like Swift and Fortran. This broad language base ensures Codestral can assist developers in various coding environments and projects.

Codestral saves developers time and effort: it can complete coding functions, write tests, and complete any partial code using a fill-in-the-middle mechanism. Interacting with Codestral will help level up the developer’s coding game and reduce the risk of errors and bugs.",France,,,,,Unreleased,"Codestral is a 22B open-weight model licensed under the new Mistral AI Non-Production License, which means that you can use it for research and testing purposes. Codestral can be downloaded on HuggingFace.",Industry
Nanbeige2-16B-Chat,Language,"Chat,Question answering",Nanbeige Lab,Open weights (unrestricted),https://huggingface.co/Nanbeige/Nanbeige2-16B-Chat,,Nanbeige2-16B-Chat,2024-05-28,Nanbeige LLM Lab,15800000000.0,https://huggingface.co/Nanbeige/Nanbeige2-16B-Chat,4.05e+23,"The model has 15.8B parameters and was trained on 4.5T tokens during the training phrase (https://huggingface.co/Nanbeige/Nanbeige2-16B-Chat). It appears to be entirely transformer-based (https://huggingface.co/Nanbeige/Nanbeige2-16B-Chat/blob/main/modeling_nanbeige.py).

Assuming training was done for 1 epoch, the 6ND approximation yields
Training compute
= # of active parameters / forward pass * # of tokens * 6 FLOPS / token
= 1.5e10 parameters * 4.5e12 tokens * 6 FLOPS / token
= 4.05e23 FLOPS",,,,The model was trained on 4.5T tokens during the training phrase (https://huggingface.co/Nanbeige/Nanbeige2-16B-Chat).,,,,Unverified,"The Nanbeige2-16B-Chat is the latest 16B model developed by the Nanbeige Lab, which utilized 4.5T tokens of high-quality training data during the training phase. During the alignment phase, we initially trained our model using 1 million samples through Supervised Fine-Tuning (SFT). We then engaged in curriculum learning with 400,000 high-quality samples that presented a greater level of difficulty. Subsequently, we incorporated human feedback through the Direct Preference Optimization (DPO), culminating in the development of Nanbeige2-16B-Chat. Nanbeige2-16B-Chat has achieved superior performance across various authoritative benchmark datasets.",China,,,,,,,Industry
360Zhinao-7B,Language,Question answering,360zhinao,,https://arxiv.org/abs/2405.13386,0.0,360Zhinao Technical Report,2024-05-22,360 Security Technology,7000000000.0,,1.428e+23,6*3400000000000*7000000000=1.428e+23,,,3400000000000.0,3.4 trillion tokens,,,,Confident,"We present 360Zhinao models with 7B parameter size and context lengths spanning 4K, 32K and 360K, all available at this https URL. For rapid development in pretraining, we establish a stable and sensitive ablation environment to evaluate and compare experiment runs with minimal model size. Under such guidance, we perfect our data cleaning and composition strategies to pretrain 360Zhinao-7B-Base on 3.4T tokens. We also mainly emphasize data during alignment, where we strive to balance quantity and quality with filtering and reformatting. With tailored data, 360Zhinao-7B's context window is easily extended to 32K and 360K. RMs and RLHF are trained following SFT and credibly applied to specific tasks. All together these contributions lead to 360Zhinao-7B's competitive performance among models of similar size.",China,,,,,,,Industry
ALLaM 34B,Language,,"M Saiful Bari, Yazeed Alnumay, Norah A. Alzahrani, Nouf M. Alotaibi, Hisham A. Alyahya, Sultan AlRashed, Faisal A. Mirza, Shaykhah Z. Alsubaie, Hassan A. Alahmed, Ghadah Alabduljabbar, Raghad Alkhathran, Yousef Almushayqih, Raneem Alnajim, Salman Alsubaihi, Maryam Al Mansour, Majed Alrubaian, Ali Alammari, Zaki Alawami, Abdulmohsen Al-Thubaity, Ahmed Abdelali, Jeril Kuriakose, Abdalghani Abujabal, Nora Al-Twairesh, Areeb Alowisheq, Haidar Khan",,https://openreview.net/pdf?id=MscdsFVZrN,9.0,AI Models for Arabic and English,2024-05-21,Saudi Data and Artificial Intelligence Authority,34000000000.0,,1.0608e+24,6*34000000000*5200000000000=1.060800e+24,,,5200000000000.0,"3,431,217,579(4.3B) total documents, with a total of 4,587,781,981,546(4.5T) words, and 5.2T tokens.",,,NVIDIA A100,Confident,,Saudi Arabia,,,,,,,"Industry,Government"
Chameleon-34B,"Multimodal,Image generation,Language,Vision","Language modeling/generation,Vision-language generation,Visual question answering,Text-to-image","Srinivasan Iyer, Bernie Huang, Lili Yu, Arun Babu, Chunting Zhou, Kushal Tirumala, Xi Victoria Lin, Hu Xu, Xian Li, Akshat Shrivastava, Omer Levy, Armen Aghajanyan, Ram Pasunuru, Andrew Cohen, Aram H. Markosyan, Koustuv Sinha, Xiaoqing Ellen Tan, Ivan Evtimov, Ping Yu, Tianlu Wang, Olga Golovneva, Asli Celikyilmaz, Pedro Rodriguez, Leonid Shamis, Vasu Sharma, Christine Jou, Karthik Padthe, Ching-Feng Yeh, Mingda Chen, Bapi Akula, Jacob Kahn, Daniel Li, Scott Yih, Barlas Oguz, Morteza Behrooz, Benjamin Muller, Carleigh Wood, Mary Williamson, Ramya Raghavendra, Barbara Usher, William Ngan, Nikolay Bashlykov, Lukas Blecher, Sony Theakanath, Ammar Rizvi, Gargi Ghosh, Luke Zettlemoyer",Open weights (non-commercial),https://arxiv.org/abs/2405.09818v1,,Chameleon: Mixed-Modal Early-Fusion Foundation Models,2024-05-16,Facebook AI Research,34000000000.0,,1.6453571041e+24,"GPU method:
Table 2 shows that 34B model pre-training uses 4282407 GPU-hours, trained across 3072 A100s.
3.12e14 * 4282407 * 3600 * 0.3 =  1.44e24

Parameter-token method:
Pre-training goes over 9.2T tokens, post-training only goes over 1.1B tokens (sum of tokens column in Table 3).
6 * 34B *  9.2T = 1.88e24

Geometric mean: sqrt(1.44e24 * 1.88e24) = 1.65e24",Unspecified unreleased,"Pre-training:
- 2.9 trillion tokens of pure text
- 1.4 billion text-image pairs, which produces 1.5 trillion text-image tokens
	- Since each image is 1024 tokens, implies 1.43 trillion image tokens and 0.07 trillion text tokens
- 400 billion tokens of image-text interleaved documents
	- Difficult to estimate image-to-text ratio, but references OBELIKS paper which had 141 million web pages, 353 million associated images, and 115 billion text tokens.
	- 353 million * 1024 = 361.5 billion image tokens, so 75.9% of the tokens would be from images
	- Implies 303.5 billion image tokens and 96.5 text tokens
- There is a second stage of ""higher quality"" pre-training tokens which are not described in detail


Post-training:
- 940 million tokens of text
- 1.1 million tokens of code
- 19.4 million tokens of visual chats
	- Featuring 16.7 thousand images, so 17.1M image tokens and 2.3M text tokens
- 68 million tokens of image generations
	- Featuring 64.3 thousand images, so 65.8M image tokens and 2.2M text tokens
- 35.8 million tokens of interleaved generations
	- Featuring 30.7 thousand images, so 31.4M image tokens and 4.4M text tokens
- 38.6 million tokens of safety training
	- Featuring 1.6 thousand images, so 1.6M image tokens and 37M text tokens",4400000000000.0,"Slightly conflicting info. Pre-training data details describe different types of data that sum to 4.8 trillion tokens, but Table 1 indicates 4.4T. Using table values as this agrees with other statements about epochs and total tokens seen.",1394.0,"34B model pre-training uses 4282407 GPU-hours, trained across 3072 A100s
4282407 / 3072 = 1394",NVIDIA A100 SXM4 80 GB,Confident,"We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",United States of America,,,"Not enough info to estimate. GPU time given for pretraining, and while we know # of fine-tuning tokens we don't know # of epochs.",3072.0,Unreleased,"https://ai.meta.com/resources/models-and-libraries/chameleon-downloads/?gk_enable=chameleon_web_flow_is_live

""The models we’re releasing today were safety tuned and support mixed-modal inputs and text-only output to be used for research purposes. While we’ve taken steps to develop these models responsibly, we recognize that risks remain. At this time, we are not releasing the Chameleon image generation model.""",Industry
FragLlama: Next-fragment prediction for molecular design,"Multimodal,Image generation,Vision,Language","Language modeling/generation,Vision-language generation,Visual question answering,Text-to-image","Srinivasan Iyer, Bernie Huang, Lili Yu, Arun Babu, Chunting Zhou, Kushal Tirumala, Xi Victoria Lin, Hu Xu, Xian Li, Akshat Shrivastava, Omer Levy, Armen Aghajanyan, Ram Pasunuru, Andrew Cohen, Aram H. Markosyan, Koustuv Sinha, Xiaoqing Ellen Tan, Ivan Evtimov, Ping Yu, Tianlu Wang, Olga Golovneva, Asli Celikyilmaz, Pedro Rodriguez, Leonid Shamis, Vasu Sharma, Christine Jou, Karthik Padthe, Ching-Feng Yeh, Mingda Chen, Bapi Akula, Jacob Kahn, Daniel Li, Scott Yih, Barlas Oguz, Morteza Behrooz, Benjamin Muller, Carleigh Wood, Mary Williamson, Ramya Raghavendra, Barbara Usher, William Ngan, Nikolay Bashlykov, Lukas Blecher, Sony Theakanath, Ammar Rizvi, Gargi Ghosh, Luke Zettlemoyer",Open weights (non-commercial),https://arxiv.org/abs/2405.09818v1,,Chameleon: Mixed-Modal Early-Fusion Foundation Models,2024-05-16,Facebook AI Research,7000000000.0,,3.3399700602e+23,"GPU method:
Table 2 shows that 7B model pre-training uses 856481 GPU-hours, trained across 1024 A100s
3.12e14 * 856481 * 3600 * 0.3 =  2.89e23

Parameter-token method:
Pre-training goes over 9.2T tokens, post-training only goes over 1.1B tokens (sum of tokens column in Table 3)
6 * 7B *  9.2T = 3.86e23

Geometric mean: sqrt(2.89e23 * 3.86e23) = 3.34e23",Unspecified unreleased,"Pre-training:
- 2.9 trillion tokens of pure text
- 1.4 billion text-image pairs, which produces 1.5 trillion text-image tokens
	- Since each image is 1024 tokens, implies 1.43 trillion image tokens and 0.07 trillion text tokens
- 400 billion tokens of image-text interleaved documents
	- Difficult to estimate image-to-text ratio, but references OBELIKS paper which had 141 million web pages, 353 million associated images, and 115 billion text tokens.
	- 353 million * 1024 = 361.5 billion image tokens, so 75.9% of the tokens would be from images
	- Implies 303.5 billion image tokens and 96.5 text tokens
- There is a second stage of ""higher quality"" pre-training tokens which are not described in detail


Post-training:
- 940 million tokens of text
- 1.1 million tokens of code
- 19.4 million tokens of visual chats
	- Featuring 16.7 thousand images, so 17.1M image tokens and 2.3M text tokens
- 68 million tokens of image generations
	- Featuring 64.3 thousand images, so 65.8M image tokens and 2.2M text tokens
- 35.8 million tokens of interleaved generations
	- Featuring 30.7 thousand images, so 31.4M image tokens and 4.4M text tokens
- 38.6 million tokens of safety training
	- Featuring 1.6 thousand images, so 1.6M image tokens and 37M text tokens",4400000000000.0,"Slightly conflicting info. Pre-training data details describe different types of data that sum to 4.8 trillion tokens, but Table 1 indicates 4.4T. Using table values as this agrees with other statements about epochs and total tokens seen.",836.4,"34B model pre-training uses 856481 GPU-hours, trained across 1024 A100s
856481 / 1024 = 836.4",NVIDIA A100 SXM4 80 GB,Confident,"We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model. It also matches or exceeds the performance of much larger models, including Gemini Pro and GPT-4V, according to human judgments on a new long-form mixed-modal generation evaluation, where either the prompt or outputs contain mixed sequences of both images and text. Chameleon marks a significant step forward in a unified modeling of full multimodal documents.",United States of America,,,"Not enough info to estimate. GPU time given for pretraining, and while we know # of fine-tuning tokens we don't know # of epochs.",,Unreleased,"https://ai.meta.com/resources/models-and-libraries/chameleon-downloads/?gk_enable=chameleon_web_flow_is_live

""The models we’re releasing today were safety tuned and support mixed-modal inputs and text-only output to be used for research purposes. While we’ve taken steps to develop these models responsibly, we recognize that risks remain. At this time, we are not releasing the Chameleon image generation model.""",Industry
Doubao-lite,Language,"Language modeling/generation,Question answering",,API access,https://www.volcengine.com/docs/6360/1264663,,Doubao General Model Lite (Doubao-lite),2024-05-15,ByteDance,,,,,Unspecified unreleased,,,,,,,Unknown,"A lightweight version of the LLM, offering lower token costs and latency compared to the Pro version. 
",China,,,,,Unreleased,,Industry
Yi-Large,Language,"Chat,Language modeling/generation","Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai",API access,,,,2024-05-13,01.AI,100000000000.0,"""Yi-Large is a software over-the-air-driven closed-source large model with a parameter of over 100 billion tokens."" from https://www.chinadaily.com.cn/a/202405/13/WS6641abd1a31082fc043c6ccd.html",1.8e+24,"6ND = 6*100000000000*3000000000000=1.8e+24

(speculative confidence because training dataset size is very uncertain)",,,3000000000000.0,"3T tokens for previous Yi models: ""Targeted as a bilingual language model and trained on 3T multilingual corpus, the Yi series models become one of the strongest LLM worldwide, showing promise in language understanding, commonsense reasoning, reading comprehension, and more.""
",,,,Speculative,,China,,,,,Unreleased,,Industry
GPT-4o,"Multimodal,Language,Audio,Speech,Vision","Chat,Image generation,Audio generation,Vision-language generation,Table tasks,Language modeling/generation,Question answering,Speech recognition","Aidan Clark, Alex Paino, Jacob Menick, Liam Fedus, Luke Metz, Clemens Winter, Lia Guy, Sam Schoenholz, Daniel Levy, Nitish Keskar, Alex Carney, Alex Paino, Ian Sohl, Qiming Yuan, Reimar Leike, Arka Dhar, Brydon Eastman, Mia Glaese, Ben Sokolowsky, Andrew Kondrich, Felipe Petroski Such, Henrique Ponde de Oliveira Pinto, Jiayi Weng, Randall Lin, Youlong Cheng, Nick Ryder, Lauren Itow, Barret Zoph, John Schulman, Mianna Chen, Adam Lerer, Adam P. Goucher, Adam Perelman, Akila Welihinda, Alec Radford, Alex Borzunov, Alex Carney, Alex Chow, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexi Christakis, Ali Kamali, Allison Moyer, Allison Tam, Amin Tootoonchian, Ananya Kumar, Andrej Karpathy, Andrey Mishchenko, Andrew Cann, Andrew Kondrich, Andrew Tulloch, Angela Jiang, Antoine Pelisse, Anuj Gosalia, Avi Nayak, Avital Oliver, Behrooz Ghorbani, Ben Leimberger, Ben Wang, Blake Samic, Brian Guarraci, Brydon Eastman, Camillo Lugaresi, Chak Li, Charlotte Barette, Chelsea Voss, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christopher Hesse, Colin Wei, Daniel Kappler, Daniel Levin, Daniel Levy, David Farhi, David Mely, David Sasaki, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Peterson, Eric Sigler, Eugene Brevdo, Farzad Khorasani, Francis Zhang, Gene Oden, Geoff Salmon, Hadi Salman, Haiming Bao, Heather Schmidt, Hongyu Ren, Hyung Won Chung, Ian Kivlichan, Ian O'Connell, Ian Osband, Ilya Kostrikov, Ingmar Kanitscheider, Jacob Coxon, James Crooks, James Lennon, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jiayi Weng, Jie Tang, Joanne Jang, Jonathan Ward, Jonathan McKay, Jong Wook Kim, Josh Gross, Josh Kaplan, Joy Jiao, Joyce Lee, Juntang Zhuang, Kai Fricke, Kavin Karthik, Kenny Hsu, Kiel Howe, Kyle Luther, Larry Kai, Lauren Itow, Leo Chen, Lia Guy, Lien Mamitsuka, Lilian Weng, Long Ouyang, Louis Feuvrier, Lukas Kondraciuk, Lukasz Kaiser, Lyric Doshi, Mada Aflak, Maddie Simens, Madeleine Thompson, Marat Dukhan, Marvin Zhang, Mateusz Litwin, Max Johnson, Mayank Gupta, Mia Glaese, Michael Janner, Michael Petrov, Michael Wu, Michelle Fradin, Michelle Pokrass, Miguel Oom Temudo de Castro, Mikhail Pavlov, Minal Khan, Mo Bavarian, Natalia Gimelshein, Natalie Staudacher, Nick Stathas, Nik Tezak, Nithanth Kudige, Noel Bundick, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivier Godement, Owen Campbell-Moore, Philip Pronin, Philippe Tillet, Rachel Lim, Rajan Troll, Randall Lin, Rapha gontijo lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Rob Honsby, Rohit Ramchandani, Rory Carmichael, Ruslan Nigmatullin, Ryan Cheu, Scott Gray, Sean Grove, Sean Metzger, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shuaiqi (Tony) Xia, Sonia Phene, Spencer Papay, Steve Coffey, Steve Lee, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tarun Gogineni, Ted Sanders, Thomas Cunninghman, Thomas Dimson, Thomas Raoux, Tianhao Zheng, Tina Kim, Todd Underwood, Tristan Heywood, Valerie Qi, Vinnie Monaco, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wojciech Zaremba, Yash Patil, Yilei, Qian, Yongjik Kim, Youlong Cheng, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, Yury Malkov",API access,"https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",,Hello GPT-4o,2024-05-13,OpenAI,,"Not known.

Inference costs in the API are 2x cheaper than GPT-4 Turbo",3.810001e+25,Training compute estimated from benchmark scores.,Unspecified unreleased,"""With GPT-4o, we trained a single new model end-to-end across text, vision, and audio.""",,,,,,Speculative,"We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

GPT-4o (“o” for “omni”) is a step towards much more natural human-computer interaction—it accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs. It can respond to audio inputs in as little as 232 milliseconds, with an average of 320 milliseconds, which is similar to human response time(opens in a new window) in a conversation. It matches GPT-4 Turbo performance on text in English and code, with significant improvement on text in non-English languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially better at vision and audio understanding compared to existing models.",United States of America,,,"Definitely a new model, not a GPT-4 finetune",,Unreleased,,Industry
Yi-1.5-34B,Language,"Chat,Language modeling/generation,Translation,Code generation",,Open weights (restricted use),https://huggingface.co/01-ai/Yi-1.5-34B,,"Yi-1.5 is an upgraded version of Yi, delivering stronger performance in coding, math, reasoning, and instruction-following capability.",2024-05-13,01.AI,34000000000.0,34b,7.344e+23,6 FLOP / parameter / token * 34*10^9 parameters * 3.6*10^12 tokens = 7.344e+23 FLOP,Unspecified unreleased,assuming same as Yi 34 - Chinese and English dataset,3600000000000.0,"3.6T
""Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.""

3.6T total pre-trained tokens ",,,,Confident,"Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.

Compared with Yi, Yi-1.5 delivers stronger performance in coding, math, reasoning, and instruction-following capability, while still maintaining excellent capabilities in language understanding, commonsense reasoning, and reading comprehension.

Yi-1.5 comes in 3 model sizes: 34B, 9B, and 6B. For model details and benchmarks, see Model Card.",China,,,,,Unreleased,"no training code

the model https://huggingface.co/01-ai/Yi-1.5-34B Apache 2.0

""If you create derivative works based on this model, please include the following attribution in your derivative works:""",Industry
Yi-1.5-9B,Language,"Language modeling/generation,Question answering",,Open weights (unrestricted),https://huggingface.co/01-ai/Yi-1.5-9B,,Yi-1.5 is an upgraded version of Yi.,2024-05-13,01.AI,8830000000.0,8.83B (safetensors),1.90728e+23,6 FLOP / parameter / token * 8.83*10^9 parameters * 3.6*10^12 tokens = 1.90728e+23 FLOP,Unspecified unreleased,,3600000000000.0,3.6T pre-trained tokens,,,,Confident,"Yi-1.5 is an upgraded version of Yi. It is continuously pre-trained on Yi with a high-quality corpus of 500B tokens and fine-tuned on 3M diverse fine-tuning samples.

Compared with Yi, Yi-1.5 delivers stronger performance in coding, math, reasoning, and instruction-following capability, while still maintaining excellent capabilities in language understanding, commonsense reasoning, and reading comprehension.",China,,,,,Unreleased,"https://huggingface.co/01-ai/Yi-1.5-9B
Apache 2.0",Industry
Falcon 2 11B,Language,Language modeling/generation,,Open weights (restricted use),https://huggingface.co/tiiuae/falcon-11B ,,Falcon2-11B,2024-05-09,Technology Innovation Institute,11000000000.0,11B,3.6e+23,"trained on 5.5T tokens

6 * 11B * 5.5T = 3.6e23",RefinedWeb,"""Falcon2-11B was trained over 5,000B tokens of RefinedWeb, a high-quality filtered and deduplicated web dataset which we enhanced with curated corpora. It followed a four stage training strategy. The first three stages were focused on increasing the context length, from to 2048 to 4096 and finally to 8192 tokens. The last stage aimed to further enhance performance using only high quality data.""

Possibly an updated version of RefinedWeb, which only had 3.5T tokens when Falcon 1 was released? not clear.",5500000000000.0,5.5T tokens: https://falconllm.tii.ae/falcon-2.html ,1400.0,"roughly two months: https://huggingface.co/tiiuae/falcon-11B 

so ~1400 days",NVIDIA A100 SXM4 40 GB,Confident,"Falcon2-11B is an 11B parameters causal decoder-only model built by TII and trained on over 5,000B tokens of RefinedWeb enhanced with curated corpora. The model is made available under the TII Falcon License 2.0, the permissive Apache 2.0-based software license which includes an acceptable use policy that promotes the responsible use of AI.",United Arab Emirates,,,,,,Open but has an acceptable use policy: https://falconllm-staging.tii.ae/falcon-2-acceptable-use-policy.html ,Government
DeepSeek-V2 (MoE-236B),Language,"Language modeling/generation,Chat,Code generation","DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J.L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R.J. Chen, R.L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S.S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W.L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X.Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun",Open weights (restricted use),"https://arxiv.org/abs/2405.04434 
https://github.com/deepseek-ai/DeepSeek-V2 ",,"DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model",2024-05-07,DeepSeek,236000000000.0,"21B active params, 236B total",1.02e+24,21b active params * 8.1 trillion * 6 = 1.02e24,Unspecified unreleased,"""We construct a high-quality and multi-source pre-training corpus consisting of 8.1T tokens.
Compared with the corpus used in DeepSeek 67B (our previous release) (DeepSeek-AI, 2024), this
corpus features an extended amount of data, especially Chinese data, and higher data quality. We
first pretrain DeepSeek-V2 on the full pre-training corpus""",8100000000000.0,8.1 Trillion,,"172.8K GPU hours, wall time not stated",NVIDIA H800 SXM5,Confident,"We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.",China,,,,,Unreleased,open weights with harmful use restrictions: https://github.com/deepseek-ai/DeepSeek-V2/blob/main/LICENSE-MODEL ,Industry
Multi-Token Prediction 7B,Language,Code generation,"Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, Gabriel Synnaeve",Open weights (non-commercial),https://arxiv.org/abs/2404.19737,,Better & Faster Large Language Models via Multi-token Prediction,2024-04-30,Facebook AI Research,6700000000.0,6.7B (“7B”),3.841092e+23,"""training all models reported in the paper required around 500K GPU hours of computation on hardware of type A100-80GB and H100.""
A100-80 GB peak FLOP/s [assumed fp16 precision]: 77970000000000
H100 peak FLOP/s [assumed SXM5 TensorCore]: 989000000000000
assuming 50/50 usage:

(77970000000000+989000000000000)*0.5*500000hours*3600s*0.3=2.880819e+23 for ALL models in the paper

assuming this model has taken around 40% of all used compute (https://docs.google.com/spreadsheets/d/1Yc-HAdYgn6e9SUIliMaQtvQscIsEZomLdXbWyYl_jfQ/edit?usp=sharing)

then it's assumed compute is 3.841092e+23 FLOPs","CodeContests,Unspecified unreleased",,250000000000.0,1T total tokens over 4 epochs (Table 1),,,"NVIDIA A100 SXM4 80 GB,NVIDIA H100 SXM5 80GB",Likely,"Large language models such as GPT and Llama are trained with a next-token prediction loss. In this work, we suggest that training language models to predict multiple future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, we ask the model to predict the following n tokens using n independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, we measure improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where our models consistently outperform strong baselines by several percentage points. Our 13B parameter models solves 12 % more problems on HumanEval and 17 % more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to 3 times faster at inference, even with large batch sizes.",United States of America,,,,,Unreleased,"https://huggingface.co/facebook/multi-token-prediction

""we’re releasing the pre-trained models for code completion under a non-commercial/research-only license.""",Industry
Multi-Token Prediction 13B,Language,Code generation,"Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, Gabriel Synnaeve",Unreleased,https://arxiv.org/abs/2404.19737,,Better & Faster Large Language Models via Multi-token Prediction,2024-04-30,Facebook AI Research,13000000000.0,13B (Figure 1),1.5364368e+23,"""training all models reported in the paper required around 500K GPU hours of computation on hardware of type A100-80GB and H100.""
A100-80 GB peak FLOP/s [assumed fp16 precision]: 77970000000000
H100 peak FLOP/s [assumed SXM5 TensorCore]: 989000000000000
assuming 50/50 usage:

(77970000000000+989000000000000)*0.5*500000hours*3600s*0.3=2.880819e+23 for ALL models in the paper

assuming this model has taken around 16% of all used compute (https://docs.google.com/spreadsheets/d/1Yc-HAdYgn6e9SUIliMaQtvQscIsEZomLdXbWyYl_jfQ/edit?usp=sharing)

then it's assumed compute is 1.5364368e+23 FLOPs",,,209700000000.0, 209.7B (Table S13),,,"NVIDIA A100 SXM4 80 GB,NVIDIA H100 SXM5 80GB",Likely,"Large language models such as GPT and Llama are trained with a next-token prediction loss. In this work, we suggest that training language models to predict multiple future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, we ask the model to predict the following n tokens using n independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, we measure improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where our models consistently outperform strong baselines by several percentage points. Our 13B parameter models solves 12 % more problems on HumanEval and 17 % more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to 3 times faster at inference, even with large batch sizes.",United States of America,,,,,Unreleased,,Industry
Qwen1.5-110B,Language,"Chat,Language modeling/generation,Quantitative reasoning,Code generation,Translation",Qwen Team,Open weights (unrestricted),https://qwenlm.github.io/blog/qwen1.5-110b/?ref=upstract.com,,Qwen1.5-110B: The First 100B+ Model of the Qwen1.5 Series,2024-04-25,Alibaba,110000000000.0,"110B
",,lower bound is taken from Qwen1.5 72B training compute estimation,Unspecified unreleased,"We pretrained the models with a large amount of data, and we post-trained the models with both supervised finetuning and direct preference optimization.",,"A Qwen developer gave token counts for other models in the series at this github issue: https://github.com/QwenLM/Qwen2/issues/97
110B was asked but got no response.
7B, 14B, and 72B got 4T, 4T, and 3T tokens respectively.

In another issue from Qwen2: ""We are not authorized to share the details right now but the rough number is over 3T tokens for Qwen1.5 and over 7T tokens for Qwen2."" https://github.com/QwenLM/Qwen2/issues/562",,,,Confident,"The Qwen1.5-110B is the largest model in the Qwen1.5 series, and it is also the first one with over 100 billion parameters in the series. It demonstrates competitive performance against the very recently released SOTA model Llama-3-70B and it is significantly better than the 72B model. This tells us that there is still a lot of room in model size scaling for better performance. While the releease of Llama-3 indicates the significance of data scaling to an extremely large scale, we believe we can get the best of both worlds by scaling both data and model size in our future release. Stay tuned for Qwen2!",China,,,,,Unreleased,https://huggingface.co/Qwen/Qwen1.5-110B,Industry
Arctic,Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning",Snowflake AI Research,Open weights (unrestricted),https://www.snowflake.com/en/blog/arctic-open-efficient-foundation-language-models-snowflake/,,"Snowflake Arctic: The Best LLM for Enterprise AI — Efficiently Intelligent, Truly Open",2024-04-24,Snowflake,480000000000.0,""" It combines a 10B dense transformer model with a residual 128x3.66B MoE MLP resulting in 480B total and 17B active parameters chosen using a top-2 gating.""",3.8347175e+23,"from the graph:
1x - Arctic
1.9X - Llama 3 8B (7.2×10^23) ~ = Yi 34B (6.1e23) -> x = 3.2105263e+23
3X - Code Llama 70B (1.26e+24) -> x = 4.2e+23
17.5X - Llama 3 70B (7.861e24) -> x =4.492e+23

= 3.7975893e+23

Operation counting (17B active parameters): 
6ND = 6 FLOP / parameter / token * 17*10^9 parameters * 3.5*10^12 tokens = 3.57e+23 FLOP

geometric mean:(3.2105263e+23*4.492e+23*4.2e+23*3.57e+23)^(1/4) = 3.8347175e+23",,,3500000000000.0,"""Arctic was trained with a three-stage curriculum each with a different data composition focusing on generic skills in the first phase (1T Tokens), and enterprise-focused skills in the latter two phases (1.5T and 1T tokens). ""

1+1.5+1 = 3.5",,"""less than 3K GPU weeks""",,Confident,"Built by Snowflake, Arctic is a family of enterprise-grade LLMs with leading performance in enterprise intelligence and breakthrough efficiency. Snowflake Arctic is a truly open, Apache 2.0 licensed model.",United States of America,,,,,Open source,Apache 2.0 license with ungated access to weights and code paired with open data recipe and research insights.,Industry
phi-3-medium 14B,Language,"Chat,Language modeling/generation","Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou",Open weights (unrestricted),https://arxiv.org/abs/2404.14219,,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,2024-04-23,Microsoft,14000000000.0,14B,4.032e+23,counting operations: 6×4.8×10^12 tokens × 14×10^9 parameters ≈ 4.032×10^23 FLOPS,Phi-3 Dataset,"we also trained phi-3-medium, a model with 14B parameters using the same tokenizer and architecture of phi-3-mini, and trained on the same data for slightly more epochs (4.8T tokens total as for phi-3-small)",4800000000000.0,,,,,Likely,"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).",United States of America,,,,,Unreleased,"MIT license for weights:
https://huggingface.co/microsoft/Phi-3-medium-128k-instruct ",Industry
SenseChat 5.0,Language,"Chat,Language modeling/generation",SenseNova Team,Hosted access (no API),https://zhidx.com/p/421866.html,,,2024-04-23,SenseTime,600000000000.0,"This article claims the model is a 600B MoE:
https://www.sensetime.com/cn/news-detail/51168158?categoryId=72",,,,,1670000000000.0,"Words per gygabyte for Mandarin Chinese: 167M

10000*167000000 = 1670000000000

""is trained based on more than 10TB of tokens, covers a large amount of synthetic data""

However, a later news article from SenseTime itself said: ""In terms of data, SenseChat V5 uses a new generation of data production pipelines to produce 10T tokens of high-quality training data."" (It also says ""10T tokens"" in the original untranslated version).",,,,Likely,"SenseTime has newly upgraded its ""SenseNova 5.0"" large model system , and its comprehensive capabilities are fully comparable to GPT-4 Turbo .","Hong Kong,China",,,,,Unreleased,,Industry
phi-3-small 7.4B,Language,"Chat,Language modeling/generation","Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou",,https://arxiv.org/abs/2404.14219,,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,2024-04-23,Microsoft,7400000000.0,7.4B,2.1312e+23,6ND = 6*7.4B parameters * 4.8T tokens =2.1312e+23,,"""4.8T tokens total as for phi-3-small""",4800000000000.0,,,,,Confident,"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in our dataset for training, a scaled-up version of the one used for phi-2, composed of heavily filtered web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T tokens, called phi-3-small and phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).",United States of America,,,,,,,Industry
Phi-3.5-MoE,Language,"Language modeling/generation,Translation,Question answering,Code generation,Quantitative reasoning","Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou",Open weights (unrestricted),https://arxiv.org/abs/2404.14219,,Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone,2024-04-23,Microsoft,60800000000.0,"""Phi-3.5-MoE has 16x3.8B parameters with 6.6B active parameters when using 2 experts. The model is a mixture-of-expert decoder-only Transformer model using the tokenizer with vocabulary size of 32,064.""",3.0202896e+23,"512 GPUs * 989500000000000 FLOP / sec * 552 hours * 3600 sec / hour * 0.3 [assumed utilization] = 3.0202896e+23 FLOP

6 FLOP / token / parameter * 4900000000000 tokens * 6.6*10^9 active parameters = 1.9404e+23 FLOP (slightly less confidence than hardware estimation since the 6ND formula is less accurate for MoE)",Unspecified unreleased,"""Our training data includes a wide variety of sources, totaling 4.9 trillion tokens (including 10% multilingual), and is a combination of

publicly available documents filtered rigorously for quality, selected high-quality educational data, and code;
newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.);
high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.
We are focusing on the quality of data that could potentially improve the reasoning ability for the""",4900000000000.0,Training data: 4.9T tokens,552.0,"GPUs: 512 H100-80G
Training time: 23 days

23 days * 24 hours / day = 552 hours",NVIDIA H100 SXM5 80GB,Confident,"We introduce phi-3-mini, a 3.8 billion parameter language model trained on 3.3 trillion tokens, whose overall performance, as measured by both academic benchmarks and internal testing, rivals that of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38 on MT-bench), despite being small enough to be deployed on a phone. Our training dataset is a scaled-up version of the one used for phi-2, composed of heavily filtered publicly available web data and synthetic data. The model is also further aligned for robustness, safety, and chat format. We also provide parameter-scaling results with a 7B, 14B models trained for 4.8T tokens, called phi-3-small, phi-3-medium, both significantly more capable than phi-3-mini (e.g., respectively 75%, 78% on MMLU, and 8.7, 8.9 on MT-bench). To enhance multilingual, multimodal, and long-context capabilities, we introduce three models in the phi-3.5 series: phi-3.5-mini, phi-3.5-MoE, and phi-3.5-Vision. The phi-3.5-MoE, a 16 x 3.8B MoE model with 6.6 billion active parameters, achieves superior performance in language reasoning, math, and code tasks compared to other open-source models of similar scale, such as Llama 3.1 and the Mixtral series, and on par with Gemini-1.5-Flash and GPT-4o-mini. Meanwhile, phi-3.5-Vision, a 4.2 billion parameter model derived from phi-3.5-mini, excels in reasoning tasks and is adept at handling both single-image and text prompts, as well as multi-image and text prompts.",United States of America,,,,512.0,Unreleased,"https://huggingface.co/microsoft/Phi-3.5-MoE-instruct

MIT license (instruct model)",Industry
Llama 3-70B,Language,"Chat,Language modeling/generation,Code generation",Aaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Amit Sangani; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Ash JJhaveri; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hamid Shojanazeri; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos,Open weights (restricted use),https://ai.meta.com/blog/meta-llama-3/,,Introducing Meta Llama 3: The most capable openly available LLM to date,2024-04-18,Meta AI,70000000000.0,,7.861e+24,"Arithmetic calculation:
6 * 15T tokens * 70B parameters = 6.3e24

GPU calculation:
https://huggingface.co/meta-llama/Meta-Llama-3-70B indicates training took 6.4M GPU-hours
We also know their larger scale training runs for 405B were getting between 0.38-0.41 MFU. Presumably the 70B model gets at least 0.43 utilization (405B has to be split across two nodes, while 70B should fit on one).
990 TFLOPS per GPU * 6.4 million GPU hours * 3600s * 0.43 = 9.808e24

Geometric mean: sqrt(6.3e24 * 9.808e24) = 7.861e24",Llama 3 dataset,"""Llama 3 is pretrained on over 15T tokens that were all collected from publicly available sources. Our training dataset is seven times larger than that used for Llama 2, and it includes four times more code. To prepare for upcoming multilingual use cases, over 5% of the Llama 3 pretraining dataset consists of high-quality non-English data that covers over 30 languages.""",15000000000000.0,,,,NVIDIA H100 SXM5 80GB,Confident,,United States of America,,,,,Unreleased,"https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md

License A custom commercial license is available at: https://llama.meta.com/llama3/license",Industry
Llama 3-8B,Language,"Chat,Language modeling/generation,Code generation",Aaditya Singh; Aaron Grattafiori; Abhimanyu Dubey; Abhinav Jauhri; Abhinav Pandey; Abhishek Kadian; Adam Kelsey; Adi Gangidi; Ahmad Al-Dahle; Amit Sangani; Ahuva Goldstand; Aiesha Letman; Ajay Menon; Akhil Mathur; Alan Schelten; Alex Vaughan; Amy Yang; Andrei Lupu; Andres Alvarado; Andrew Gallagher; Andrew Gu; Andrew Ho; Andrew Poulton; Andrew Ryan; Angela Fan; Ankit Ramchandani; Anthony Hartshorn; Archi Mitra; Archie Sravankumar; Artem Korenev; Arun Rao; Ashley Gabriel; Ashwin Bharambe; Assaf Eisenman; Aston Zhang; Ash JJhaveri; Aurelien Rodriguez; Austen Gregerson; Ava Spataru; Baptiste Roziere; Ben Maurer; Benjamin Leonhardi; Bernie Huang; Bhargavi Paranjape; Bing Liu; Binh Tang; Bobbie Chern; Brani Stojkovic; Brian Fuller; Catalina Mejia Arenas; Chao Zhou; Charlotte Caucheteux; Chaya Nayak; Ching-Hsiang Chu; Chloe Bi; Chris Cai; Chris Cox; Chris Marra; Chris McConnell; Christian Keller; Christoph Feichtenhofer; Christophe Touret; Chunyang Wu; Corinne Wong; Cristian Canton Ferrer; Damien Allonsius; Daniel Kreymer; Daniel Haziza; Daniel Li; Danielle Pintz; Danny Livshits; Danny Wyatt; David Adkins; David Esiobu; David Xu; Davide Testuggine; Delia David; Devi Parikh; Dhruv Choudhary; Dhruv Mahajan; Diana Liskovich; Diego Garcia-Olano; Diego Perino; Dieuwke Hupkes; Dingkang Wang; Dustin Holland; Egor Lakomkin; Elina Lobanova; Xiaoqing Ellen Tan; Emily Dinan; Eric Smith; Erik Brinkman; Esteban Arcaute; Filip Radenovic; Firat Ozgenel; Francesco Caggioni; Frank Seide; Frank Zhang; Gabriel Synnaeve; Gabriella Schwarz; Gabrielle Lee; Gada Badeer; Georgia Anderson; Graeme Nail; Gregoire Mialon; Guan Pang; Guillem Cucurell; Hailey Nguyen; Hamid Shojanazeri; Hannah Korevaar; Hannah Wang; Haroun Habeeb; Harrison Rudolph; Henry Aspegren; Hu Xu; Hugo Touvron; Iga Kozlowska; Igor Molybog; Igor Tufanov; Iliyan Zarov; Imanol Arrieta Ibarra; Irina-Elena Veliche; Isabel Kloumann; Ishan Misra; Ivan Evtimov; Jacob Xu; Jade Copet; Jake Weissman; Jan Geffert; Jana Vranes; Japhet Asher; Jason Park; Jay Mahadeokar; Jean-Baptiste Gaya; Jeet Shah; Jelmer van der Linde; Jennifer Chan; Jenny Hong; Jenya Lee; Jeremy Fu; Jeremy Teboul; Jianfeng Chi; Jianyu Huang; Jie Wang; Jiecao Yu; Joanna Bitton; Joe Spisak; Joelle Pineau; Jon Carvill; Jongsoo Park; Joseph Rocca; Joshua Johnstun; Junteng Jia; Kalyan Vasuden Alwala; Kam Hou U; Kate Plawiak; Kartikeya Upasani; Kaushik Veeraraghavan; Ke Li; Kenneth Heafield; Kevin Stone; Khalid El-Arini; Krithika Iyer; Kshitiz Malik; Kuenley Chiu; Kunal Bhalla; Kyle Huang; Lakshya Garg; Lauren Rantala-Yeary; Laurens van der Maaten; Lawrence Chen; Leandro Silva; Lee Bell; Lei Zhang; Liang Tan; Louis Martin; Lovish Madaan; Luca Wehrstedt; Lukas Blecher; Luke de Oliveira; Madeline Muzzi; Madian Khabsa; Manav Avlani; Mannat Singh; Manohar Paluri; Mark Zuckerberg; Marcin Kardas; Martynas Mankus; Mathew Oldham; Mathieu Rita; Matthew Lennie; Maya Pavlova; Meghan Keneally; Melanie Kambadur; Mihir Patel; Mikayel Samvelyan; Mike Clark; Mike Lewis; Min Si; Mitesh Kumar Singh; Mo Metanat; Mona Hassan; Naman Goyal; Narjes Torabi; Nicolas Usunier; Nikolay Bashlykov; Nikolay Bogoychev; Niladri Chatterji; Ning Dong; Oliver Aobo Yang; Olivier Duchenne; Onur Celebi; Parth Parekh; Patrick Alrassy; Paul Saab; Pavan Balaji; Pedro Rittner; Pengchuan Zhang; Pengwei Li; Petar Vasic; Peter Weng; Polina Zvyagina; Prajjwal Bhargava; Pratik Dubal; Praveen Krishnan; Punit Singh Koura; Qing He; Rachel Rodriguez; Ragavan Srinivasan; Rahul Mitra; Ramon Calderer; Raymond Li; Robert Stojnic; Roberta Raileanu; Robin Battey; Rocky Wang; Rohit Girdhar; Rohit Patel; Romain Sauvestre; Ronnie Polidoro; Roshan Sumbaly; Ross Taylor; Ruan Silva; Rui Hou; Rui Wang; Russ Howes; Ruty Rinott; Saghar Hosseini; Sai Jayesh Bondu; Samyak Datta; Sanjay Singh; Sara Chugh; Sargun Dhillon; Satadru Pan; Sean Bell; Sergey Edunov; Shaoliang Nie; Sharan Narang; Sharath Raparthy; Shaun Lindsay; Sheng Feng; Sheng Shen; Shenghao Lin; Shiva Shankar; Shruti Bhosale; Shun Zhang; Simon Vandenhende; Sinong Wang; Seohyun Sonia Kim; Soumya Batra; Sten Sootla; Steve Kehoe; Suchin Gururangan; Sumit Gupta; Sunny Virk; Sydney Borodinsky; Tamar Glaser; Tamar Herman; Tamara Best; Tara Fowler; Thomas Georgiou; Thomas Scialom; Tianhe Li; Todor Mihaylov; Tong Xiao; Ujjwal Karn; Vedanuj Goswami; Vibhor Gupta; Vignesh Ramanathan; Viktor Kerkez; Vinay Satish Kumar; Vincent Gonguet; Vish Vogeti; Vlad Poenaru; Vlad Tiberiu Mihailescu; Vladan Petrovic; Vladimir Ivanov; Wei Li; Weiwei Chu; Wenhan Xiong; Wenyin Fu; Wes Bouaziz; Whitney Meers; Will Constable; Xavier Martinet; Xiaojian Wu; Xinbo Gao; Xinfeng Xie; Xuchao Jia; Yaelle Goldschlag; Yann LeCun; Yashesh Gaur; Yasmine Babaei; Ye Qi; Yenda Li; Yi Wen; Yiwen Song; Youngjin Nam; Yuchen Hao; Yuchen Zhang; Yun Wang; Yuning Mao; Yuzi He; Zacharie Delpierre Coudert; Zachary DeVito; Zahra Hankir; Zhaoduo Wen; Zheng Yan; Zhengxing Chen; Zhenyu Yang; Zoe Papakipos,Open weights (restricted use),https://ai.meta.com/blog/meta-llama-3/,,Introducing Meta Llama 3: The most capable openly available LLM to date,2024-04-18,Meta AI,8000000000.0,,7.2e+23,"Counting operations
15000000000000 tokens*8000000000.00 parameters*6 FLOP / token / parameter = 7.2×10^23 FLOP

GPU calculation
400 TFLOPS per GPU * 1.3M GPU hours * 3600s=1.872×10^24 
(it is not confident that 400 TFLOPs applies to the Llama 3-8B training run)",Llama 3 dataset,,15000000000000.0,,,,NVIDIA H100 SXM5 80GB,Confident,,United States of America,,,,,Unreleased,"https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md

License A custom commercial license is available at: https://llama.meta.com/llama3/license",Industry
abab6.5,Language,Language modeling/generation,,API access,https://www.minimaxi.com/en/news/abab65-series,,The General Large Language Model abab6.5 Series,2024-04-17,MiniMax,1000000000000.0,,,,Unspecified unreleased,,,,,,,Confident,"The abab6.5 series includes two models: abab6.5 and abab6.5s. Abab6.5 features a trillion parameters and supports a context length of 200k tokens; abab6.5s uses the same training techniques and data as abab6.5 but is more efficient, supporting a 200k token context length and capable of processing nearly 30,000 words within a second. With significant cost advantages, an industry-leading context length and fast speed, MiniMax's abab6.5 series of LLMs offer unique value propositions.",China,,,,,Unreleased,,Industry
OLMo 1.7-7B ,Language,"Language modeling/generation,Chat",,,https://allenai.org/blog/olmo-1-7-7b-a-24-point-improvement-on-mmlu-92b43f7d269d,,OLMo 1.7–7B: A 24 point improvement on MMLU,2024-04-17,Allen Institute for AI,7000000000.0,,,,Dolma 1.7,,2300000000000.0,The full Dolma 1.7 collection is 2.3 trillion tokens summing across all sources.,,,,,,United States of America,,,,,,"License: The code and model are released under Apache 2.0.

Weights https://huggingface.co/allenai/OLMo-7B
Code https://github.com/allenai/OLMo",Research collective
Reka Core,"Multimodal,Language,Vision","Chat,Language modeling/generation,Image captioning,Code generation,Code autocompletion","Aitor Ormazabal Che Zheng Cyprien de Masson d’Autume Dani Yogatama
Deyu Fu Donovan Ong Eric Chen Eugenie Lamprecht Hai Pham Isaac Ong
Kaloyan Aleksiev Lei Li Matthew Henderson Max Bain Mikel Artetxe
Nishant Relan Piotr Padlewski Qi Liu Ren Chen Samuel Phua
Yazheng Yang Yi Tay Yuqi Wang Zhongkai Zhu Zhihui Xie",API access,https://publications.reka.ai/reka-core-tech-report.pdf,,"Reka Core, Flash, and Edge: A Series of Powerful
Multimodal Language Models",2024-04-15,Reka AI,67000000000.0,,8.400010000000001e+24,"No direct information about Reka Core model (""Reka Core has not finished training and is still improving."")

The smaller dense model Reka Flash has 21B parameters and was trained on 5 trillion language tokens.

There is information about compute: ""Our setup comprises of clusters from a mixture of vendors with our peak compute being approximately
2.5K H100s and 2.5K A100s.""

If we assume 2 months of training with 2.5k H100s and 2.5k A100s at utilization 0.5 we get 8.4e24 FLOP (2500*9.9e14+2500*3.12e14)*60*60*24*60*0.5.","Wikipedia,Unspecified unreleased","The training data comprises a mixture of publicly available and proprietary/licensed datasets with a dataset knowledge cutoff of November 2023. The dataset ingested by our model comprises of text, images, videos, and audio clips. Reka Flash and Reka Edge were trained on approximately 5 trillion and 4.5 trillion extensively deduplicated and filtered language tokens, respectively. While the classification of corpora is not strictly defined to one class or category, approximately 25% of our pretraining data is code related, and 30% are STEM related. Approximately 25% of the data is web crawl. About 10% of our data has some relation to math.",,,,,"NVIDIA A100,NVIDIA H100 SXM5 80GB",Speculative,,United States of America,,,,,Unreleased,,Industry
Reka Flash,"Multimodal,Language,Vision","Chat,Language modeling/generation,Image captioning,Code generation,Code autocompletion","Aitor Ormazabal Che Zheng Cyprien de Masson d’Autume Dani Yogatama
Deyu Fu Donovan Ong Eric Chen Eugenie Lamprecht Hai Pham Isaac Ong
Kaloyan Aleksiev Lei Li Matthew Henderson Max Bain Mikel Artetxe
Nishant Relan Piotr Padlewski Qi Liu Ren Chen Samuel Phua
Yazheng Yang Yi Tay Yuqi Wang Zhongkai Zhu Zhihui Xie",API access,https://publications.reka.ai/reka-core-tech-report.pdf,,"Reka Core, Flash, and Edge: A Series of Powerful
Multimodal Language Models",2024-04-15,Reka AI,21000000000.0,,6.3e+23,"Reka Flash has 21B parameters and was trained on 5 trillion language tokens 

6*21B*5trillion=6.3 × 10^23

Agrees with GPU details: ""Reka Flash and Edge were trained on several hundreds of H100s across a period of several weeks.""

3 weeks * 300 H100s * 7 day/week * 24 hour/day * 3600 s/day * 9.9e14 FLOP/GPU-s = 5.4e23

Not enough info to estimate SFT and RLHF post-training FLOPs.",Unspecified unreleased,"The training data comprises a mixture of publicly available and proprietary/licensed datasets with a dataset
knowledge cutoff of November 2023. The dataset ingested by our model comprises of text, images, videos, and
audio clips. Reka Flash and Reka Edge were trained on approximately 5 trillion and 4.5 trillion extensively
deduplicated and filtered language tokens, respectively. While the classification of corpora is not strictly
defined to one class or category, approximately 25% of our pretraining data is code related, and 30% are
STEM related. Approximately 25% of the data is web crawl. About 10% of our data has some relation to
math. O",5000000000000.0,,,,"NVIDIA A100,NVIDIA H100 SXM5 80GB",Likely,,United States of America,,,,,Unreleased,,Industry
Stable LM 2 12B,Language,"Language modeling/generation,Translation",,Open weights (restricted use),"https://stability.ai/news/introducing-stable-lm-2-12b
https://huggingface.co/stabilityai/stablelm-2-12b",,Introducing Stable LM 2 12B,2024-04-08,Stability AI,12143605760.0,Precise number given in HF model card,2.91e+23,"2* 12143605760 params * 3* 2T tokens * 2 epochs = 2.91e23. 
Trained on 384 H100s (AWS P5 instances).","RefinedWeb,RedPajama-Data,The Pile,StarCoder,CulturaX","The dataset is comprised of a filtered mixture of open-source large-scale datasets available on the HuggingFace Hub: Falcon RefinedWeb extract (Penedo et al., 2023), RedPajama-Data (Together Computer., 2023) and The Pile (Gao et al., 2020) both without the Books3 subset, and StarCoder (Li et al., 2023). We further supplement our training with multi-lingual data from CulturaX (Nguyen et al., 2023) and, in particular, from its OSCAR corpora, as well as restructured data in the style of Yuan & Liu (2022).",2000000000000.0,2T tokens,,,NVIDIA H100 SXM5 80GB,Confident,"Introducing the latest additions to our Stable LM 2 language model series: a 12 billion parameter base model and an instruction-tuned variant, trained on 2 trillion tokens in seven languages: English, Spanish, German, Italian, French, Portuguese, and Dutch. This medium-sized model balances strong performance, efficiency, memory requirements, and speed, following our established Stable LM 2 1.6B framework as detailed in our previously released technical report. With this release, we’re extending our model range, offering a transparent and powerful tool for developers to innovate in AI language technology. Soon, we plan to introduce a long-context variant of these models which will be available on Hugging Face upon release.

From Hugging Face:
Stable LM 2 12B is a 12.1 billion parameter decoder-only language model pre-trained on 2 trillion tokens of diverse multilingual and code datasets for two epochs.","Multinational,United Kingdom of Great Britain and Northern Ireland",,,,,Open source,"Requires Stability AI Membership. Free for non-commercial use, $20/month for commercial use if less than $1M in annual revenue, $1M in institutional funding, and 1M monthly active users. 

Apache 2.0 license for repo, which includes detailed hyperparams and training details: https://github.com/Stability-AI/StableLM/blob/main/LICENSE  ",Industry
Command R+,Language,"Language modeling/generation,Language generation,Translation,Code autocompletion",,Open weights (non-commercial),https://txt.cohere.com/command-r-plus-microsoft-azure/,,,2024-04-04,"Cohere,Cohere for AI",104000000000.0,,,,Unspecified unreleased,,,,,,,,"C4AI Command R+ is an open weights research release of a 104B billion parameter model with highly advanced capabilities, this includes Retrieval Augmented Generation (RAG) and tool use to automate sophisticated tasks. The tool use in this model generation enables multi-step tool use which allows the model to combine multiple tools over multiple steps to accomplish difficult tasks. C4AI Command R+ is a multilingual model evaluated in 10 languages for performance: English, French, Spanish, Italian, German, Brazilian Portuguese, Japanese, Korean, Arabic, and Simplified Chinese. Command R+ is optimized for a variety of use cases including reasoning, summarization, and question answering.","Canada,Multinational,Canada",,,,,Unreleased,"cc-by-nc license, weights only: https://huggingface.co/CohereForAI/c4ai-command-r-plus ","Industry,Industry"
Viking,Language,"Language modeling/generation,Language generation,Translation",,Open weights (unrestricted),https://huggingface.co/LumiOpen/Viking-33B,,"Viking 33B is a 33B parameter decoder-only transformer pretrained on Finnish, English, Swedish, Danish, Norwegian, Icelandic and code. It is being trained on 2 trillion tokens (700B billion as of this release). Viking 33B is a fully open source model and is made available under the Apache 2.0 License.",2024-04-04,"Silo AI,University of Turku",33000000000.0,,2.574e+23,"Plan is to train on 2 trillion tokens, but most recent release is at 1.3T
6 * 33B * 1.3 trillion = 2.574E23",,,2000000000000.0,"Viking is being trained on a 2 trillion token mixed dataset of English, Finnish, Swedish, Danish, Norwegian, Icelandic and code.",,,AMD Radeon Instinct MI250X,Confident,,"Finland,Finland",,,,1024.0,Open source,code here: https://github.com/LumiOpen/Megatron-DeepSpeed/blob/main/pretrain_viking_33B.sh ,"Industry,Academia"
Grok-1.5,Language,"Language modeling,Chat",,Hosted access (no API),https://x.ai/blog/grok-1.5,,"Introducing Grok-1.5, our latest model capable of long context understanding and advanced reasoning. Grok-1.5 will be available to our early testers and existing Grok users on the 𝕏 platform in the coming days.",2024-03-28,xAI,,,9.26e+24,"Lower bound is taken from Grok-1 estimation
Upper bound is taken from Grok-2 estimation

geometric mean: 
sqrt(2.90000000001*29.6)*10^24 = 9.26e+24",Unspecified unreleased,,,,,,,Speculative,,United States of America,,,,,Unreleased,"Musk noted that Grok-1.5 will power xAI’s ChatGPT-challenging chatbot on the X platform, while Grok-2, the successor of the new model, is still in the training phase",Industry
Grok-1.5V,"Multimodal,Language,Vision","Language modeling,Chat,Image captioning,Code autocompletion,Code generation,Visual question answering",,Hosted access (no API),https://x.ai/blog/grok-1.5v,,"Introducing Grok-1.5V, our first-generation multimodal model. In addition to its strong text capabilities, Grok can now process a wide variety of visual information, including documents, diagrams, charts, screenshots, and photographs. Grok-1.5V will be available soon to our early testers and existing Grok users.",2024-03-28,xAI,,,,Lower bound is taken from Grok-1 estimation,Unspecified unreleased,,,,,,,Speculative,,United States of America,,,,,Unreleased,,Industry
YandexGPT 3,Language,"Language modeling/generation,Chat,Question answering,Text summarization,Table tasks",,API access,https://ya.ru/ai/gpt-3,,,2024-03-28,Yandex,,,,,,,,,,,,Unknown,,Russia,,,,,,,Industry
DBRX,Language,"Chat,Code generation",Mosaic Research Team,Open weights (restricted use),https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm,,Introducing DBRX: A New State-of-the-Art Open LLM,2024-03-27,Databricks,132000000000.0,132B mixture of experts. 36B parameters active per inference,2.6e+24,"Mixture of Experts (MoE)

36 billion active params * 12 trillion tokens * 6 ~= 2.6e24
https://www.wolframalpha.com/input?i=6+FLOP+*+36+billion+*+12+trillion

also, it was trained on 3072 NVIDIA H100s, but with an unclear timeframe (end-end process was three months, including evals and red-teaming).",,"12T tokens, text and code

""It was pre-trained on 12T tokens of text and code data...

DBRX was pretrained on 12T tokens of carefully curated data and a maximum context length of 32k tokens. We estimate that this data is at least 2x better token-for-token than the data we used to pretrain the MPT family of models""

from HF: https://huggingface.co/databricks/dbrx-base

The training mix used for DBRX contains both natural-language and code examples. The vast majority of our training data is in the English language",9000000000000.0,"12T tokens is equivalent to 9T words. Though it includes code data, so not very literally 9T words",,,NVIDIA H100 SXM5 80GB,Confident,"Today, we are excited to introduce DBRX, an open, general-purpose LLM created by Databricks. Across a range of standard benchmarks, DBRX sets a new state-of-the-art for established open LLMs. Moreover, it provides the open community and enterprises building their own LLMs with capabilities that were previously limited to closed model APIs; according to our measurements, it surpasses GPT-3.5, and it is competitive with Gemini 1.0 Pro. It is an especially capable code model, surpassing specialized models like CodeLLaMA-70B on programming, in addition to its strength as a general-purpose LLM.",United States of America,,,,,Unreleased,"license: https://www.databricks.com/legal/open-model-license
conditions based on monthly users",Industry
MM1-30B,"Multimodal,Language,Vision","Chat,Image captioning","Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, Yinfei Yang",Unreleased,https://arxiv.org/abs/2403.09611,122.0,"MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training",2024-03-14,Apple,30000000000.0,30B,4.86e+23,"Pre-trained on ~2B image-text pairs and 2T tokens (Table 2). Each image is 144 tokens, so the images are ~300B tokens.
Then additional multimodal training for 400B tokens, for a total of ~2.7T tokens.

This is the final training recipe: ""We initialize both the image encoder and the underlying LLM decoder weights for MM1 from in-house pre-trained models2. We then perform multimodal pre-training on the above data mix for 200k steps (approx. 400B tokens).""

Compute  = 6ND = 6 * 2.7 trillion * 30 billion = 4.86e23

maybe the size of the visual connector is relevant","Conceptual Captions (CC3M),Conceptual Captions 12M (CC12M),COYO-700M,Unspecified unreleased,OBELICS","Text, captioned images. See Table 2",1500000000000.0,at least 2T tokens,,,,Likely,"In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up to 30B parameters, including both dense models and mixture-of-experts (MoE) variants, that are SOTA in pre-training metrics and achieve competitive performance after supervised fine-tuning on a range of established multimodal benchmarks. Thanks to large-scale pre-training, MM1 enjoys appealing properties such as enhanced in-context learning, and multi-image reasoning, enabling few-shot chain-of-thought prompting.",United States of America,,,,,Unreleased,,Industry
Inflection-2.5,Language,Chat,,Hosted access (no API),https://inflection.ai/inflection-2-5,,Inflection-2.5: meet the world's best personal AI,2024-03-07,Inflection AI,,,1.0001e+25,"""Inflection-1 used approximately 4% the training FLOPs of GPT-4 and, on average, performed at approximately 72% GPT-4 level on a diverse range of IQ-oriented tasks. Inflection-2.5, now powering Pi, achieves more than 94% the average performance of GPT-4 despite using only 40% the training FLOPs.""

This is a weird one - we estimated GPT-4 at 2.1e25 FLOP (which could be off somewhat, or Inflection could believe a different number). 40% of that is ~8e24. But Inflection 2, the previous model, was trained on ~1e25 FLOP per Inflection. Inflection-2.5 also does better on benchmarks than 2. Intuitively Inflection-2.5 would be trained on appreciably more compute. 

1e25 seems like a rough, perhaps conservative guess given all this.",,,,,,,NVIDIA H100 SXM5 80GB,Speculative,"At Inflection, our mission is to create a personal AI for everyone. Last May, we released Pi—a personal AI, designed to be empathetic, helpful, and safe. In November we announced a new major foundation model, Inflection-2, the second best LLM in the world at the time.

Now we are adding IQ to Pi’s exceptional EQ.

We are launching Inflection-2.5, our upgraded in-house model that is competitive with all the world's leading LLMs like GPT-4 and Gemini. It couples raw capability with our signature personality and unique empathetic fine-tuning. Inflection-2.5 is available to all Pi's users today, at pi.ai, on iOS, on Android, or our new desktop app.",United States of America,,,,,Unreleased,,Industry
Claude 3 Haiku,Language,"Chat,Code generation,Language modeling/generation,Question answering",,API access,https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,,"The Claude 3 Model Family: Opus, Sonnet, Haiku",2024-03-04,Anthropic,,,,,Unspecified unreleased,"Claude 3 models are trained on a proprietary mix of publicly available information on the Internet as of August 2023, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we generate internally. We employ several data cleaning and filtering methods, including deduplication and classification. The Claude 3 suite of models have not been trained on any user prompt or output data submitted to us by users or customers, including free users, Claude Pro users, and API customers.",,,,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",,Unknown,"We introduce Claude 3, a new family of large multimodal models – Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on
measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].
",United States of America,,,,,Unreleased,,Industry
Claude 3 Sonnet,"Multimodal,Language,Vision","Chat,Image captioning,Code generation,Language modeling/generation",,API access,https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,,"The Claude 3 Model Family: Opus, Sonnet, Haiku",2024-03-04,Anthropic,,,,,Unspecified unreleased,"Claude 3 models are trained on a proprietary mix of publicly available information on the Internet as of August 2023, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we generate internally. We employ several data cleaning and filtering methods, including deduplication and classification. The Claude 3 suite of models have not been trained on any user prompt or output data submitted to us by users or customers, including free users, Claude Pro users, and API customers.",,,,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",,Unknown,"We introduce Claude 3, a new family of large multimodal models – Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on
measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].
",United States of America,,,,,Unreleased,,Industry
Claude 3 Opus,"Multimodal,Language,Vision","Chat,Image captioning,Code generation,Language modeling/generation",,API access,https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,,"The Claude 3 Model Family: Opus, Sonnet, Haiku",2024-03-04,Anthropic,,,1.640001e+25,Training compute estimated from benchmark scores.,Unspecified unreleased,"Claude 3 models are trained on a proprietary mix of publicly available information on the Internet as of August 2023, as well as non-public data from third parties, data provided by data labeling services and paid contractors, and data we generate internally. We employ several data cleaning and filtering methods, including deduplication and classification. The Claude 3 suite of models have not been trained on any user prompt or output data submitted to us by users or customers, including free users, Claude Pro users, and API customers.",,,,"Like its predecessors, Claude 3 models employ various training methods, such as unsupervised learning and Constitutional AI [6]. These models were trained using hardware from Amazon Web Services (AWS) and Google Cloud Platform (GCP)",,Speculative,"We introduce Claude 3, a new family of large multimodal models – Claude 3 Opus, our most capable offering, Claude 3 Sonnet, which provides a combination of skills and speed, and Claude 3 Haiku, our fastest and least expensive model. All new models have vision capabilities that enable them to process and analyze image data. The Claude 3 family demonstrates strong performance across benchmark evaluations and sets a new standard on
measures of reasoning, math, and coding. Claude 3 Opus achieves state-of-the-art results on evaluations like GPQA [1], MMLU [2], MMMU [3] and many more. Claude 3 Haiku performs as well or better than Claude 2 [4] on most pure-text tasks, while Sonnet and Opus significantly outperform it. Additionally, these models exhibit improved fluency in non-English languages, making them more versatile for a global audience. In this report, we provide an in-depth analysis of our evaluations, focusing on core capabilities, safety, societal impacts, and the catastrophic risk assessments we committed to in our Responsible Scaling Policy [5].
",United States of America,,,,,Unreleased,,Industry
Aramco Metabrain AI,Language,Language modeling/generation,Saudi Aramco,Unreleased,https://www.offshore-technology.com/news/saudi-aramco-unveils-industry-first-generative-ai-model/,,Saudi Aramco unveils industry’s first generative AI model,2024-03-04,Saudi Aramco,250000000000.0,"""It has 250 billion parameters that are adjustable during training to generate outputs or make predictions.""",1.05e+25,6*250B*7T=1.05e+25,,"""The AI was trained using seven trillion data points, collecting more than 90 years of company history.""",7000000000000.0,,,,,Likely,,Saudi Arabia,,,,,,,"Industry,Government"
Step-2,Language,"Language modeling/generation,Question answering",,API access,"https://www.sohu.com/a/791101272_114877
https://platform.stepfun.com/#step2",,"100天后，阶跃星辰交出了第二份答卷
",2024-03-01,StepFun,2000000000000.0,,,,,,,,,,,Unverified,"A new generation of MoE architecture large model, with parameters breaking through trillions. Model performance/sensory/planning capabilities are fully approaching the international mainstream model to meet the various needs of users in the Chinese/English field, reflecting the cutting-edge results of Scaling Laws.",China,,,,,,,Industry
StarCoder 2 15B,Language,"Code generation,Code autocompletion","Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries",Open weights (restricted use),https://arxiv.org/abs/2402.19173,,StarCoder 2 and The Stack v2: The Next Generation,2024-02-29,"Hugging Face,ServiceNow,NVIDIA,BigCode",15000000000.0,15B,3.87e+23,estimation is given in Table 6 ,The Stack v2,See Table 4. The Stack V2 plus some extras. Created from repositorites from Github with permissive licences.,913230000000.0,from Table 4,,,,Confident,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ","Multinational,United States of America,United States of America,United States of America",,,,,Unreleased,"commercial use allowed, but various use cases restricted: https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement

code is fine-tune only: https://github.com/bigcode-project/starcoder2?tab=readme-ov-file#fine-tuning ","Industry,Industry,Industry"
StarCoder 2 7B,Language,"Code generation,Code autocompletion","Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries",Open weights (restricted use),https://arxiv.org/abs/2402.19173,,StarCoder 2 and The Stack v2: The Next Generation,2024-02-29,"Hugging Face,ServiceNow,NVIDIA,BigCode",7000000000.0,7B,1.55e+23,estimation is given in Table 6 ,The Stack v2,See Table 4. The Stack V2 plus some extras. Created from repositorites from Github with permissive licences.,658580000000.0,from Table 4,,"""A cumulative of 145,152 hours of computation was performed on hardware of type H100""",NVIDIA H100 SXM5 80GB,Confident,"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data. ","Multinational,United States of America,United States of America,United States of America",,,,,,https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement,"Industry,Industry,Industry"
Nemotron-4 15B,Language,"Language modeling/generation,Code generation,Question answering,Translation,Quantitative reasoning","Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, Vibhu Jawa, Jiwei Liu, Ameya Mahabaleshwarkar, Osvald Nitski, Annika Brundyn, James Maki, Miguel Martinez, Jiaxuan You, John Kamalu, Patrick LeGresley, Denys Fridman, Jared Casper, Ashwath Aithal, Oleksii Kuchaiev, Mohammad Shoeybi, Jonathan Cohen, Bryan Catanzaro",Unreleased,https://arxiv.org/abs/2402.16819,,Nemotron-4 15B Technical Report,2024-02-27,NVIDIA,15000000000.0,15b,7.5005116e+23,"6ND = 6 FLOP/token/parameter * 15*10^9 parameters * 8*10^12 tokens = 7.2e+23 FLOP

""Nemotron-4 was trained using 384 DGX H100 nodes; each node contains 8 H100 80GB SXM5 GPUs based on the NVIDIA Hopper architecture (NVIDIA, 2022). Each H100 GPU has a peak throughput of 989 teraFLOP/s when doing 16-bit floating point (bfloat16) arithmetic without sparsity.

Table 2 reports more detailed training schedule:

989*10^12 FLOP/sec * 3600 sec/hour * 24 hours * (768 gpus * 0.343 [reported utilization] * 0.8 days +  1536 gpus * 0.333 [reported utilization] * 0.4 days + 2304 gpus * 0.305 [reported utilization] * 11.9 days) = 7.5005116e+23 FLOP",Unspecified unreleased,"""At a high-level, the data blend is split into three different types of data: English natural language data (70%), multilingual natural language data (15%), and source-code data (15%).""",8000000000000.0,"""15-billion-parameter large multilingual language model trained on 8 trillion text tokens""",312.0,"""Training was completed in approximately 13 calendar days.""",NVIDIA H100 SXM5 80GB,Confident,"We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks.",United States of America,,,,3072.0,Unreleased,,Industry
Mistral Large,Language,Chat,,API access,https://mistral.ai/news/mistral-large/,,"Mistral Large, our new flagship model",2024-02-26,Mistral AI,,,1.1199999999999999e+25,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48

Mistral spent <20 million euro (meaning approximately 20 million?) to train Mistral Large:

https://x.com/EMostaque/status/1762152740938031484?s=20
""assuming this is on H100s with @Scaleway who are €1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad Mostaque

Assuming bf16 or fp16, H100 SXM performance is 989 TFLOPS
At 1.9 euro per H100-hour and 30% utilization, spending 20M euro produces 1.12*10^25 FLOP.
https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+989+TFLOPS+*+0.30 ",,,,,2500.0,Speculation by Emad Mostaque: 20M euro spent at Scaleway (1.9 euro per H100-hour) would be around 3 months on 4000 H100s.,NVIDIA H100 SXM5 80GB,Likely,,France,,,,,Unreleased,,Industry
Gemma 1.1 7B Instruct,Language,"Language modeling/generation,Question answering","Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot and et al.",Open weights (restricted use),https://huggingface.co/google/gemma-1.1-7b-it,,,2024-02-24,Google,8540000000.0,"Safetensors 
Model size 8.54B params",3.0744e+23,6ND = 6*6000000000000*8540000000=3.0744e+23,Unspecified unreleased,"""These models were trained on a dataset of text data that includes a wide variety of sources, totaling 6 trillion tokens. Here are the key components:

Web Documents: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. Primarily English-language content.
Code: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code or understand code-related questions.
Mathematics: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries.""",6000000000000.0,"""These models were trained on a dataset of text data that includes a wide variety of sources, totaling 6 trillion tokens. """,,,Google TPU v5e,Confident,"This is Gemma 1.1 7B (IT), an update over the original instruction-tuned Gemma release.

Gemma 1.1 was trained using a novel RLHF method, leading to substantial gains on quality, coding capabilities, factuality, instruction following and multi-turn conversation quality. We also fixed a bug in multi-turn conversations, and made sure that model responses don't always start with ""Sure,"".

We believe this release represents an improvement for most use cases, but we encourage users to test in their particular applications. The previous model will continue to be available in the same repo. We appreciate the enthusiastic adoption of Gemma, and we continue to welcome all feedback from the community.",United States of America,,,,,Unreleased,"https://huggingface.co/google/gemma-1.1-7b-it

""This repository is publicly accessible, but you have to accept the conditions to access its files and content.""",Industry
MegaScale (175B),Language,Language modeling/generation,"Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, Xin Liu",Unreleased,https://arxiv.org/abs/2402.15627,40.0,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",2024-02-23,"ByteDance,Peking University",175000000000.0,"Two models are trained for epoch to evaluate the MegaScale training system; one model with 175B and another with 530B parameters. This entry reports the 175B model.

There is a third production model mentioned, with fewer details.",2.7385671436e+23,"Table 2 gives details for the 175B model. Looking at the largest 1 epoch run with 12288 GPUs:
2166.3 aggregate PFlops/sec * 1.75 days * 24 hours/day * 3600 seconds/hour = 3.275e23

This is consistent with the theoretical computation counting estimate, if they factor MFU rate into their 2166.3 figure:
2 × 175B params × 3 × 300B tokens × 1 epoch = 2.29e23

I use the geometric mean of these two:
(3.275e23 + 2.29e23) / 2 = 2.74e23",,175B and 530B models trained for paper use 300B tokens each.,225000000000.0,300B tokens * 0.75 words/token = 225B words,42.0,,NVIDIA A100,Confident,"We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.","China,China",,,,12288.0,Unreleased,"repo, but no training code for the big model https://github.com/volcengine/vescale
Model weights unreleased","Industry,Academia"
MegaScale (530B),Language,Language modeling/generation,"Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, Xin Liu",Unreleased,https://arxiv.org/abs/2402.15627,40.0,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",2024-02-23,"ByteDance,Peking University",530000000000.0,"Two models are trained for epoch to evaluate the MegaScale training system; one model with 175B and another with 530B parameters. This entry reports the 530B model.

There is a third production model mentioned, with fewer details.",9.6910000000001e+23,"175B models uses 3.2e23 FLOPs (Table 2, bottom row)

With constant dataset size and utilization, FLOPs should scale linearly in # parameters, so: 3.2e23 * (530/175) = 9.7e23",,175B and 530B models trained for paper use 300B tokens each.,300000000000.0,300B tokens,117.9,"175B parameter model is stated to have taken 1.75 days. 500B model used more compute, fewer GPUs, and had slightly worse MFU. Accounting for these: 
1.75 days * 24 hours/day * (9.7e23/3.2e23) * (11200/12288) * (0.552/0.543) = 117.9 hours",NVIDIA A100,Confident,"We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.","China,China",,,,11200.0,Unreleased,"they open-sourced their framework but don't see training code for their big model. 
https://github.com/volcengine/vescale
Model weights are unreleased","Industry,Academia"
MegaScale (Production),Language,Language modeling/generation,"Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, Yulu Jia, Sun He, Hongmin Chen, Zhihao Bai, Qi Hou, Shipeng Yan, Ding Zhou, Yiyao Sheng, Zhuo Jiang, Haohan Xu, Haoran Wei, Zhang Zhang, Pengfei Nie, Leqi Zou, Sida Zhao, Liang Xiang, Zherui Liu, Zhe Li, Xiaoying Jia, Jianxi Ye, Xin Jin, Xin Liu",Unreleased,https://arxiv.org/abs/2402.15627,40.0,"MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs",2024-02-23,"ByteDance,Peking University",530000000000.0,"Production run is stated to have ""hundreds of billions of parameters"". Since the authors also do a number of experiments with a 530B model, I speculate they've used 530B for the production model.",3.9e+24,"Speculative. The model is stated to have trained for ""several weeks"". Assuming 530B parameters and ""several"" = 3, compute can be estimated from the 175B model's stated PFLOP/sec:
2166.3 aggregate PFlops/sec * 3 weeks * 7 days/week * 24 hours/day * 3600 seconds/hour = 3.9e+24.
As an upper bound, say 8e+24. ",,,,"Speculative. Authors note production system was trained on ""multi-trillions of tokens"". This could refer to training for multiple epochs on the same 300B tokens used to train the 175B and 530B models outlined in more detail in the paper. Alternatively, it could refer to a larger dataset of perhaps 3-9 trillion tokens.",504.0,"Speculative. Authors state ""several weeks"". For analysis, I've assumed this means around 3 weeks.",NVIDIA A100,Speculative,"We present the design, implementation and engineering experience in building and deploying MegaScale, a production system for training large language models (LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale brings unprecedented challenges to training efficiency and stability. We take a full-stack approach that co-designs the algorithmic and system components across model block and optimizer design, computation and communication overlapping, operator optimization, data pipeline, and network performance tuning. Maintaining high efficiency throughout the training process (i.e., stability) is an important consideration in production given the long extent of LLM training jobs. Many hard stability issues only emerge at large scale, and in-depth observability is the key to address them. We develop a set of diagnosis tools to monitor system components and events deep in the stack, identify root causes, and derive effective techniques to achieve fault tolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs Utilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the MFU by 1.34x compared to Megatron-LM. We share our operational experience in identifying and fixing failures and stragglers. We hope by articulating the problems and sharing our experience from a systems perspective, this work can inspire future LLM systems research.","China,China",,,,12288.0,Unreleased,"Code for MegaScale (also called veScale) training system are released under Apache Licence: https://github.com/volcengine/vescale
The model itself is unreleased.","Industry,Academia"
Gemma 7B,Language,"Language modeling/generation,Chat,Code generation,Question answering,Quantitative reasoning","Gemma Team, Google DeepMind",Open weights (restricted use),https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf,,Gemma: Open Models Based on Gemini Research and Technology,2024-02-21,Google DeepMind,8538074112.0,"Table 2, sum of embedding and non-embedding parameters",3.07e+23,"6ND aproximation 6*8.54B*6T = 3.07e23
""Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""

As confirmation: ""We estimate the carbon emissions from pretraining the Gemma models to be ∼ 131 𝑡𝐶𝑂2𝑒𝑞. ""

U.S. avg CO2 per kWh is ~0.87lbs 
131 tCO2 * 2000 lb/t * (1 kWh/0.87lb) = 3.01e5 kWh

Per SemiAnalysis TPU v5e uses ~ 5x less power than H100, so ~140 W TDP
3.01e5 kWh * 1000 W/kW * 1 TPUv5e/140 W  = 2.15e6 TPUv5e-hours

In bf16 precision, TPUv5e has peak performance of 197 TF/s, so:
2.15e6 * 3600 * 197e12 * 0.3 = 4.57e23",Unspecified unreleased,"""Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""",6000000000000.0,"""Gemma 2B and 7B are trained on 2T and 6T tokens respectively of primarily-English data from web documents, mathematics, and code.""
Not explicitly stated that this doesn't involve multiple epochs, but I expect it does not.",,,Google TPU v5e,Confident,,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,4096.0,Unreleased,"https://ai.google.dev/gemma/terms

no illegal use or abuse",Industry
Sora,"Video,Vision",Video generation,,Unreleased,https://openai.com/index/video-generation-models-as-world-simulators/,,Video generation models as world simulators,2024-02-15,OpenAI,,,,,Unspecified unreleased,"Sora was trained on diverse datasets, including a mix of publicly available data, proprietary data accessed through partnerships, and custom datasets developed in-house. These consist of:

Select publicly available data, mostly collected from industry-standard machine learning datasets and web crawls.

Proprietary data from data partnerships. We form partnerships to access non-publicly available data. For example, we partnered with Shutterstock⁠ Pond5 on building and delivering AI-generated images. We also partner to commission and create datasets fit for our needs.

Human data: Feedback from AI trainers, red teamers, and employees. ",,,,,,Unknown,"Sora is OpenAI’s video generation model, designed to take text, image, and video inputs and generate a new video as an output. Users can create videos up to 1080p resolution (20 seconds max) in various formats, generate new content from text, or enhance, remix, and blend their own assets. Users will be able to explore the Featured and Recent feeds which showcase community creations and offer inspiration for new ideas. Sora builds on learnings from DALL·E and GPT models, and is designed to give people expanded tools for storytelling and creative expression. 

Sora is a diffusion model, which generates a video by starting off with a base video that looks like static noise and gradually transforms it by removing the noise over many steps. By giving the model foresight of many frames at a time, we’ve solved a challenging problem of making sure a subject stays the same even when it goes out of view temporarily. Similar to GPT models, Sora uses a transformer architecture, unlocking superior scaling performance. ",United States of America,,,,,Unreleased,,Industry
Gemini 1.5 Pro,"Language,Multimodal","Language modeling,Visual question answering",Gemini Team,API access,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,,Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,2024-02-15,Google DeepMind,,MoE architecture,1.580001e+25,Training compute imputed from benchmark scores.,Unspecified unreleased,,,,,,Google TPU v4,Speculative,,"United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,Unreleased,API access: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models,Industry
Gemini 1.0 Pro Vision,"Vision,Language,Video","Language modeling,Visual question answering,Chat,Translation,Video description,Question answering",Gemini Team,API access,,,The best performing image and video understanding model to handle a broad range of applications. ,2024-02-15,Google DeepMind,,,,,Unspecified unreleased,,,,,,,Unknown,"Gemini 1.0 Pro Vision supports text, image, and video as inputs.

Max input tokens: 16,384
Max output tokens: 2,048
Max images per prompt: 16
Max video length: 2 minutes
Max videos per prompt: 1
Training data: Up to February 2023","United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,Unreleased,API Access: https://console.cloud.google.com/vertex-ai/generative/multimodal/create/text?model=gemini-1.0-pro-vision-001,Industry
Sora Turbo,"Video,Vision",Video generation,,Unreleased,https://openai.com/index/sora-is-here/,,Sora is here,2024-02-15,OpenAI,,,,,Unspecified unreleased,"Sora was trained on diverse datasets, including a mix of publicly available data, proprietary data accessed through partnerships, and custom datasets developed in-house. These consist of:

Select publicly available data, mostly collected from industry-standard machine learning datasets and web crawls.

Proprietary data from data partnerships. We form partnerships to access non-publicly available data. For example, we partnered with Shutterstock⁠ Pond5 on building and delivering AI-generated images. We also partner to commission and create datasets fit for our needs.

Human data: Feedback from AI trainers, red teamers, and employees. ",,,,,,Unknown,"Our video generation model is rolling out at sora.com⁠(opens in a new window).

Earlier this year, we introduced Sora⁠, our model that can create realistic videos from text, and shared our initial research progress⁠ on world simulation. Sora serves as a foundation for AI that understands and simulates reality—an important step towards developing models that can interact with the physical world.

We developed a new version of Sora—Sora Turbo—that is significantly faster than the model we previewed in February. We’re releasing it today as a standalone product at Sora.com to ChatGPT Plus and Pro users.",United States of America,,,,,Unreleased,,Industry
KwaiYii 175B,Language,Language modeling/generation,,,https://blog.csdn.net/kuaishoutech/article/details/140542568,,,2024-02-14,Kuaishou Technology,175000000000.0,175B,,,,,,,,,,Confident,"In June 2024, quick-handed NLP experts reported on the “KwaiYii” model at the Global Artificial Intelligence Technology Conference. The model was developed in early 2023, and the 175B model was released at the end of February 2024. Many capabilities are close to the latest version of GPT-4. Introduced eight key technological innovations and landing practices in scenes such as AI Xiaofu, including solutions to various challenges, which will continue to be iterated in the future.",China,,,,,Unreleased,,Industry
Qwen1.5-32B,Language,"Chat,Language modeling/generation,Quantitative reasoning,Code generation,Translation",Qwen Team,Open weights (restricted use),https://qwenlm.github.io/blog/qwen1.5/,,Introducing Qwen1.5,2024-02-05,Alibaba,32000000000.0,32B,,"upper bound is taken from Qwen1.5 72B training compute estimation

lower bound is taken from Qwen1.5 14B training compute estimation",Unspecified unreleased,,,,,,,Confident,"Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen, the improvements include:

8 model sizes, including 0.5B, 1.8B, 4B, 7B, 14B, 32B and 72B dense models, and an MoE model of 14B with 2.7B activated;
Significant performance improvement in human preference for chat models;
Multilingual support of both base and chat models;
Stable support of 32K context length for models of all sizes
No need of trust_remote_code.",China,,,,,Unreleased,"https://huggingface.co/Qwen/Qwen1.5-32B
""If you are commercially using the Materials, and your product or service has more than 100 million monthly active users, You shall request a license from Us""",Industry
Qwen1.5-72B,Language,"Chat,Language modeling/generation,Quantitative reasoning,Code generation,Translation",Qwen Team,Open weights (restricted use),https://qwenlm.github.io/blog/qwen1.5/,,Introducing Qwen1.5,2024-02-04,Alibaba,72000000000.0,72B,1.3e+24,"3T training tokens: https://github.com/QwenLM/Qwen2/issues/97 

6 * 72 billion * 3 trillion = ~1.3e24",Unspecified unreleased,"""We pretrained the models with a large amount of data, and we post-trained the models with both supervised finetuning and direct preference optimization.""",3000000000000.0,3 trillion tokens from this response https://github.com/QwenLM/Qwen2/issues/97,,,,Confident,"In recent months, our focus has been on developing a “good” model while optimizing the developer experience. As we progress towards Qwen1.5, the next iteration in our Qwen series, this update arrives just before the Chinese New Year. With Qwen1.5, we are open-sourcing base and chat models across six sizes: 0.5B, 1.8B, 4B, 7B, 14B, and 72B. In line with tradition, we’re also providing quantized models, including Int4 and Int8 GPTQ models, as well as AWQ and GGUF quantized models. To enhance the developer experience, we’ve merged Qwen1.5’s code into Hugging Face transformers, making it accessible with transformers>=4.37.0 without needing trust_remote_code.",China,,,,,Unreleased,"restriction on >100m monthly users:

https://huggingface.co/Qwen/Qwen1.5-72B/blob/main/LICENSE",Industry
Qwen1.5-7B,Language,"Chat,Language modeling/generation,Quantitative reasoning,Code generation,Translation",Qwen Team,Open weights (unrestricted),https://huggingface.co/Qwen/Qwen1.5-7B,,Introducing Qwen1.5,2024-02-04,Alibaba,7000000000.0,7B,1.68e+23,6 FLOP / parameter / token * 7*10^9 parameters * 4*10^12 tokens =  1.68e+23 FLOP,Unspecified unreleased,"""We pretrained the models with a large amount of data, and we post-trained the models with both supervised finetuning and direct preference optimization.""",4000000000000.0,4 trillion tokens from this response https://github.com/QwenLM/Qwen2/issues/97,,,,Confident,"Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen, the improvements include:

8 model sizes, including 0.5B, 1.8B, 4B, 7B, 14B, 32B and 72B dense models, and an MoE model of 14B with 2.7B activated;
Significant performance improvement in human preference for chat models;
Multilingual support of both base and chat models;
Stable support of 32K context length for models of all sizes
No need of trust_remote_code.",China,,,,,Unreleased,https://huggingface.co/Qwen/Qwen1.5-7B,Industry
Qwen1.5-14B,Language,"Chat,Language modeling/generation,Quantitative reasoning,Code generation,Translation",Qwen Team,Open weights (unrestricted),https://huggingface.co/Qwen/Qwen1.5-14B,,Introducing Qwen1.5,2024-02-04,Alibaba,14000000000.0,14B,3.36e+23,6 FLOP / parameter / token * 14*10^9 parameters * 4*10^12 tokens =  3.36e+23 FLOP,Unspecified unreleased,"""We pretrained the models with a large amount of data, and we post-trained the models with both supervised finetuning and direct preference optimization.""",4000000000000.0,4 trillion tokens from this response https://github.com/QwenLM/Qwen2/issues/97,,,,Confident,"Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. In comparison with the previous released Qwen, the improvements include:

8 model sizes, including 0.5B, 1.8B, 4B, 7B, 14B, 32B and 72B dense models, and an MoE model of 14B with 2.7B activated;
Significant performance improvement in human preference for chat models;
Multilingual support of both base and chat models;
Stable support of 32K context length for models of all sizes
No need of trust_remote_code.",China,,,,,Unreleased,https://huggingface.co/Qwen/Qwen1.5-14B,Industry
OLMo-7B,Language,"Language modeling/generation,Chat","Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hannaneh Hajishirzi",Open weights (unrestricted),https://arxiv.org/abs/2402.00838v1,,OLMo: Accelerating the Science of Language Models,2024-02-01,"Allen Institute for AI,University of Washington",7000000000.0,,1.0332e+23,"direct calculation:
6*7B*2.46trillion=1.0332 × 10^23

(calculation also reporoduced by the developers in https://arxiv.org/pdf/2501.00656)",Dolma,,2000000000000.0,"""We built our training dataset out of a 2T-token sample from our open dataset, Dolma [...] All of our released models have been trained to at least 2T tokens (a single epoch over our training data), and some have been trained beyond that by starting a second epoch over the data with a different shuffling order"" 

Table 1 indicates total tokens seen are 2.46T for the 7B parameter model, though note that a later release in July 2024 has been trained to 2.75T tokens: https://github.com/allenai/OLMo?tab=readme-ov-file",,,"AMD Radeon Instinct MI250X,NVIDIA A100",Confident,"Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.","United States of America,United States of America",,,,,Open source,"License: The code and model are released under Apache 2.0.

Weights https://huggingface.co/allenai/OLMo-7B
Code https://github.com/allenai/OLMo","Research collective,Academia"
DeepSeek Coder 33B,Language,Code generation,"Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y.K. Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang",Open weights (restricted use),https://arxiv.org/abs/2401.14196,,DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence,2024-01-25,"DeepSeek,Peking University",33000000000.0,33B,3.96e+23,"""Step 1: Initially pre-trained with a dataset consisting of 87% code, 10% code-related language (Github Markdown and StackExchange), and 3% non-code-related Chinese language. Models are pre-trained using 1.8T tokens and a 4K window size in this step.
Step 2: Further Pre-training using an extended 16K window size on an additional 200B tokens, resulting in foundational models (DeepSeek-Coder-Base).
Step 3: Instruction Fine-tuning on 2B tokens of instruction data, resulting in instruction-tuned models (DeepSeek-Coder-Instruct).""

This means it was trained on 2T tokens. 2T * 33B * 6 = 3.96e23",,"""Trained from scratch on 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages.""",2000000000000.0,"""Trained from scratch on 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages.""

""The total data volume is 798 GB with 603 million files.""",,,,Likely,"The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.","China,China",,,,,Unreleased,"code doesn't seem to be training code.

deepseek license:
https://github.com/deepseek-ai/DeepSeek-Coder/blob/main/LICENSE-MODEL","Industry,Academia"
Qwen-VL-Max,"Multimodal,Language,Vision","Chat,Image captioning,Face recognition,Visual question answering",,API access,https://qwenlm.github.io/blog/qwen-vl/,,Introducing Qwen-VL,2024-01-25,Alibaba,7000000000.0,"Not stated. Qwen-VL (less capable, presumably smaller version) is 9.6B

Upd: 7B parameters mentioned here
https://github.com/QwenLM/Qwen-VL#qwen-vl-plus",,,Unspecified unreleased,,,,,,,Confident,"Along with the rapid development of our large language model Qwen, we leveraged Qwen’s capabilities and unified multimodal pretraining to address the limitations of multimodal models in generalization, and we opensourced multimodal model Qwen-VL in Sep. 2023. Recently, the Qwen-VL series has undergone a significant upgrade with the launch of two enhanced versions, Qwen-VL-Plus and Qwen-VL-Max. The key technical advancements in these versions include:

Substantially boost in image-related reasoning capabilities;
Considerable enhancement in recognizing, extracting, and analyzing details within images and texts contained therein;
Support for high-definition images with resolutions above one million pixels and images of various aspect ratios.",China,,,,,,https://help.aliyun.com/zh/dashscope/developer-reference/tongyi-qianwen-vl-plus-api,Industry
Fuyu-Heavy,"Multimodal,Language",Chat,,,https://www.adept.ai/blog/adept-fuyu-heavy,,Adept Fuyu-Heavy: A new multimodal model,2024-01-24,Adept,100000000000.0,"""Fuyu-Heavy is the world’s third-most-capable multimodal model, behind only GPT4-V and Gemini Ultra, which are 10-20 times bigger""

So possibly around ~100B params, though GPT-4/Gemini params aren't public",,Nvidia hardware,,"curated/generated image data:

""high-quality image pre-training data is scarce, we’ve devoted a lot of effort to collecting, curating, and even creating this data. There’s also a delicate balance between text and image tasks — we had to develop recipes for striking this balance at scale""",,,,,,Speculative,"We’re excited to introduce Adept Fuyu-Heavy, a new multimodal model designed specifically for digital agents. Fuyu-Heavy is the world’s third-most-capable multimodal model, behind only GPT4-V and Gemini Ultra, which are 10-20 times bigger. We’re excited about this model because:

It excels at multimodal reasoning. To us the killer feature is UI understanding, but it also performs well on more traditional multimodal benchmarks. In particular, Fuyu-Heavy scores higher on the MMMU benchmark than even Gemini Pro.
On standard text-based benchmarks, it matches or exceeds the performance of models in the same compute class despite having to devote some of its capacity to image modeling.
It demonstrates that (with some modifications) we can scale up the Fuyu architecture and reap all of the associated benefits, including handling arbitrary size/shape images and efficiently re-using existing transformer optimizations.",United States of America,,,,,,,Industry
GLM-4 (0116),Language,"Language modeling/generation,Question answering,Code generation,Quantitative reasoning,Translation","Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang",API access,"https://arxiv.org/abs/2406.12793
https://zhipuai.cn/en/devday",,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools,2024-01-17,Zhipu AI,,"ChatGLM was 130B parameters, and the paper implies GLM-4 was scaled larger than previous models.",1.2e+25,"- 0116 has slightly worse performance than 0520
- “the GLM-4 models are pre-trained on ten trillions of tokens”
- I did not find any information about parameters or compute. Over here they speculatively estimate GLM-4 to be 200B parameters (which seems plausible to me), though no source provided. 
- “GLM-4 gets close to the state-of-the-art models (GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus)”  none of these models has parameters disclosed or compute estimation.

6*10000000000000*200000000000 = 1.2e+25 FLOPs with “Likely” confidence (+/- 1 OOM)",,"""To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage""",10000000000000.0,,,,,Likely,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",China,,,,,,"GLM-4 (0116) has been made available through the GLM-4 API at
https://bigmodel.cn",Industry
GLM-4,"Language,Multimodal,Image generation","Language modeling/generation,Question answering,Code generation,Text-to-image,Image generation","Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang",Hosted access (no API),https://arxiv.org/abs/2406.12793,,ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools,2024-01-17,Zhipu AI,,,,,,"""To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage""",10000000000000.0,,,,,Confident,"We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through this https URL and this https URL.",China,,,,,,GLM-4 All Tools is accessible via the website https://chatglm.cn,Industry
InternLM2-20B,Language,"Chat,Language modeling/generation,Question answering","Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, Dahua Lin",Open weights (restricted use),https://arxiv.org/abs/2403.17297,,InternLM2 Technical Report,2024-01-12,"Shanghai AI Lab,SenseTime,Chinese University of Hong Kong (CUHK),Fudan University",20000000000.0,20B,3.12e+23,6ND = 6 * 2600000000000 * 20000000000 = 3.12e+23,Unspecified unreleased,"""The text data in our pre-training dataset can be categorized by source into web pages,
papers, patents, and books. To transform these sources into a pre-training dataset, we first
standardize all data into a specified format, categorize them by type and language, and
store them in JSON Lines (jsonl) format. Then, for all data, we apply several processing
steps including rule-based filtering, data deduplication, safety filtering, and quality filtering.
This results in a rich, safe, and high-quality text dataset.""",2600000000000.0,"""The total number of tokens used for pre-training the 1.8B, 7B, and 20B models ranges from 2.0T to 2.6T, and the pre-training process consists of three distinct phases. """,,,,Confident,"The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has sparked discussions on the advent of Artificial General Intelligence (AGI). However, replicating such advancements in open-source models has been challenging. This paper introduces InternLM2, an open-source LLM that outperforms its predecessors in comprehensive evaluations across 6 dimensions and 30 benchmarks, long-context modeling, and open-ended subjective evaluations through innovative pre-training and optimization techniques. The pre-training process of InternLM2 is meticulously detailed, highlighting the preparation of diverse data types including text, code, and long-context data. InternLM2 efficiently captures long-term dependencies, initially trained on 4k tokens before advancing to 32k tokens in pre-training and fine-tuning stages, exhibiting remarkable performance on the 200k ``Needle-in-a-Haystack"" test. InternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel Conditional Online Reinforcement Learning from Human Feedback (COOL RLHF) strategy that addresses conflicting human preferences and reward hacking. By releasing InternLM2 models in different training stages and model sizes, we provide the community with insights into the model's evolution.","China,Hong Kong,China,Hong Kong,China,China",,,,,Unreleased,need to apply for commercial license. there's a repo but doesn't look like there's pretraining code.  https://github.com/InternLM/InternLM ,"Academia,Industry,Academia,Academia"
DeepSeek LLM 67B,Language,"Chat,Language modeling/generation,Question answering","Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y.K. Li, Wenfeng Liang, Fangyun Lin, A.X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R.X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, Yuheng Zou",Open weights (restricted use),"https://arxiv.org/abs/2401.02954, https://github.com/deepseek-ai/DeepSeek-LLM",,DeepSeek LLM: Scaling Open-Source Language Models with Longtermism,2024-01-05,DeepSeek,67000000000.0,"67B
",8.04e+23,67B * 2T * 6 = 8.04e23,Unspecified unreleased,"""We collect 2 trillion tokens for pre-training, primarily in Chinese and English.""

""We have gained valuable insights from reputable sources such as (Computer, 2023; Gao et al., 2020; Penedo et al., 2023; Touvron et al., 2023a)... We adopted an aggressive deduplication strategy, expanding the deduplication scope. Our analysis revealed that deduplicating the entire Common Crawl corpus results in higher removal of duplicate instances compared to deduplicating within a single dump""",2000000000000.0,"""We collect 2 trillion tokens for pre-training, primarily in Chinese and English""",,,,Confident,"The rapid development of open-source large language models (LLMs) has been truly remarkable. However, the scaling law described in previous literature presents varying conclusions, which casts a dark cloud over scaling LLMs. We delve into the study of scaling laws and present our distinctive findings that facilitate scaling of large scale models in two commonly used open-source configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek LLM, a project dedicated to advancing open-source language models with a long-term perspective. To support the pre-training phase, we have developed a dataset that currently consists of 2 trillion tokens and is continuously expanding. We further conduct supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the creation of DeepSeek Chat models. Our evaluation results demonstrate that DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in the domains of code, mathematics, and reasoning. Furthermore, open-ended evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance compared to GPT-3.5.",China,,,,,Unreleased,"repo with inference code and details, but no training code:

https://github.com/deepseek-ai/deepseek-LLM/blob/main/LICENSE-MODEL",Industry
YaYi 2.0,,,"Yin Luo, Qingchao Kong, Nan Xu, Jia Cao, Bao Hao, Baoyu Qu, Bo Chen, Chao Zhu Chenyang Zhao, Donglei Zhang, Fan Feng, Feifei Zhao, Hailong Sun, Hanxuan Yang, Haojun Pan, Hongyu Liu, Jianbin Guo, Jiangtao Du, Jingyi Wang, Junfeng Li, Lei Sun, Liduo Liu, Lifeng Dong, Lili Liu, Lin Wang, Liwen Zhang, Minzheng Wang, Pin Wang, Ping Yu, Qingxiao Li, Rui Yan, Rui Zou, Ruiqun Li, Taiwen Huang, Xiaodong Wang, Xiaofei Wu, Xin Peng, Xina Zhang, Xing Fang, Xinglin Xiao, Yanni Hao, Yao Dong, Yigang Wang, Ying Liu, Yongyu Jiang, Yungan Wang, Yuqi Wang, Zhangsheng Wang, Zhaoxin Yu, Zhen Luo, Wenji Mao, Lei Wang, Dajun Zeng",,https://arxiv.org/abs/2312.14862v1,3.0,"YAYI 2: Multilingual Open-Source Large Language Models
",2023-12-22,Yayi (Wenge),30000000000.0,,4.77e+23,1000 A800 GPUs,,2650000000000,2650000000000.0,,,6*30000000000*2650000000000=4.77e+23,NVIDIA A800,Likely,"In this technical report, we propose YAYI 2, including both base and chat models, with 30 billion parameters. YAYI 2 is pre-trained from scratch on a multilingual corpus which contains 2.65 trillion tokens filtered by our pre-training data processing pipeline.",China,,,,1000.0,,,Industry
Gemini Nano-2,"Multimodal,Language,Vision,Audio","Chat,Image captioning,Speech recognition",Gemini Team,Unreleased,https://arxiv.org/abs/2312.11805,633.0,Gemini: A Family of Highly Capable Multimodal Models,2023-12-19,Google DeepMind,3250000000.0,3.25B,,"More tokens than Chinchilla-optimal:

""The number of tokens used to train the largest models were determined following the approach in Hoffmann et al. (2022). The smaller models are trained for significantly more tokens to improve performance for a given inference budget, similar to the approach advocated in Touvron et al. (2023a)""

Chinchilla was 1.4T tokens for 70B params, so Chinchilla-optimal for 3.25B params would be ~1.4T/20 = 70B tokens.

So compute was significantly greater than 3.25B * 70B * 6, which is 1.4e21. 

Touvron et al. is the Llama 1 paper, in which a 6.7B model is trained for 1T tokens. Using the same ratio, a 3.25B model would be trained on ~500B tokens. 3.25 * 500B * 6 = 9.75e21. No guarantee that the exact ratio for Nano is close to Llama's, of course.",Unspecified unreleased,"""Gemini models are trained on a dataset that is both multimodal and multilingual. Our pre-training dataset uses data from web documents, books, and code, and includes image, audio, and video data.""",,,,,Google TPU v5e,Confident,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.","United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,"May be API access in the future. There is an Android API but it ""is under a closed early access preview program at this time"": https://ai.google.dev/gemini-api/docs/get-started/android_aicore",Industry
Lyra-Fr 10B,Language,"Language modeling/generation,Semantic search,Text classification,Question answering,Text summarization,Sentiment classification",,API access,https://aws.amazon.com/fr/blogs/machine-learning/lighton-lyra-fr-model-is-now-available-on-amazon-sagemaker/,,LightOn Lyra-fr model is now available on Amazon SageMaker,2023-12-15,LightOn,10000000000.0,10B,,,,"""Lyra-fr was trained on a large corpus of French curated data""",,,,,,Likely,"We are thrilled to announce the availability of the LightOn Lyra-fr foundation model for customers using Amazon SageMaker. LightOn is a leader in building foundation models specializing in European languages. Lyra-fr is a state-of-the-art French language model that can be used to build conversational AI, copywriting tools, text classifiers, semantic search, and more. You can easily try out this model and use it with Amazon SageMaker JumpStart. JumpStart is the machine learning (ML) hub of SageMaker that provides access to foundation models in addition to built-in algorithms and end-to-end solution templates to help you quickly get started with ML.",France,,,,,,,Industry
Konan LLM 41B,"Language,Vision",Language modeling/generation,"Yang Seung-hyun, Wiretin, Changmin, Kim Jong-tae",Hosted access (no API),"https://en.konantech.com/en/llm/konanllm
https://techfinch.kr/ai/konan-technology-unveils-konan-llm--its-own-ai-language-model
https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11610127 ",,Konan LLM: A Korean Large Language Model,2023-12-15,Konan Technology,41000000000.0,,1.722e+23,=41000000000*700000000000*6=1.722 × 10^23,Unspecified unreleased,,700000000000.0,"https://www.konantech.com/pr/press?number=2628&pn=1&stype2=&sfi=subj&sword=

Since 2007, via the real-time AI analysis service pulseK, over 20.5 billion pieces of data have been independently secured.
Among them, only 2 billion high-quality, large-scale data pieces have been used for training.",,,,Likely,"Konan LLM is a Large Language Model developed in-house by Konan Technology. Konan Technology optimized for super-large AI training, it leverages high-quality, large-scale data and over 20 years of expertise in natural language processing.
Konan LLM supports all corporate documentation and creative tasks,
leading the way in workplace innovation.",Korea (Republic of),,,,,Unreleased,,Industry
Poro 34B,Language,"Code generation,Language modeling/generation","Risto Luukkonen, Jonathan Burdge, Elaine Zosa, Aarne Talman, Ville Komulainen, Väinö Hatanpää, Peter Sarlin, Sampo Pyysalo",Open weights (unrestricted),https://arxiv.org/abs/2404.01856,,Poro 34B and the Blessing of Multilinguality,2023-12-14,"High-Performance Language Technologies (HPLT),University of Turku",34200000000.0,https://huggingface.co/LumiOpen/Poro-34B,2.052e+23,"6ND = 6*1T*34.2B= 2.04e+23

""This allowed total training cycle throughput of 49618 TFLOPs and 174378 tokens/second.""

the training took around 18 months (https://hplt-project.org/deliverables)
49618*18*30*24*3600*10^12=2.3149774e+24

","mC4,SlimPajama,StarCoder,Dolma","https://huggingface.co/LumiOpen/Poro-34B

""The Finnish dataset is a combination of many Finnish resources:

    Finnish Internet Parsebank
    mC4 multilingual colossal, cleaned Common Crawl
    Common Crawl Finnish
    Finnish Wikipedia
    Lönnrot Projekti Lönnrot
    Suomi24 The Suomi 24 Corpus 2001-2020
    Reddit r/Suomi submissions and comments
    STT Finnish News Agency Archive 1992-2018
    Yle Finnish News Archive 2011-2018
    Yle Finnish News Archive 2019-2020
    Yle News Archive Easy-to-read Finnish 2011-2018
    Yle News Archive Easy-to-read Finnish 2019-2020""
""

""Poro is a 34B parameter decoder-only transformer pretrained on Finnish, English and code. It is being trained on 1 trillion tokens (700 billion as of this release). Poro is a fully open source model and is made available under the Apache 2.0 License.""",1000000000000.0,"1T tokens, assuming 0.75 word per token
""Poro is a 34B parameter decoder-only transformer pretrained on Finnish, English and code. It is being trained on 1 trillion tokens. Poro is a fully open source model and is made available under the Apache 2.0 License.""",,,AMD Radeon Instinct MI250X,Confident,"The pretraining of state-of-the-art large language models now requires trillions of words of text, which is orders of magnitude more than available for the vast majority of languages. While including text in more than one language is an obvious way to acquire more pretraining data, multilinguality is often seen as a curse, and most model training efforts continue to focus near-exclusively on individual large languages. We believe that multilinguality can be a blessing and that it should be possible to substantially improve over the capabilities of monolingual models for small languages through multilingual training. In this study, we introduce Poro 34B, a 34 billion parameter model trained for 1 trillion tokens of Finnish, English, and programming languages, and demonstrate that a multilingual training approach can produce a model that not only substantially advances over the capabilities of existing models for Finnish, but also excels in translation and is competitive in its class in generating English and programming languages. We release the model parameters, scripts, and data under open licenses at this https URL.","Multinational,Finland",,,,512.0,,"Apache 2.0
https://huggingface.co/LumiOpen/Poro-34B","Research collective,Academia"
Mixtral 8x7B,Language,"Language modeling/generation,Question answering,Quantitative reasoning,Translation","Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed.",Open weights (unrestricted),"https://mistral.ai/news/mixtral-of-experts/, https://arxiv.org/abs/2401.04088",,Mixtral of experts: A high quality Sparse Mixture-of-Experts.,2023-12-11,Mistral AI,46700000000.0,"46.7B *sparse* params. 12.9B params used on average:

""Concretely, Mixtral has 46.7B total parameters but only uses 12.9B parameters per token. It, therefore, processes input and generates output at the same speed and for the same cost as a 12.9B model.""",,,,"""Mixtral is pretrained with multilingual data using a context size of 32k tokens""",,,,,,Confident,"Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks.",France,,,,,Unreleased,Apache 2.0,Industry
Mistral Medium,Language,Chat,,API access,https://mistral.ai/news/la-plateforme/,,La Plateforme,2023-12-11,Mistral AI,,"May be 70B, based on this weird leak episode. 

https://venturebeat.com/ai/mistral-ceo-confirms-leak-of-new-open-source-ai-model-nearing-gpt-4-performance/",,,,,,,,,,Unknown,"Mistral-medium. Our highest-quality endpoint currently serves a prototype model, that is currently among the top serviced models available based on standard benchmarks. It masters English/French/Italian/German/Spanish and code and obtains a score of 8.6 on MT-Bench. The following table compare the performance of the base models of Mistral-medium, Mistral-small and the endpoint of a competitor.",France,,,,,Unreleased,,Industry
XVERSE-65B-2,Language,"Chat,Language modeling/generation",,Open weights (restricted use),https://github.com/xverse-ai/XVERSE-65B/blob/main/README_EN.md,,,2023-12-08,"XVERSE Technology,Shenzhen Yuanxiang Technology",65000000000.0,Based on the name. Exact count unknown but may be listed on Hugging Face.,1.24800000000001e+24,C = 6ND = 6 * 3.2T tokens * 65B params = 1.248e24 FLOP,,"[2023/12/08] Released the XVERSE-65B-2 base model. This model builds upon its predecessor through Continual Pre-Training, reaching a total training volume of 3.2 trillion tokens.",3200000000000.0,"Training Data: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 2.6 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.

Assume 0.85 words per token on average for the mix of languages.",4096.0,November 6 to December 8 is 32 days. They did 600B tokens of continual pretraining during this period. The model's total tokens are 3200B. Therefore the total pretraining time was around (32 days * 24 hours/day)*(3200/600) = 4096 hours.,,Confident,,"China,China",,,,,Open source,"https://github.com/xverse-ai/XVERSE-65B/blob/main/README_EN.md 

license info:
""The use of the source code in this repository must follow the Apache-2.0 open-source license, while the use of the model weights of XVERSE-65B needs to adhere to the Model License Agreement.
The XVERSE-65B model weights are fully open to academic research and support free commercial use. To apply for a commercial license, please fill in the application form. For other questions or collaborations, please contact opensource@xverse.cn.""","Industry,Industry"
Gemini 1.0 Ultra,"Multimodal,Language,Vision","Language modeling,Visual question answering,Chat,Translation",Gemini Team,API access,https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,633.0,Gemini: A Family of Highly Capable Multimodal Models,2023-12-06,Google DeepMind,,,5.0000000001e+25,"This number is an estimate based on limited evidence. In particular, we combine information about the performance of Gemini Ultra on various benchmarks compared to other models, and guesstimates about the hardware setup used for training to arrive at our estimate. Our reasoning and calculations are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",Unspecified unreleased,"""Gemini models are trained on a dataset that is both multimodal and multilingual. Our pretraining dataset uses data from web documents, books, and code, and includes image, audio, and video data... We find that data quality is critical to a highlyperforming model, and believe that many interesting questions remain around finding the optimal dataset distribution for pretraining.""",,,2400.0,"Dylan Patel, author of SemiAnalysis, speculates that the training duration of Gemini may have been 100 days.",Google TPU v4,Speculative,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
","United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,57000.0,Unreleased,API access: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models,Industry
Gemini 1.0 Pro,"Multimodal,Language,Vision","Language modeling,Visual question answering,Chat,Translation",Gemini Team,API access,https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,633.0,Gemini: A Family of Highly Capable Multimodal Models,2023-12-06,Google DeepMind,,,1.830001e+24,"Training compute estimated from benchmark scores.

Our reasoning and calculations for Gemini 1 Ultra are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c

",Unspecified unreleased,"""Gemini models are trained on a dataset that is both multimodal and multilingual. Our pretraining dataset uses data from web documents, books, and code, and includes image, audio, and video data... We find that data quality is critical to a highlyperforming model, and believe that many interesting questions remain around finding the optimal
dataset distribution for pretraining.""",,,,,Google TPU v4,Speculative,"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.
","United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,Unreleased,API access: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models,Industry
tsuzumi 7B,Language,"Chat,Language modeling/generation",,,https://group.ntt/en/magazine/blog/tsuzumi/,,"NTT's Large Language Model ""tsuzumi"" is Here!",2023-12-01,NTT Communication Science Laboratories,7000000000.0,7B,,,,,,,,,,Confident,,Japan,,,,,,,Industry
Qwen-72B,Language,"Chat,Code generation","Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu",Open weights (restricted use),https://huggingface.co/Qwen/Qwen-72B,,,2023-11-30,Alibaba,72000000000.0,72B,1.3e+24,"72 billion params, 3 trillion tokens
72b * 3T * 6 = 1.3e24",,"""It is pretrained on over 3 trillion tokens, including Chinese, English, multilingual texts, code, and mathematics, covering general and professional fields""",3000000000000.0,Assuming not trained for multiple epochs.,,,,Confident,"Qwen-72B is the 72B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-72B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. Additionally, based on the pretrained Qwen-72B, we release Qwen-72B-Chat, a large-model-based AI assistant, which is trained with alignment techniques.",China,,,,,Unreleased,"up to 100m active users:
https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT",Industry
Granite 13B,Language,"Chat,Language modeling/generation,Question answering,Text summarization",,API access,https://www.ibm.com/downloads/cas/X9W4O6BM,,Granite Foundation Models,2023-11-30,IBM,13000000000.0,13 billion,2.44e+23,"Estimate using hardware:

""Granite.13b.v1 used 256 A100 GPUs for 1056 hours and 120 TFLOPs.
Granite.13b.v2 was trained on the same infrastructure for an
additional 1152 hours with 120 TFLOPS, bringing the total to
2208 hours""

Seems like 120 TFLOPS is the output per GPU after utilization, though they don't explicitly explain that part. That's 38% utilization.

256 * 2208 * 3600 * 120 TFLOPS = 2.44e23

Using 6ND:

""The second version of the granite.13b models leverages an updated base model trained on 2.5T trillion tokens.""

""The granite.13b.v1 base model is trained for 300K iterations,
with a batch size of 4M tokens, for a total of 1.25 trillion
5 tokens. The granite.13b.v2 base model continued pre-training
on top of the granite.13b.v1 checkpoint for an additional 300K
iterations and a total of 2.5 trillion tokens.""

2.5T * 13B * 6 = 1.95e23","Unspecified unreleased,Common Crawl,arXiv,OPENWEBTEXT","""To support the training of large enterprise-grade foundation
models, including granite.13b, IBM curated a massive dataset
of relevant unstructured language data from sources across
academia, the internet, enterprise (e.g., financial, legal), and
code.""

More breakdowns in paper, 20 sources in total

https://www.ibm.com/docs/en/cloud-paks/cp-data/4.8.x?topic=models-granite-13b-v1-model-card ",2500000000000.0,"2.5T tokens, 1.875T words at 0.75 words/token

https://www.ibm.com/docs/en/cloud-paks/cp-data/5.0.x?topic=models-granite-13b-chat-v2-model-card",2208.0,"""Granite.13b.v1 used 256 A100 GPUs for 1056 hours and 120 TFLOPs. Granite.13b.v2 was trained on the same infrastructure for an
additional 1152 hours with 120 TFLOPS, bringing the total to
2208 hours""",NVIDIA A100,Likely,"We introduce the Granite series of decoder-only foundation models for generative artificial intelligence (AI) tasks that are ready for enterprise use. We report on the architecture, capabilities, underlying data and data governance, training algorithms, compute infrastructure, energy and carbon footprint,
testing and evaluation, socio-technical harms and mitigations, and usage policies.",United States of America,,,,,Unreleased,,Industry
Yuan 2.0,Language,"Language modeling/generation,Translation,Code generation","Shaohua Wu, Xudong Zhao, Shenling Wang, Jiangang Luo, Lingjun Li, Xi Chen, Bing Zhao, Wei Wang, Tong Yu, Rongguo Zhang, Jiahua Zhang, Chao Wang",Open weights (restricted use),https://arxiv.org/abs/2311.15786v1,,YUAN 2.0: A Large Language Model with Localized Filtering-based Attention,2023-11-27,Inspur,102600000000.0,102.6 billion,1.78e+23,"Trained on 288B tokens

6*102.6b*288b = 1.78e23",,"""The pretraining corpus includes a mix of books, codes, and encyclopedia in both Chinese and English (Table 2)""

with synthetic code data:
""Code (CN). Considering the diversity of programming tasks, we also build a synthesized instruction dataset
with 4 million code samples in Chinese. To cover the concepts involved in programming tasks as many as
possible, we collect 15,000 words of programming, computer science, mathematics, and other relevant
topics from the Sogou input dictionary. Two topic words are randomly selected as the seeds for a wellcrafted prompt in each time. Then the prompt will be fed to GPT-3.5 to generate a programming task and
corresponding Python solution. ""

and translated open-source fine-tuning instruction data:
""We construct a fine-tuning dataset focused on code, math and chat tasks.
Code Instruction dataset. We collect some open-source code instruction datasets, including CodeAlpaca20k [28], Evol-Instruct-Code-80k[38], CodeFuse-CodeExercise-Python-27k and CodeFuse-Evolinstruction-66k [39]. We translate the English code instruction into Chinese with GPT-3.5""",288000000000.0,"Most likely the 288B tokens do not represent multiple epochs. As a sense check, Table 2 appears to indicate that 5.73% of pre-training tokens come from synthetically generated text output by GPT-3.5. If the full training corpus is 288B tokens, this would imply ~$24k in API costs at $1.50/1M tokens to generate the data, which seems plausible.",,,,Confident,"In this work, the Localized Filtering-based Attention (LFA) is introduced to incorporate prior knowledge of local dependencies of natural language into Attention. Based on LFA, we develop and release Yuan 2.0, a large language model with parameters ranging from 2.1 billion to 102.6 billion. A data filtering and generation method is presented to build pretraining and fine-tuning dataset in high quality. A distributed training method with non-uniform pipeline parallel, data parallel, and optimizer parallel is proposed, which greatly reduces the bandwidth requirements of intra-node communication, and achieves good performance in large-scale distributed training. Yuan 2.0 models display impressive ability in code generation, math problem-solving, and chat compared with existing models. The latest version of YUAN 2.0, including model weights and source code, is accessible at Github.",China,,,,,,"commercial ok, but nothing that ""may cause harm to the country and society, or for any services that have not undergone security assessment and filing""
https://huggingface.co/IEITYuan/Yuan2-102B-hf",Industry
Inflection-2,Language,"Language modeling,Language modeling/generation,Chat,Question answering",,Hosted access (no API),https://inflection.ai/inflection-2,,Inflection-2: The Next Step Up,2023-11-22,Inflection AI,,,1.001e+25,"""Inflection-2 was trained on 5,000 NVIDIA H100 GPUs in fp8 mixed precision for ~10²⁵ FLOPs""

(the second 1 is there because of airtable being wonky, it's not a real sig fig)",Unspecified unreleased,,,,,,NVIDIA H100 SXM5 80GB,Confident,"Today we are proud to announce that we have completed training of Inflection-2, the best model in the world for its compute class and the second most capable LLM in the world today. Our mission at Inflection is to create a personal AI for everyone. Just a few months ago, we announced Inflection-1 — a best-in-class language model that currently powers Pi. Our new model, Inflection-2, is substantially more capable than Inflection-1, demonstrating much improved factual knowledge, better stylistic control, and dramatically improved reasoning.",United States of America,,,,5000.0,Unreleased,"via Pi, no API",Industry
Lyria,Audio,Audio generation,"Kazuya Kawakami, David Ding, Björn Winckler, Cătălina Cangea, Tobenna Peter Igwe, Will Grathwohl, Yan Wu, Yury Sulsky, Jacob Kelly, Charlie Nash, Conor Durkan, Yaroslav Ganin, Tom Eccles, Zach Eaton-Rosen, Jakob Bauer, Mikita Sazanovich, Morgane Rivière, Evgeny Gladchenko, Mikołaj Bińkowski, Ali Razavi, Jeff Donahue, Benigno Uria, Sander Dieleman, Sherjil Ozair, John Schultz, Ankush Gupta, Junlin Zhang, Drew Jaegle, Aäron van den Oord.",,https://deepmind.google/discover/blog/transforming-the-future-of-music-creation/,,Transforming the future of music creation,2023-11-16,Google DeepMind,,,,,Unspecified unreleased,,,,,,,Unknown,"Music contains huge amounts of information — consider every beat, note, and vocal harmony in every second. When generating long sequences of sound, it’s difficult for AI models to maintain musical continuity across phrases, verses, or extended passages. Since music often includes multiple voices and instruments at the same time, it's much harder to create than speech.

Built by Google DeepMind, the Lyria model excels at generating high-quality music with instrumentals and vocals, performing transformation and continuation tasks, and giving users more nuanced control of the output’s style and performance.","United States of America,United Kingdom of Great Britain and Northern Ireland,Multinational,Canada,France",,,,,,,Industry
Nemotron-3-8B,Language,"Chat,Language generation,Language modeling/generation,Translation,Code generation,Question answering",,Open weights (restricted use),"https://developer.nvidia.com/blog/nvidia-ai-foundation-models-build-custom-enterprise-chatbots-and-co-pilots-with-production-ready-llms/

https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/nemotron-3-8b-base-4k",,NVIDIA AI Foundation Models: Build Custom Enterprise Chatbots and Co-Pilots with Production-Ready LLMs,2023-11-15,NVIDIA,8000000000.0,,1.8e+23,"https://huggingface.co/nvidia/nemotron-3-8b-base-4k

""This model was trained on a dataset containing 3.8 Trillion tokens of text""

8 billion * 3.8 trillion * 6 = 1.8e23

Also, using the hardware method: ""1,024 A100s were used for 19 days to train the model.""

19*1024 * 312 trillion * 24 * 3600 * 0.3 = 1.57e23","Unspecified unreleased,Flan,P3 (Public Pool of Prompts)","""NVIDIA models are trained on a diverse set of public and proprietary datasets. This model was trained on a dataset containing 3.8 Trillion tokens of text. The dataset contains 53 different human languages (including English, German, Russian, Spanish, French, Japanese, Chinese, Italian, and Dutch) and 37 programming languages. The model also uses the training subsets of downstream academic benchmarks from sources like FLANv2, P3, and NaturalInstructions v2""",3800000000000.0,,456.0,19 days,NVIDIA A100,Confident,"Large language models (LLMs) are revolutionizing data science, enabling advanced capabilities in natural language understanding, AI, and machine learning. Custom LLMs, tailored for domain-specific insights, are finding increased traction in enterprise applications.

The NVIDIA Nemotron-3 8B family of foundation models is a powerful new tool for building production-ready generative AI applications for the enterprise–fostering innovations ranging from customer service AI chatbots to cutting-edge AI products.",United States of America,,,,1024.0,,"can't use to train other models:

https://developer.download.nvidia.com/ai-foundation-models/nvidia-ai-foundation-models-license-10Nov2023.pdf",Industry
Samsung Gauss Language,Language,"Language modeling/generation,Question answering,Text summarization,Translation",,,https://techcrunch.com/2023/11/08/samsung-unveils-chatgpt-alternative-samsung-gauss-that-can-generate-text-code-and-images/,,,2023-11-08,Samsung,,,,,,,,,,,,Unknown,"Just a few days after OpenAI’s developer event, Samsung unveiled its own generative AI model, Samsung Gauss, at the Samsung AI Forum 2023.

Samsung Gauss, developed by the tech giant’s research unit Samsung Research, consists of three tools: Samsung Gauss Language, Samsung Gauss Code and Samsung Gauss Image.

Samsung Gauss Language is a large language model that can understand human language and answer questions like ChatGPT. It can be used to increase productivity in several ways. For instance, it can help you write and edit emails, summarize documents and translate languages. Samsung plans to incorporate the large language model into its devices like phones, laptops and tablets to make the company’s smart devices a bit smarter. When asked if it supports both English and Korean as interaction languages, a spokesperson of Samsung declined to comment on it.",Korea (Republic of),,,,,,,Industry
Samsung Gauss Code,Language,Code generation,,,https://techcrunch.com/2023/11/08/samsung-unveils-chatgpt-alternative-samsung-gauss-that-can-generate-text-code-and-images/,,,2023-11-08,Samsung,,,,,,,,,,,,Unknown,"Just a few days after OpenAI’s developer event, Samsung unveiled its own generative AI model, Samsung Gauss, at the Samsung AI Forum 2023.

Samsung Gauss, developed by the tech giant’s research unit Samsung Research, consists of three tools: Samsung Gauss Language, Samsung Gauss Code and Samsung Gauss Image.

Samsung Gauss Language is a large language model that can understand human language and answer questions like ChatGPT. It can be used to increase productivity in several ways. For instance, it can help you write and edit emails, summarize documents and translate languages. Samsung plans to incorporate the large language model into its devices like phones, laptops and tablets to make the company’s smart devices a bit smarter. When asked if it supports both English and Korean as interaction languages, a spokesperson of Samsung declined to comment on it.",Korea (Republic of),,,,,,,Industry
Samsung Gauss Image,Image generation,Image generation,,,https://techcrunch.com/2023/11/08/samsung-unveils-chatgpt-alternative-samsung-gauss-that-can-generate-text-code-and-images/,,,2023-11-08,Samsung,,,,,Unspecified unreleased,,,,,,,Unknown,"Just a few days after OpenAI’s developer event, Samsung unveiled its own generative AI model, Samsung Gauss, at the Samsung AI Forum 2023.

Samsung Gauss, developed by the tech giant’s research unit Samsung Research, consists of three tools: Samsung Gauss Language, Samsung Gauss Code and Samsung Gauss Image.

Samsung Gauss Language is a large language model that can understand human language and answer questions like ChatGPT. It can be used to increase productivity in several ways. For instance, it can help you write and edit emails, summarize documents and translate languages. Samsung plans to incorporate the large language model into its devices like phones, laptops and tablets to make the company’s smart devices a bit smarter. When asked if it supports both English and Korean as interaction languages, a spokesperson of Samsung declined to comment on it.",Korea (Republic of),,,,,,,Industry
XVERSE-13B-2,Language,"Language generation,Language modeling/generation,Question answering,Text summarization,Translation",,Open weights (restricted use),https://huggingface.co/xverse/XVERSE-13B,,,2023-11-06,"XVERSE Technology,Shenzhen Yuanxiang Technology",13000000000.0,13B,,"Not enough info, eg number of epochs",,"""The model has been thoroughly trained on a diversified and high-quality dataset consisting of 3.2 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.""",3200000000000.0,"Multilingual, 3.2 trillion tokens. Note that model was originally stated to have been trained on 1.4T tokens, so while the wording suggests a dataset of 3.2T unique tokens, it may actually be referencing the number of tokens seen by the model (i.e. possibly over multiple epochs).",,,,Likely,"XVERSE-13B is a multilingual large language model, independently developed by Shenzhen Yuanxiang Technology. Its key features are as follows:

Model Structure: XVERSE-13B uses the mainstream Decoder-only Transformer network structure, supports 8k context length, the longest one among models of the same size, which can meet the need of longer multi-round dialogues, knowledge question-answering, and summarization. This makes the model more versatile in application scenarios.
Training Data: The model has been thoroughly trained on a diversified and high-quality dataset consisting of 3.2 trillion of tokens, including more than 40 languages such as Chinese, English, Russian, and Spanish. The sampling ratio of different types of data is finely set, which makes the performance of Chinese and English excellent, and also takes into account the effect of other languages.
Tokenization: Based on the BPE (Byte-Pair Encoding) algorithm, a tokenizer with a vocabulary size of 100,534 has been trained using hundreds of gigabytes of language data. This tokenizer is capable of supporting multilingual without the need for additional vocabulary expansion.
Training Framework: Several key technologies have also been independently developed, including efficient operators, memory optimization, parallel scheduling strategies, overlap of data-computation-communication, and synergy between platforms and frameworks. These advancements enhance training efficiency and model stability. With these technologies, the peak computational power utilization rate on a thousand-card cluster can reach 58.5%, ranking at the forefront of the industry.","China,China",,,,,Open source,must apply for commercial license,"Industry,Industry"
Whisper v3,Speech,Speech recognition,,Open weights (unrestricted),https://huggingface.co/openai/whisper-large-v3,,,2023-11-06,OpenAI,1550000000.0,,2.7e+23,"Could derive this in terms of Whisper v1, which according to the paper was trained for 680k hours for between 2-3 epochs. Whisper v3 was trained on 5 million hours for 2 epochs, or ~5-7x as much data, and has the same architecture. We have an estimate of 4.65e22 for Whisper 1.

Assume Whisper v1 was trained on 2.5 epochs, or 2.5*680k = 1.7M hours. Whisper v3 was trained on 10M hours. 10/1.7 * 4.65e22 ~= 2.7e23",Unspecified unreleased,"""The Whisper large-v3 model is trained on 1 million hours of weakly labeled audio and 4 million hours of pseudolabeled audio collected using Whisper large-v2""",60000000000.0,"English audio is roughly 228 wpm: https://docs.google.com/document/d/1G3vvQkn4x_W71MKg0GmHVtzfd9m0y3_Ofcoew0v902Q/edit#heading=h.sxcem9l5k3ce

The dataset is multilingual and other languages seem to have lower wpms. So using 200 wpm, we have

200*60*5 million hours = 60,000,000,000 (60B) words",,,,Likely,"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.

Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford et al. from OpenAI. The original code repository can be found here.",United States of America,,,,,Unreleased,"Apache 2.0: https://huggingface.co/openai/whisper-large-v3 

this seems to be inference code not training: https://github.com/openai/whisper ",Industry
GPT-4 Turbo,"Multimodal,Vision,Language,Image generation","Chat,Language modeling/generation,Image generation,Speech synthesis,Table tasks,Visual question answering,Image captioning",,API access,https://openai.com/blog/new-models-and-developer-products-announced-at-devday,,New models and developer products announced at DevDay,2023-11-06,OpenAI,,Not known. Maybe smaller/sparser than GPT-4.,2.2e+25,Estimated using benchmark imputation,Unspecified unreleased,,,,,,,Unknown,"Today, we shared dozens of new additions and improvements, and reduced pricing across many parts of our platform. These include:

New GPT-4 Turbo model that is more capable, cheaper and supports a 128K context window",United States of America,,,,,Unreleased,,Industry
Grok-1,Language,"Language modeling,Chat",,Open weights (unrestricted),"https://x.ai/model-card/, https://x.ai/blog/grok-os",,Announcing Grok,2023-11-04,xAI,314000000000.0,"""314B parameter Mixture-of-Experts model with 25% of the weights active on a given token"". So effectively 78B parameters

Mixture of 8 experts: https://github.com/xai-org/grok-1",2.90000000001e+24,"""On these benchmarks, Grok-1 displayed strong results, surpassing all other models in its compute class, including ChatGPT-3.5 and Inflection-1. It is only surpassed by models that were trained with a significantly larger amount of training data and compute resources like GPT-4""

Per table, Grok-1 is surpassed by Palm 2, Claude 2, GPT-4, so it required less compute than these three models. Palm 2 was trained on 7e24 FLOP.

GPT-3.5 is ~2.6e24. Inflection-1's compute is not public/known by us but Inflection says Inflection-1 compute was <= Palm-540B's (which was ~2.5e24). 

For optimal training, our current working hypothesis is that you still need something like Chinchilla scaling on the total number of parameters in the model, even for MoE models, so optimal dataset size would be 20*310B tokens. With 25%*314B params active per forward pass, this would be around 3e24 FLOP.
https://www.wolframalpha.com/input?i=20*310+billion+*+6+*+25%25+*+314+billion",Unspecified unreleased,"""Base model trained on a large amount of text data, not fine-tuned for any particular task.""

""The training data used for the release version of Grok-1 comes from both the Internet up to Q3 2023 and the data provided by our AI Tutors.""",6200000000000.0,"(Speculative confidence, see compute notes)",,,,Likely,"Grok is an AI modeled after the Hitchhiker’s Guide to the Galaxy, so intended to answer almost anything and, far harder, even suggest what questions to ask!

Grok is designed to answer questions with a bit of wit and has a rebellious streak, so please don’t use it if you hate humor!

A unique and fundamental advantage of Grok is that it has real-time knowledge of the world via the 𝕏 platform. It will also answer spicy questions that are rejected by most other AI systems.

Grok is still a very early beta product – the best we could do with 2 months of training – so expect it to improve rapidly with each passing week with your help.",United States of America,,,,,Unreleased,apache 2.0,Industry
Grok-0,Language,"Chat,Language modeling/generation",,Hosted access (no API),https://x.ai/,,Announcing Grok,2023-11-04,xAI,33000000000.0,33 billion,,"Half of Llama 2-70B? (which we estimated at 8e23) ""This early model approaches LLaMA 2 (70B) capabilities on standard LM benchmarks but uses only half of its training resources""",Unspecified unreleased,,,no information about the dataset found except for this speculation https://docs.google.com/spreadsheets/d/1O5KVQW1Hx5ZAkcg8AIRjbQLQzx2wVaLl0SqUu-ir9Fs/edit?gid=1158069878#gid=1158069878,,,,Likely,"""The engine powering Grok is Grok-1, our frontier LLM, which we developed over the last four months. Grok-1 has gone through many iterations over this span of time.

After announcing xAI, we trained a prototype LLM (Grok-0) with 33 billion parameters. This early model approaches LLaMA 2 (70B) capabilities on standard LM benchmarks but uses only half of its training resources.""",United States of America,,,,,Unreleased,,Industry
Yi-34B,Language,"Chat,Language modeling/generation,Translation,Code generation","Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai",Open weights (restricted use),https://arxiv.org/abs/2403.04652,,Yi: Open Foundation Models by 01.AI,2023-11-02,01.AI,34000000000.0,34b,6.1e+23,"""The dataset we use contains Chinese & English only. We used approximately 3T tokens"" sounds like this means it was trained on 3T tokens, not necessarily that the dataset contains 3T tokens?

If so, 34b * 3T * 6 = 6.1e23",Unspecified unreleased,"Chinese and English dataset

""For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less than 10K) instruction dataset over multiple iterations such that every single instance has been verified directly by our machine learning engineers. For vision-language, we combine the chat language model with a vision transformer encoder and train the model to align visual representations to the semantic space of the language model.""",3100000000000.0,"""language models pretrained from scratch on 3.1T highly-engineered large amount of data, and finetuned on a small but meticulously polished alignment data.""",,,NVIDIA A100,Confident,The Yi series models are large language models trained from scratch by developers at 01.AI.,China,,,,128.0,Unreleased,"apply for commercial license:
no training code
https://github.com/01-ai/Yi/blob/main/MODEL_LICENSE_AGREEMENT.txt

the model https://huggingface.co/01-ai/Yi-34B-Chat Apache 2.0
""If you create derivative works based on this model, please include the following attribution in your derivative works: ....""",Industry
Cohere Embed,Language,Semantic embedding,"Nils Reimers, Elliott Choi, Amr Kayid, Alekhya Nandula, Manoj Govindassamy, Abdullah Elkady",API access,https://txt.cohere.com/introducing-embed-v3/,,Cohere Command & Embed on Amazon Bedrock,2023-11-02,Cohere,,,,"https://docs.cohere.com/docs/environmental-impact

Embed v2 (older version) produced 6689.76 kg CO2 to train. Using the calculator Cohere links (https://mlco2.github.io/impact/) that's the equivalent of 80,000 TPUv3-hours in the ""us-west1"" region. That's 3.5e22 FLOP without considering utilization. However, I have no idea which region Cohere's GPUs are in (looks like CO2/energy can vary a lot by region), and they probably used a more recent GPU.",Unspecified unreleased,,,"""First, they have been trained on questions and answers from a large web crawl. When we presented our multilingual-v2.0 model last year, we had a collection of over 1.4 billion question-and-answer pairs from 100+ languages on basically every topic on the internet.""

""Hence, the second stage involved measuring content quality. We used over 3 million search queries from search engines and retrieved the top-10 most similar documents for each query.""",,,,Unknown,"We're excited to introduce Embed v3, our latest and most advanced embeddings model. Embed v3 offers state-of-the-art performance per trusted MTEB and BEIR benchmarks.",Canada,,,,,Unreleased,,Industry
BlueLM 70B,Language,"Chat,Language modeling/generation,Question answering",,Unreleased,https://baijiahao.baidu.com/s?id=1781445143383237948&wfr=spider&for=pc,,,2023-11-02,vivo AI lab,70000000000.0,,4.2e+23,6ND = 6*70B*1000B=4.2e+23,Unspecified unreleased,,,"1000B Text data
10B Image data
100M video data
100M Knowledge graph

(from the conference handout)",,,,Confident,,China,,,,,Unreleased,information about the model is from their paper catalogue and not found on the internet,Industry
BlueLM 130B,Language,"Chat,Language modeling/generation,Question answering",,Unreleased,https://baijiahao.baidu.com/s?id=1781445143383237948&wfr=spider&for=pc,,,2023-11-02,vivo AI lab,130000000000.0,,7.8e+23,6ND = 6*130B*1000B=7.8e+23,Unspecified unreleased,,,"1000B Text data
10B Image data
100M video data
100M Knowledge graph

(from the conference handout)",,,,Confident,,China,,,,,Unreleased,information about the model is from their paper catalogue and not found on the internet,Industry
BlueLM 175B,Language,"Chat,Language modeling/generation,Question answering",,Unreleased,https://baijiahao.baidu.com/s?id=1781445143383237948&wfr=spider&for=pc,,,2023-11-02,vivo AI lab,175000000000.0,,1.05e+24,6ND = 6*175B*1000B=1.05e+24,Unspecified unreleased,,,"1000B Text data
10B Image data
100M video data
100M Knowledge graph

(from the conference handout)",,,,Confident,,China,,,,,Unreleased,information about the model is from their paper catalogue and not found on the internet,Industry
Yi 6B,Language,"Chat,Language modeling/generation,Translation,Code generation","Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai",Open weights (restricted use),https://arxiv.org/abs/2403.04652,,Yi: Open Foundation Models by 01.AI,2023-11-02,01.AI,6000000000.0,6B,1.26e+23,6*7*10^9*3*10^12 = 1.26e+23,Unspecified unreleased,,3100000000000.0,"""language models pretrained from scratch on 3.1T highly-engineered large amount of
data, and finetuned on a small but meticulously polished alignment data.""",,,,Confident,The Yi series models are large language models trained from scratch by developers at 01.AI.,China,,,,,Unreleased,"llama license
https://huggingface.co/01-ai/Yi-6B

no training code",Industry
Nanbeige-16B,Language,"Chat,Language modeling/generation,Code generation,Question answering",,Open weights (unrestricted),https://github.com/Nanbeige/Nanbeige/blob/main/README_EN.md,,,2023-11-01,Nanbeige LLM Lab,16000000000.0,16 billion,2.4e+23,"""It uses 2.5T Tokens for pre-training"". I think that's the number of tokens the model was trained on, not the dataset size, but I'm not sure.

16 billion * 2.5 trillion * 6 = 2.4e23",Unspecified unreleased,"""The training data includes a large amount of high-quality internet corpus, various books, code, etc""",2500000000000.0,"""It uses 2.5T Tokens for pre-training""",,,,Likely,"Nanbeige-16B is a 16 billion parameter language model developed by Nanbeige LLM Lab. It uses 2.5T Tokens for pre-training. The training data includes a large amount of high-quality internet corpus, various books, code, etc. It has achieved good results on various authoritative evaluation data sets. This release includes the Base, Chat, Base-32k and Chat-32k.",China,,,,,Open source,"Apache 2.0

training code: https://github.com/Nanbeige/Nanbeige/blob/main/scripts/train.sh ",Industry
LingoWhale-8B,Language,"Language modeling/generation,Code generation,Translation",,Open weights (non-commercial),https://github.com/DeepLangAI/LingoWhale-8B/blob/main/README_EN.md,,,2023-11-01,DeepLang AI,8000000000.0,,,,,"""pre-trained on a large volume of high-quality bilingual data"" Chinese + English",,,,,,Confident,"LingoWhale-8B is the first open-source model in the LingoWhale series introduced by DeepLangAI. It's a bilingual (Chinese-English) large language model.

LingoWhale-8B has been pre-trained on a large volume of high-quality bilingual data and exhibits powerful capabilities as a foundation model. It has achieved leading results on multiple public benchmarks. During its pre-training phase, the model was trained with a context window of 8K, allowing it to comprehend and generate longer sequences.

LingoWhale-8B is fully open for academic research. Users can apply for commercial use by email, and once granted official commercial permission, they can use it for free.",China,,,,,Open (non-commercial),"requires form for commercial:
https://github.com/DeepLangAI/LingoWhale-8B/blob/main/MODEL_LICENSE.md",Industry
BlueLM 7B,Language,"Chat,Translation,Language modeling/generation,Question answering,Code generation",,Open weights (restricted use),https://github.com/vivo-ai-lab/BlueLM/blob/main/BlueLM_technical_report.pdf,,BlueLM: An Open Multilingual 7B Language Model,2023-10-31,vivo AI lab,7000000000.0,"""BlueLM is a large-scale pre-trained language model independently developed by vivo AI Global Research Institute. This release includes 7B base (base) model and 7B conversation (chat) model. At the same time, we have open sourced the long text base (base) model that supports 32K and conversation (chat) model."" from GitHub https://github.com/vivo-ai-lab/BlueLM

",1.0920000000001e+23,"C = 6DN = 6 * 2.6T * 7B = 1.092*10^23 FLOP
https://www.wolframalpha.com/input?i=6*7+billion+*+2.6+trillion
(assuming 1 epoch)

Figure 1 gives compute of 10^12 FLOPs  but this seems improbable

Training over 2.59T tokens took approximately 26 days using the vivolm system, with a throughput of 3150 tokens/sec/GPU.",Unspecified unreleased,,2592000000000.0,"""Larger amounts of high-quality data : high-quality corpus for training, reaching a scale of 2.6 trillion tokens. The corpus includes Chinese, English and a small amount of Japanese and Korean data"" from GitHub

see 2.1 https://github.com/vivo-ai-lab/BlueLM/blob/main/BlueLM_technical_report.pdf",,,,Confident,"BlueLM is a large-scale open-source language model independently developed by the vivo AI Lab. This release includes 2K and 32K context length versions for both Base and Chat models.

High-quality Data: BlueLM is trained on a high-quality data with 2.6 trillion tokens. Our train corpus mainly consists of Chinese and English data, with a small amount of Japanese and Korean data.
Stronger Performance: BlueLM-7B-Chat achieves a strong competitive performance in C-Eval and CMMLU benchmarks of the same size.
Longer Context: We have extended the context length of both BlueLM-7B-Base-32K and BlueLM-7B-Chat-32K models from 2K to 32K. The models can support longer context understanding while maintaining the same basic capabilities.
Model License: BlueLM weights are open for academic research and commercial use.",China,,,,,Unreleased,"https://github.com/vivo-ai-lab/BlueLM/blob/main/MODEL_LICENSE_EN.pdf

Our code is licensed under the Apache-2.0 and Community License for BlueLM Model. The BlueLM weights are completely open for academic research, and free commercial use is allowed after completing the questionnaire.

""BlueLM weights are open for academic research and commercial use.""",Industry
Tongyi Qianwen 2.0,Language,"Chat,Language modeling/generation",,API access,https://www.alibabacloud.com/blog/alibaba-cloud-launches-tongyi-qianwen-2-0-and-industry-specific-models-to-support-customers-reap-benefits-of-generative-ai_600526,,Alibaba Cloud Launches Tongyi Qianwen 2.0 and Industry-specific Models to Support Customers Reap Benefits of Generative AI,2023-10-31,Alibaba,,"""Tongyi Qianwen 2.0, a generic LLM with a few hundreds of billions of parameters""",,,Unspecified unreleased,,,,,,,Unknown,"Alibaba Cloud, the digital technology and intelligence backbone of Alibaba Group, today announced the launch of Tongyi Qianwen 2.0, its latest large language model (LLM), along with new industry-specific models at its annual flagship tech event Apsara Conference. This release signifies another significant progress in Alibaba Cloud's pursuit of cutting-edge AI innovation and its ongoing commitment to fuel digital transformation in businesses.",China,,,,,,,Industry
Mi:dm 200B,Language,Language modeling/generation,,API access,https://genielabs.ai/midm/about,,,2023-10-31,KT,200000000000.0,200B,1.1999999999999999e+24,6ND=1000000000000*200000000000.00*6=1.2 × 10^24,,,1000000000000.0,Mi:dm is the first Korean LLM trained on over 1 trillion tokens.,,,,Confident,"TL;DR:
KT Corp introduces Mi:dm, a massive AI model aimed at diverse sectors.
Mi:dm is the first Korean LLM trained on over 1 trillion tokens.
It offers four models, from basic to large, with up to 200 billion parameters.
KT plans to share Mi:dm’s foundational model with other companies.
Three advanced technologies reduce AI hallucinations by up to 70%.
Collaborations with AI startups, including Upstage, aim to conquer the global generative AI market.",Korea (Republic of),,,,,Unreleased,"KT said it will open up the foundation model of Mi:dm to other companies, providing a full AI development package, including KT Cloud's hyperscale AI computing service and AI chip startup Rebellions Inc.'s neural processing unit infrastructure, fostering the development of various AI services.",Industry
Skywork-13B,Language,"Language modeling,Language modeling/generation,Translation","Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, Yahui Zhou",Open weights (restricted use),https://arxiv.org/abs/2310.19341,75.0,Skywork: A More Open Bilingual Foundation Model,2023-10-30,Kunlun Inc.,13000000000.0,13B,2.5e+23,"""Our Skywork-13B is trained on a cluster of 64 NVIDIA-HGX-A800 nodes, a total of 512 A800-80G SXM GPUs... The training process of Skywork-13B spanned a total of 39 days.""

They note that ""we achieved a token throughput of 1873 per GPU per second and a model flops utilization (MFU) of 56.5%... "". 

""MFU"" was coined in the Palm paper (https://arxiv.org/pdf/2204.02311.pdf) and only counts operations used to train the model, not all operations observed on the hardware. MFU is lower than traditionally measured utilization.

Using the 56.5% number, and a peak tensor performance of 623.8 TFLOPS for the A800, this suggests 512 * 623.8 TFLOPS * 39 days * 86400 seconds/day * 0.565 = 6.08e23 FLOP.

Based on C=6ND, with 13B parameters and 3.2T tokens, we have C=6*(13B)*(3.2T)=2.5e23 FLOP.

Since the reported MFU is quite high, and would imply a higher compute usage than 6ND, it seems they may have trained on mixed precision and with the GPUs not always operating in the 623.8 TFLOPS mode.",SkyPile,"""In order to train Skywork-13B, we build SkyPile, a vast, high quality corpus comprising more than 6 trillion tokens. A segment of the corpus, comprising over 150 billion tokens of web text, has been open sourced to facilitate research and training on Chinese LLMs""",3180000000000.0,"The full SkyPile dataset is 6 trillion tokens, roughly half English and half Chinese: (https://huggingface.co/Skywork/Skywork-13B-base).

The model is trained for the equivalent of 0.53 epochs on the full dataset, or 3.18 trillion unique tokens. This is around 2.78 trillion words, based on an average of 1 word/token for the Chinese portion and 0.75 word/token on the English portion.",940.0,39 days,NVIDIA A800,Confident,"In this technical report, we present Skywork-13B, a family of large language models (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both English and Chinese texts. This bilingual foundation model is the most extensively trained and openly published LLMs of comparable size to date. We introduce a two-stage training methodology using a segmented corpus, targeting general purpose training and then domain-specific enhancement training, respectively. We show that our model not only excels on popular benchmarks, but also achieves state of the art performance in Chinese language modeling on diverse domains. Furthermore, we propose a novel leakage detection method, demonstrating that test data contamination is a pressing issue warranting further investigation by the LLM community. To spur future research, we release Skywork-13B along with checkpoints obtained during intermediate stages of the training process. We are also releasing part of our SkyPile corpus, a collection of over 150 billion tokens of web text, which is the largest high quality open Chinese pre-training corpus to date. We hope Skywork-13B and our open corpus will serve as a valuable open-source resource to democratize access to high-quality LLMs.",China,,,,512.0,Open (restricted use),"commercial but restrictive license: https://github.com/SkyworkAI/Skywork/blob/main/LICENSE

part of the training data is open, but only 2.5%: ""In order to train Skywork-13B, we build SkyPile, a vast, high quality corpus comprising more than 6 trillion tokens. A segment of the corpus, comprising over 150 billion tokens of web text, has been open sourced to facilitate research and training on Chinese LLMs""

training code: https://github.com/SkyworkAI/Skywork/blob/main/train/train.py ",Industry
Xinghan Foundation Model,"Multimodal,Video,Language,Vision","Video description,Visual question answering,Language modeling/generation",,,https://www.prnewswire.com/news-releases/dahua-announces-think-2-0-strategy-to-accelerate-innovation-for-a-digital-intelligent-future-301967122.html,,Dahua Announces Think# 2.0 Strategy to Accelerate Innovation for a Digital Intelligent Future,2023-10-25,Dahua Technology,,,,,,,,,,,,Unknown,"At the summit, Dahua's ""Xinghan"" Foundation Model was launched. With video as the core, this multimodal fusion industry foundation model catapults the accuracy and generalization of AI algorithms, with breakthroughs in visual cognition capabilities, independent analysis of various scenarios, and efficient fulfillment of massive fragmented needs, especially urban governance and power industry applications.",China,,,,,,,Industry
Spark 3.0,Language,"Code generation,Language generation,Language modeling/generation,Question answering",,,https://www.gizmochina.com/2023/10/26/iflytek-spark-3-0-vs-openai-gpt/,,,2023-10-24,iFlytek,,,,"""The company said that Spark 3.0 has been trained on a dataset of 1.2 trillion words and code, and that it can perform a variety of tasks, including text generation, language understanding, knowledge answering, logical reasoning, mathematical computation, code generation, and multimodal interaction."" https://www.gizmochina.com/2023/10/26/iflytek-spark-3-0-vs-openai-gpt/",,,1200000000000.0,"""Spark 3.0 has been trained on a dataset of 1.2 trillion words and code""",,,,Likely,,China,,,,,,,Industry
DALL·E 3,Image generation,"Image generation,Text-to-image","James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, Aditya Ramesh",API access,https://cdn.openai.com/papers/dall-e-3.pdf,622.0,Improving Image Generation with Better Captions,2023-10-19,OpenAI,,,,,Unspecified unreleased,,,,,,,Unknown,"We show that prompt following abilities of text-to-image models can be substantially improved by training on highly descriptive generated image captions.
Existing text-to-image models struggle to follow detailed image descriptions and often ignore words or confuse the meaning of prompts. We hypothesize that this issue stems from noisy and inaccurate image captions in the training dataset. We address this by training a bespoke image captioner and use it to recaption the training dataset. We then train several text-to-image models and find that training on these synthetic captions reliably improves prompt following ability. Finally, we use these findings to build DALL-E 3: a new text-to-image generation system, and benchmark its performance on an evaluation designed to measure prompt following, coherence, and aesthetics, finding that it compares favorably to competitors. We publish samples and code for these evaluations so that future research can continue optimizing this important aspect of text-to-image systems.",United States of America,,,,,Unreleased,https://platform.openai.com/docs/models/dall-e,Industry
ERNIE 4.0,"Multimodal,Language,Video,Image generation","Chat,Language modeling/generation,Video generation,Image generation",,,https://www.prnewswire.com/news-releases/baidu-launches-ernie-4-0-foundation-model-leading-a-new-wave-of-ai-native-applications-301958681.html,,"Baidu Launches ERNIE 4.0 Foundation Model, Leading a New Wave of AI-Native Applications",2023-10-17,Baidu,,"""similar architecture with 3.5 version""  -interpreter dub at 01:25:08 https://www.youtube.com/watch?v=wYozcsavRuM",,may be mentioned here https://www.youtube.com/watch?v=wYozcsavRuM,,may be mentioned here https://www.youtube.com/watch?v=wYozcsavRuM,,,,,,Unknown,"Baidu, Inc. (NASDAQ: BIDU and HKEX: 9888), a leading AI company with strong Internet foundation, today hosted its annual flagship technology conference Baidu World 2023 in Beijing, marking the conference's return to an offline format after four years. With the theme ""Prompt the World,"" this year's Baidu World conference saw Baidu launch ERNIE 4.0, Baidu's next-generation and most powerful foundation model offering drastically enhanced core AI capabilities. Baidu also showcased some of its most popular applications, solutions, and products re-built around the company's state-of-the-art generative AI.                                                                               

Robin Li, Co-founder, Chairman and CEO of Baidu, announced ERNIE 4.0 at Baidu World 2023
""ERNIE 4.0 has achieved a full upgrade with drastically improved performance in understanding, generation, reasoning, and memory,"" Robin Li, Co-founder, Chairman and CEO of Baidu, said at the event. ""These four core capabilities form the foundation of AI-native applications and have now unleashed unlimited opportunities for new innovations.""

",China,,,,,,,Industry
Fuyu-8B,"Multimodal,Language,Vision","Chat,Image classification,Visual question answering,Image captioning","Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, Sağnak Taşırlar",Open weights (non-commercial),"https://www.adept.ai/blog/fuyu-8b, www.huggingface.co/adept/fuyu-8b",,Fuyu-8B: A Multimodal Architecture for AI Agents,2023-10-17,Adept,8000000000.0,"8B

Also a ""Fuyu-medium"" with unstated param count (<56B: ""Fuyu-Medium performs comparably to PALM-E-562B despite having fewer than a tenth as many parameters"")",,,,,,,,,,Confident,"We’re releasing Fuyu-8B, a small version of the multimodal1 model that powers our product. The model is available on HuggingFace. We think Fuyu-8B is exciting because:

It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy.
It’s designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images.
It’s fast - we can get responses for large images in less than 100 milliseconds.
Despite being optimized for our use-case, it performs well at standard image understanding benchmarks such as visual question-answering and natural-image-captioning.",United States of America,,,,,Unreleased,non-commercial: https://huggingface.co/adept/fuyu-8b,Industry
Aquila2 34B,Language,"Chat,Language modeling/generation",,Open weights (unrestricted),"https://github.com/FlagAI-Open/Aquila2
https://huggingface.co/BAAI/Aquila2-34B",,,2023-10-13,Beijing Academy of Artificial Intelligence / BAAI,34000000000.0,"34B
safetensors say it is 18.2B params

There's also a 70B ""experimental"" version: https://github.com/FlagAI-Open/Aquila2",,"6ND = 6*34000000000*2000000000000=4.08e+23

'Likely' confidence because unknown number of epochs, dataset size and number of parameters are not certain.",Unspecified unreleased,,2000000000000.0,"""we have investigated all 2 trillion tokens of data"" from https://github.com/FlagAI-Open/Aquila2",,,,Likely,"We announce that our Aquila2 series is now open source, comprising Aquila2 (the base language models: Aquila2-7B, Aquila2-34B and Aquila2-70B-Expr) and AquilaChat2 (the chat models, namely AquilaChat2-7B, AquilaChat2-34B and AquilaChat2-70B-Expr, as well as the long-text chat models, namely AquilaChat2-7B-16k and AquilaChat2-34B-16k). You can find the links in the following table. Kindly click on them to access the model cards.",China,,,,,Open source,apache 2.0,Academia
Jiutian,Language,Language modeling/generation,,,https://www.globaltimes.cn/page/202310/1299716.shtml,,,2023-10-12,China Mobile,13900000000.0,"A 13.9B parameter model is mentioned prominently at
https://jiutian.10086.cn/portal/#/home
2025-01-13.",1.668e+23,6*13.9e9*2e12=1.668e23,,,2000000000000.0,"""Designed to enhance efficiency, the model has trained over 2 trillion tokens""",,,,Likely,"China Mobile, the largest telecom operator in the world by subscribers, unveiled its ""Jiutian"" artificial intelligence (AI) large-scale model on Thursday, which has reportedly won support from large enterprises including China Ocean Shipping (Group) Co and China Railway Construction Co.",China,,,,,,,Industry
Mistral 7B,Language,"Code generation,Language generation,Language modeling/generation,Question answering","Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed",Open weights (unrestricted),https://arxiv.org/abs/2310.06825,1244.0,Mistral 7B,2023-10-10,Mistral AI,7000000000.0,,,,Unspecified unreleased,"""Unfortunately we're unable to share details about the training and the datasets (extracted from the open Web) due to the highly competitive nature of the field.""",,,,,,Confident,"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.",France,,,,,Unreleased,apache 2.0,Industry
CodeFuse-13B,Language,Code generation,"Peng Di, Jianguo Li, Hang Yu, Wei Jiang, Wenting Cai, Yang Cao, Chaoyu Chen, Dajun Chen, Hongwei Chen, Liang Chen, Gang Fan, Jie Gong, Zi Gong, Wen Hu, Tingting Guo, Zhichao Lei, Ting Li, Zheng Li, Ming Liang, Cong Liao, Bingchang Liu, Jiachen Liu, Zhiwei Liu, Shaojun Lu, Min Shen, Guangpei Wang, Huan Wang, Zhi Wang, Zhaogui Xu, Jiawei Yang, Qing Ye, Gehao Zhang, Yu Zhang, Zelin Zhao, Xunjin Zheng, Hailian Zhou, Lifu Zhu, Xianying Zhu",Open weights (unrestricted),https://arxiv.org/abs/2310.06266,3.0,CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model,2023-10-10,Ant Group,13000000000.0,,3.09e+23,"""CodeFuse-13B was trained using 512 Nvidia A100 GPU cards, with
a Hardware FLOPs Utilization (HFU) of approximately 60%. The
training process took approximately 40 days to complete."" Later they state utilization of 56%

512 * 312 trillion * 40 * 24 * 3600 * 0.56 = 3.09e23

Using params*tokens, we have 13 billion * 1 trillion * 6 = 7.8e22. might be a sign of multiple epochs? 1T is the size of the dataset; they don't clearly state the number of training tokens","The Stack,GitHub","80% code, 10% English, 10% Chinese: ""The pre-training data for CodeFuse consists of 196TB of code, 1.75TB of Chinese raw data, and 1.7TB of English raw data, totaling 200TB, that are tokenized into 800 billion
tokens of code, 100 billion tokens of Chinese corpus, and 100 billion
tokens of English corpus (see Section 3.1).""

""We collected about 200+ TB of code-related data, and finally refined it to around 1.6TB (1T Token) of clean data suitable for pre-training.""",1000000000000.0,"1T tokens, mostly code but some Chinese/English",960.0,~40 days,NVIDIA A100 SXM4 80 GB,Confident,"Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software development process where CodeFuse has been successfully deployed. The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes. In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts.",China,,,,512.0,Unreleased,apache: https://github.com/codefuse-ai/codefuse-chatbot?tab=License-1-ov-file#readme,Industry
Qwen-14B,Language,Language modeling/generation,"Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu",Open weights (restricted use),https://arxiv.org/abs/2309.16609,169.0,Qwen Technical Report,2023-09-28,Alibaba,14000000000.0,14B,2.5e+23,"3T tokens per Table 1

14B*3T*6 = 2.5e23",,"""Our dataset is designed to meet these requirements and includes public
web documents, encyclopedia, books, codes, etc. Additionally, our dataset is multilingual, with a significant portion of the data being in English and Chinese.""",3000000000000.0,"""We have pretrained the language models, namely QWEN, on massive datasets containing trillions of tokens""
Table 1 indicates 3T tokens for Qwen-14B, and the above quote suggests the 3T aren't from multiple epochs on a smaller dataset.",,,,Confident,"Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.",China,,,,,Unreleased,"commercial allowed, can't use to train models
https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT",Industry
Amazon Titan,"Language,Image generation","Semantic search,Image generation,Language modeling/generation,Code generation,Chat,Text-to-image,Translation",,API access,https://aws.amazon.com/bedrock/titan/,,,2023-09-28,Amazon,200000000000.0,"200B dense model
https://importai.substack.com/p/import-ai-365-wmd-benchmark-amazon",4.7999999999999996e+24,"trained using NVIDIA NeMo: https://blogs.nvidia.com/blog/nemo-amazon-titan/

13,760 NVIDIA A100 chips (using 1,720 P4d nodes). It took 48 days to train.
from https://importai.substack.com/p/import-ai-365-wmd-benchmark-amazon

counting operations: 6*200000000000*4000000000000=4.8e+24

gpu usage: 312000000000000(FLOP/s)*0.3*13760*1152*3600=5.3413281792e+24",,,4000000000000.0,"4T tokens of data, based on comments from amazon engineer James Hamilton at a 2024 talk: https://perspectives.mvdirona.com/2024/01/cidr-2024/
Also cited here:
https://lifearchitect.ai/titan/",1152.0,,NVIDIA A100,Likely,,United States of America,,,,13760.0,Unreleased,,Industry
Qwen-7B,Language,"Language modeling/generation,Translation","Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, Tianhang Zhu",Open weights (restricted use),"https://arxiv.org/abs/2309.16609, https://huggingface.co/Qwen/Qwen-7B",169.0,Qwen Technical Report,2023-09-28,Alibaba,7000000000.0,7B,1.01e+23,"2.4T tokens per Table 1

7b*2.4T*6 = 1.01e23",Unspecified unreleased,"""Our dataset is designed to meet these requirements and includes public
web documents, encyclopedia, books, codes, etc. Additionally, our dataset is multilingual, with a significant portion of the data being in English and Chinese.""",2400000000000.0,"""We have pretrained the language models, namely QWEN, on massive datasets containing trillions of tokens""
Table 1 indicates 2.4T tokens for Qwen-7B, and the above quote suggests the 2.4T aren't from multiple epochs on a smaller dataset.",,,,Confident,"Large language models (LLMs) have revolutionized the field of artificial intelligence, enabling natural language processing tasks that were previously thought to be exclusive to humans. In this work, we introduce Qwen, the first installment of our large language model series. Qwen is a comprehensive language model series that encompasses distinct models with varying parameter counts. It includes Qwen, the base pretrained language models, and Qwen-Chat, the chat models finetuned with human alignment techniques. The base language models consistently demonstrate superior performance across a multitude of downstream tasks, and the chat models, particularly those trained using Reinforcement Learning from Human Feedback (RLHF), are highly competitive. The chat models possess advanced tool-use and planning capabilities for creating agent applications, showcasing impressive performance even when compared to bigger models on complex tasks like utilizing a code interpreter. Furthermore, we have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as well as mathematics-focused models, Math-Qwen-Chat, which are built upon base language models. These models demonstrate significantly improved performance in comparison with open-source models, and slightly fall behind the proprietary models.",China,,,,,Unreleased,"commercial allowed, can't use to train models
https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT",Industry
PLaMo-13B,Language,"Language modeling/generation,Chat,Question answering","Preferred Networks, Inc",Open weights (unrestricted),https://huggingface.co/pfnet/plamo-13b,, PLaMo-13B,2023-09-28,Preferred Networks Inc,13000000000.0,,1.17e+23,"6ND = 6*13e9*1.5e12=1.17e+23
from https://huggingface.co/pfnet/plamo-13b#model-details

480 GPUs * 30 days [assumed, likely less] * 24 hours * 3600 s * 77970000000000 FLOP/s * 41.0 [reported utilization] = 3.9772934e+24


","C4,Project Gutenberg,RedPajama,mC4,Wikipedia (ja)",from https://huggingface.co/pfnet/plamo-13b#training-dataset,1500000000000.0,"Trained tokens: 1.5T tokens (English: 1.32T tokens, Japanese: 0.18T tokens)
from https://huggingface.co/pfnet/plamo-13b#model-details

0.75*1.32T + 0.18T = 1170000000000
0.75 words per token for English
1 for Japanese ",720.0,"""We used 60 ABCI A nodes (480 GPUs) for just under a month, and trained the training data with a total of 1.4T tokens with a context length of 4096.""

https://tech.preferred.jp/ja/blog/llm-plamo/",NVIDIA A100 SXM4 40 GB,Confident,,Japan,,,,480.0,Unreleased,Apache 2.0 for weights. Open data,Industry
Show-1,Video,Video generation,"David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, Mike Zheng Shou",Open weights (non-commercial),https://arxiv.org/abs/2309.15818,,Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation,2023-09-27,National University of Singapore,,,,,WebVid-10M,"""WebVid-10M is a large-scale dataset of short videos with textual descriptions sourced from stock footage sites. The videos are diverse and rich in their content. 10.7M video-caption pairs. 52K total video hours.""",,"WebVid-10M
10.7M video-caption pairs. 52K total video hours.",,,NVIDIA A100,Unknown,"Significant advancements have been achieved in the realm of large-scale pre-trained text-to-video Diffusion Models (VDMs). However, previous methods either rely solely on pixel-based VDMs, which come with high computational costs, or on latent-based VDMs, which often struggle with precise text-video alignment. In this paper, we are the first to propose a hybrid model, dubbed as Show-1, which marries pixel-based and latent-based VDMs for text-to-video generation. Our model first uses pixel-based VDMs to produce a low-resolution video of strong text-video correlation. After that, we propose a novel expert translation method that employs the latent-based VDMs to further upsample the low-resolution video to high resolution. Compared to latent VDMs, Show-1 can produce high-quality videos of precise text-video alignment; Compared to pixel VDMs, Show-1 is much more efficient (GPU memory usage during inference is 15G vs 72G). We also validate our model on standard video generation benchmarks. Our code and model weights are publicly available at this https URL.",Singapore,,,,,Unreleased,"https://github.com/showlab/Show-1 don't see training code
Attribution-NonCommercial 4.0 International
",Academia
Baichuan 2-7B,,,"Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, Zhiying Wu",,https://arxiv.org/pdf/2309.10305,405.0,"Baichuan 2: Open Large-scale Language Models
",2023-09-20,Baichuan,7000000000.0,,1.092e+23,"7b * 2.6t * 6 = 1.092e23
Also mentions 1,024 NVIDIA A800 GPUs at 180 TFLOPS per GPU",,,2600000000000.0,,,,NVIDIA A800,Confident,"In this technical report, we present Baichuan 2, a series of large-scale multilingual language models containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion tokens. ",China,,,,1024.0,,,Industry
Hunyuan,"Language,Image generation,Multimodal","Language modeling/generation,Image generation,Question answering",,API access,https://www.tencent.com/en-us/articles/2201685.html,,"Tencent Unveils Hunyuan, its Proprietary Large Foundation Model on Tencent Cloud",2023-09-07,Tencent,100000000000.0,"""Presently, the Hunyuan model has over 100 billion parameters, with more than two trillion tokens in pre-training data.""",1.1999999999999999e+24,6ND = 6*100*10^9*2*10^12 = 1.2*10^24,Unspecified unreleased,,2000000000000.0,"""Presently, the Hunyuan model has over 100 billion parameters, with more than two trillion tokens in pre-training data.""",,,,Confident,"Enterprises in China may now access Hunyuan via Tencent’s public cloud platform and finetune it to their specific needs. The platform features strong Chinese language processing abilities, advanced logical reasoning, and comes with reliable task execution abilities.

Tencent’s foundation model supports a wide array of functions spanning the creation of images, copywriting, text recognition, and customer service, to name a few. These will be instrumental in key industries like finance, public services, social media, e-commerce, transportation, games, and many more.

This empowers enterprises to build powerful tools, in addition to training their own unique large models derived from Tencent’s Model-as-a-Service (MaaS) offering, which was first introduced in June this year. The MaaS provides enterprises with economically viable, industry-specific large models, featuring more than 50 solutions spanning 20 major industries. This creates a virtuous cycle in which enterprises refine their large models with Hunyuan to create uniquely intelligent services across their operations. ",China,,,,,Unreleased,,Industry
Falcon-180B,Language,Language modeling,"Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, Guilherme Penedo",Open weights (restricted use),https://falconllm.tii.ae/falcon-180b.html; https://arxiv.org/abs/2311.16867,261.0,The Falcon Series of Open Language Models,2023-09-06,Technology Innovation Institute,180000000000.0,"""Falcon 180B is a super-powerful language model with 180 billion parameters""",3.76e+24,"43,500 petaflop-days per Table 1 of the paper

43500 * 1e15 * 24 * 3600 = 3.76e24


C = 6ND = 6 FLOP/token/parameter * 3.5 trillion tokens * 180 billion parameters = 3.78*10^24 FLOP",RefinedWeb,"""The Falcon series is made of three causal decoder-only models trained on up to 4,096 A100. We assembled a pretraining dataset of 3,500 billion tokens, predominantly sourced from our work on RefinedWeb (Penedo et al., 2023)–a massive filtered and deduplicated web dataset""

Training dataset composition is described in Table 3. Falcon was trained for 1 epoch.",2625000000000.0,3.5 trillion tokens * (~3 words per 4 tokens) ~= 2.625 trillion words,4320.0,"Stanford CRFM foundation model ecosystem graph data page https://crfm.stanford.edu/ecosystem-graphs/index.html?asset=Falcon-180B says 9 months, which is the maximum possible amount of time: training began sometime in 2023, and it was released in September. 

However, 6 months is more realistic. That is the length of the gap between Falcon 40B and Falcon 180B. Additionally, the amount of compute is specified in the paper, so there is only one degree of freedom in the uncertain values of training duration and hardware utilization rate. At six months, the utilization is unusually low, so the training was probably not longer than that.",NVIDIA A100 SXM4 40 GB,Confident,"Falcon 180B is a super-powerful language model with 180 billion parameters, trained on 3.5 trillion tokens. It's currently at the top of the Hugging Face Leaderboard for pre-trained Open Large Language Models and is available for both research and commercial use.

This model performs exceptionally well in various tasks like reasoning, coding, proficiency, and knowledge tests, even beating competitors like Meta's LLaMA 2.

Among closed source models, it ranks just behind OpenAI's GPT 4, and performs on par with Google's PaLM 2 Large, which powers Bard, despite being half the size of the model.",United Arab Emirates,,,,4096.0,Unreleased,"""Falcon 180b can be commercially used but under very restrictive conditions, excluding any ""hosting use""."" https://huggingface.co/blog/falcon-180b",Government
Baichuan2-13B,Language,Chat,"Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, JunTao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, Zhiying Wu",Open weights (restricted use),"https://huggingface.co/baichuan-inc/Baichuan2-13B-Base, https://arxiv.org/abs/2309.10305",,Baichuan 2: Open Large-scale Language Models,2023-09-06,Baichuan,13000000000.0,,2.03e+23,"They describe the dataset as having 2.6T tokens, but the checkpoint graph makes it clear that's also the number of tokens the model was trained on.

13b * 2.6t * 6 = 2.03e23",,"2.6 trillion tokens, bilingual.

paper/model card don't give breakdown between English and Chinese",2275000000000.0,"2.6T tokens, or ~2.3T words assuming that the dataset is roughly even English (0.75 words/token) and Chinese (1 word/token)

1.3T Chinese tokens * (1 word/token) = 1.3T Chinese words
1.3T English tokens * (0.75 words/token) = 0.975T English words
total: 2.275T, or ~2.3T",,,,Confident,,China,,,,1024.0,Unreleased,"Baichuan community license, restrictive commercial: https://huggingface.co/baichuan-inc/Baichuan2-13B-Base",Industry
360 Smart Brain,"Multimodal,Language,Image generation","Language generation,Chat,Image generation",,,https://www.hayo.com/article/64f68f1d8578eea6c7ec663f,,The 360 ​​Brain Model is now open to the public,2023-09-04,360 Security Technology,,"""hundreds of billions"" https://www.hayo.com/article/650babb37c769bcba319ed83",,"hundreds of billions of parameters, trained on over 1T tokens, per this: https://www.hayo.com/article/650babb37c769bcba319ed83

vague report from a Google-translated article, though",,"hundreds of billions of parameters, trained on over 1T tokens, per this: https://www.hayo.com/article/650babb37c769bcba319ed83",1000000000000.0,,,,,Likely,"According to news on September 5, the 360 ​​Smart Brain large model will be open to the public from now on and will be fully accessible to the 360 ​​“Family Bucket”.

360 Zhi Nao will be open to the public on five major platforms. Users can download the “360 Zhi Nao” App through the 360 ​​Zhi Nao official website and major application stores.",China,,,,,,,Industry
ABAB,Language,Language generation,,API access,"https://kr-asia.com/baidus-ernie-bot-among-eight-chinese-llm-products-approved-for-public-launch, https://api.minimax.chat/",,,2023-08-30,MiniMax,,"""hundreds of billions"" per https://api.minimax.chat/",,,Unspecified unreleased,,,,,,,Unknown,"MiniMax has released three foundational model architectures: text-to-visual, text-to-audio, and text-to-text. The startup has also introduced a self-developed general LLM “ABAB”, named after the sound of baby babble.",China,,,,,Unreleased,,Industry
"Luca 2.0
",,,,,https://www.163.com/dy/article/IDBGA8840511FQO9.html,,,2023-08-29,Mianbi Intelligence,100000000000.0,"https://www.leiphone.com/category/ai/23kbzQXj60xZgUgO.html 
English translation:
""Li Dahai: From a technical point of view, the CPM2 (Chinese Pretrained Model) 100 billion model we launched at that time was a sparse model of MoE, which is different from the 100 billion model we are promoting now.""

This suggests it is a dense model.",1.1999999999999999e+24,"Assume Chinchilla-optimal dataset size:
20 * 100B * 100B * 6 = 1.2e24 FLOP",,,,,,,,Unverified,,China,,,,,,,Industry
MathGPT,Language,Quantitative reasoning,,Hosted access (no API),https://www.gizmochina.com/2023/08/24/mathgpt-launch-public-beta-next-gen-math-assistant/,,TAL’s MathGPT launches public beta: your next-gen math assistant,2023-08-24,TAL Education Group (Xueersi),,,,,,,,,,,,Unknown,"During its 20th-anniversary event, TAL Education Group launched the public beta testing of its innovative mathematical large model, MathGPT. This LLM model is designed primarily for global mathematics enthusiasts and research institutions, marking a significant milestone as China’s first large model tailored for mathematics.",China,,,,,Unreleased,,Industry
HyperCLOVA X,Language,"Language modeling/generation,Chat,Search,Translation,Code generation,Question answering,Quantitative reasoning","Kang Min Yoo, Jaegeun Han, Sookyo In, Heewon Jeon, Jisu Jeong, Jaewook Kang, Hyunwook Kim, Kyung-Min Kim, Munhyong Kim, Sungju Kim, Donghyun Kwak, Hanock Kwak, Se Jung Kwon, Bado Lee, Dongsoo Lee, Gichang Lee, Jooho Lee, Baeseong Park, Seongjin Shin, Joonsang Yu, Seolki Baek, Sumin Byeon, Eungsup Cho, Dooseok Choe, Jeesung Han, Youngkyun Jin, Hyein Jun, Jaeseung Jung, Chanwoong Kim, Jinhong Kim, Jinuk Kim, Dokyeong Lee, Dongwook Park, Jeong Min Sohn, Sujung Han, Jiae Heo, Sungju Hong, Mina Jeon, Hyunhoon Jung, Jungeun Jung, Wangkyo Jung, Chungjoon Kim, Hyeri Kim, Jonghyun Kim, Min Young Kim, Soeun Lee, Joonhee Park, Jieun Shin, Sojin Yang, Jungsoon Yoon, Hwaran Lee, Sanghwan Bae, Jeehwan Cha, Karl Gylleus, Donghoon Ham, Mihak Hong, Youngki Hong, Yunki Hong, Dahyun Jang, Hyojun Jeon, Yujin Jeon, Yeji Jeong, Myunggeun Ji, Yeguk Jin, Chansong Jo, Shinyoung Joo, Seunghwan Jung, Adrian Jungmyung Kim, Byoung Hoon Kim, Hyomin Kim, Jungwhan Kim, Minkyoung Kim, Minseung Kim, Sungdong Kim, Yonghee Kim, Youngjun Kim, Youngkwan Kim, Donghyeon Ko, Dughyun Lee, Ha Young Lee, Jaehong Lee, Jieun Lee, Jonghyun Lee, Jongjin Lee, Min Young Lee, Yehbin Lee, Taehong Min, Yuri Min, Kiyoon Moon, Hyangnam Oh, Jaesun Park, Kyuyon Park, Younghun Park, Hanbae Seo, Seunghyun Seo, Mihyun Sim, Gyubin Son, Matt Yeo, Kyung Hoon Yeom, Wonjoon Yoo et al. (296 additional authors not shown)",API access,"https://arxiv.org/abs/2404.01954
https://techcrunch.com/2023/08/24/koreas-internet-giant-naver-unveils-generative-ai-services/, https://www.ncloud.com/solution/featured/hyperclovax",,Korea’s internet giant Naver unveils generative AI services,2023-08-24,NAVER,,"Unknown

Previous version had 204B parameters: ""Naver says HyperCLOVA has more than 204 billion parameters, but it did not disclose how many parameters have been trained on the HyperCLOVA X""

Maybe ambiguous whether HyperCLOVA X is a new and separate model? But HyperClova is pretty old.

""HyperCLOVA X is built on HyperCLOVA and improves on the previous LLMs""

https://www.ncloud.com/solution/featured/hyperclovax",,Estimations for 82B model are marked as lower bound estimations,,,,,,,,Speculative,"South Korean internet search company Naver on Thursday rolled out its own generative artificial intelligence tool, HyperCLOVA X. The company’s large language model (LLM) offers services such as a ChatGPT-like AI chatbot, CLOVA X, and a generative AI-based search engine, Cue, equivalent to Microsoft Bing. ",Korea (Republic of),,,,,Unreleased,"API access for HyperCLOVA X is available at CLOVA Studio, a Hyperscale AI development tool optimized for businesses and provided via NAVER Cloud Platform. The chat service is available at https://clovax.naver.com/.",Industry
Dou Bao,Language,"Chat,Language modeling/generation",,Hosted access (no API),https://pandaily.com/bytedance-launches-its-first-large-scale-ai-conversation-product-dou-bao/,,,2023-08-18,ByteDance,,,,,,,,,,,,Unknown,"The first AI conversational app “Dou Bao” and its web version have recently been launched, with the download channel for Android already open. The “Dou Bao” is the internal codename “Grace” AI project by ByteDance, and currently has functions such as text-based conversation and image-based conversation.",China,,,,,Unreleased,,Industry
KwaiYii 13B,Language,Chat,,Unreleased,https://github.com/kwai/KwaiYii,,"""KwaiYii"" large-scale language model (KwaiYii)",2023-08-16,Kuaishou Technology,13000000000.0,13B,,,,,,,,,,Likely,"""KwaiYii"" is a series of large-scale language models (LLM) independently developed by the Kuaishou AI team from scratch. It currently includes models with multiple parameter sizes and covers pre-training models. (KwaiYii-Base), dialogue model (KwaiYii-Chat). Here we introduce the 13B scale series model KwaiYii-13B.""",China,,,,,,,Industry
VARCO LLM 2.0 base,Language,"Language modeling/generation,Chat,Translation,Question answering",,API access,"https://ncsoft.github.io/ncresearch/varco-llm-details/
https://aws.amazon.com/marketplace/pp/prodview-d7amr4yxpibew?sr=0-3&ref_=beagle&applicationId=AWSMPContessa",,VARCO LLM 2.0 is NCSOFT's large language model that can be applied to the development of natural language processing-based AI services.,2023-08-16,NCSOFT,13000000000.0,,1.248e+23,=1600000000000*6*13000000000=1.248×10^23,,"""Our LLM is trained with datasets that are either publicly available for pretraining, collected from the Internet or internally constructed,” Jehee Lee, CRO of NCSOFT, told Engadget via email.",1600000000000.0,https://ncsoft.github.io/ncresearch/varco-llm-details/,,,,Likely,"VARCO LLM 2.0 is NCSOFT's large language model that can be applied to the development of various natural language processing-based AI services such as text generation, question answering, chatbots, summarization, and information extraction. NCSOFT's VARCO LLM 2.0 was developed with our own technology, including data construction, pre-training, instruction tuning and alignment tuning. We evaluated VARCO LLM 2.0 on various NLP tasks and its performance has significantly improved compared to VARCO LLM 1.0, and it boasts the highest performance among other Korean LLMs of similar sizes. In particular, it has been trained to be used in high-level natural language processing applications such as creative writing, summarization, question and answering, chatbots and translation, and shows high performance in related quantitative indicators. For inquiries regarding further performance improvement or collaboration for service applications, please contact us by email (varco_llm@ncsoft.com).

Korean Text Generation : VARCO LLM 2.0 is optimized for Korean natural language generation applications. In particular, it provides more natural and creative responses in understanding user instructions and generating text.",Korea (Republic of),,,,,,,Industry
Baichuan2-53B,Language,"Language modeling/generation,Chat",,,https://technode.com/2023/08/09/chinese-ai-startup-baichuan-rolls-out-third-llm-in-four-months/,,Chinese AI startup Baichuan rolls out third LLM in four months,2023-08-09,Baichuan,53000000000.0,,8.268e+23,"Given that it was announced at a similar time to the other Baichuan2 models, this assumes that the dataset size is the same at 2.6T tokens while the parameter count was scaled up. This would be consistent with many other model releases, such as Meta's Llama models.
53b * 2.6t * 6 = 8.268e23
",,,,,,,,Likely,"On Tuesday, four-month-old AI startup Baichuan Intelligent Technology unveiled its first closed-source model equipped with 53 billion parameters. Following the Chinese company’s rapid release of two open-source large language models since its founding in April, the new model demonstrates the firm’s fast pace in delivering pre-trained models for larger parameters. The freshly introduced model, Baichuan-53B, is mainly for corporate clients and focused on text generation. A ChatGPT-like chat service built on the model entered internal testing on the same day the model was launched, its official website shows, with plans for the firm to publicly launch APIs and associated components next month.",China,,,,,,,Industry
Claude Instant,Language,"Language modeling,Chat",,API access,https://www.anthropic.com/news/releasing-claude-instant-1-2,,Releasing Claude Instant 1.2,2023-08-09,Anthropic,,"speculatively, Anthropic charges 1/10 as much for Claude Instant as Claude 2, so it may have around 1/10 the parameters (Claude 2 parameters are not public info)

https://cdn.sanity.io/files/4zrzovbb/website/90df03aed08b794ab03c5a7bf28b2ad9cf26cf3c.pdf",,,Unspecified unreleased,,,,,,,Unknown,"Businesses working with Claude can now access our latest version of Claude Instant, version 1.2, available through our API. Claude Instant is our faster, lower-priced yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document comprehension.

Claude Instant 1.2 incorporates the strengths of our latest model Claude 2 in real-world use cases and shows significant gains in key areas like math, coding, reasoning, and safety. It generates longer, more structured responses and follows formatting instructions better. Instant 1.2 also shows improvements in quote extraction, multilingual capabilities, and question answering.",United States of America,,,,,Unreleased,,Industry
CALM,Robotics,Animal (human/non-human) imitation,"Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal Chechik, Xue Bin Peng",,https://research.nvidia.com/labs/par/calm/,9.0,CALM: Conditional Adversarial Latent Models for Directable Virtual Characters,2023-08-06,"NVIDIA,Technion - Israel Institute of Technology",,,,The total pre-training of the networks involved 5 billion environment steps. The low-level policy operates at 30Hz while the high-level policy operates at 6Hz.,,"160 diverse motion clips totaling 30 minutes, from a motion capture dataset. These include motions like walking, running, sword strikes, etc.",,,,,NVIDIA A100,Unknown,"In this work, we present Conditional Adversarial Latent Models (CALM),
an approach for generating diverse and directable behaviors for user-controlled
interactive virtual characters. Using imitation learning, CALM learns a representation of movement that captures the complexity and diversity of human
motion, and enables direct control over character movements. The approach
jointly learns a control policy and a motion encoder that reconstructs key
characteristics of a given motion without merely replicating it. The results
show that CALM learns a semantic motion representation, enabling control
over the generated motions and style-conditioning for higher-level task training. Once trained, the character can be controlled using intuitive interfaces,
akin to those found in video games.","United States of America,Israel",,,,,,,"Industry,Academia"
Zi Yue,Language,Chat,,,https://www.chinadaily.com.cn/a/202307/28/WS64c3226ea31035260b8190a4.html,,NetEase Youdao launches first large model in education,2023-07-28,NetEase,,,,,,,,,,,,Unknown,"As Open AI's ChatGPT takes the tech world by storm, Chinese educational technology firm NetEase Youdao launched its large model, along with up to six applications, on Thursday, which marked the birth of one of China's first large models in the education sector.",China,,,,,,,Industry
Zi Yue 2.0,Language,Chat,,,https://m.aastocks.com/en/usq/news/comment.aspx?source=AAFN&id=NOW.1317044&catg=1,,"NetEase Youdao Upgrades 'Ziyue' Foundation Model to 2.0 Ver, Encompassing More Subjects & Teaching Areas",2023-07-28,NetEase,,,,,,,,,,,,Unknown,"Chinese media reported that Youdao (DAO.US)     has released the 2.0 upgrade for its Ziyue educational foundation model. Youdao also launched Youdao Speed Reading (literal translation of ""有道速讀""), new-generation virtual personality verbal language trainer, AI home tutors, and Youdao-branded new-generation intelligent hardware applications.

It is reported that Ziyue 2.0 has been upgraded in the knowledge question and answering ability within the education scene, with it expanding to more subjects and teaching areas. The amount of educational data has been largely expanded, the model's context window has been upgraded to 16,000 tokens, and new Agent and retrieval enhancement capabilities have been added.",China,,,,,,,Industry
EXAONE 2.0,"Multimodal,Image generation,Language,Biology,Vision","Language modeling,Image generation,Visual question answering",,Unreleased,https://aws.amazon.com/solutions/case-studies/lg-ai-research-case-study/,,LG AI Research Develops Foundation Model Using Amazon SageMaker,2023-07-19,LG AI Research,300000000000.0,300 billion,,,Unspecified unreleased,"From KoreaTimes (https://www.koreatimes.co.kr/www/tech/2023/12/129_355258.html)

""EXAONE 2.0 studied about 45 million specialized documents and 350 million images, including patents and papers secured through partnerships.

Considering that much of the existing expertise data is in English, EXAONE 2.0 is developed as a bilingual model that can understand and answer both in Korean and English at the same time. It also learns over four times more data than the previous model.""",,,,,,Speculative,,Korea (Republic of),,,,,,,Industry
Llama 2-70B,Language,Language modeling,"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom
",Open weights (restricted use),"https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
https://arxiv.org/abs/2307.09288",8056.0,Llama 2: Open Foundation and Fine-Tuned Chat Models,2023-07-18,Meta AI,70000000000.0,"Llama has been released in 7B, 13B, 34B, and 70B variants.",8.1e+23,"""Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB"" of which 1720320 GPU hours were used to train the 70B model.

311.84 BF16 TFLOP/s * 1720320 hours * 0.40 utilization = 7.725e+23 FLOP.

Alternatively: the model was trained for 1 epoch on 2 trillion tokens and has 70B parameters. C = 6ND = 6*70B*2T = 8.4e+23 FLOP.",Llama 2 dataset,"2 trillion tokens of publicly available text, with no text from Meta's products.
""Our training corpus includes a new mix of data from publicly available sources, which does not include data from Meta’s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.""",1500000000000.0,2 trillion tokens ~= 1.5 trillion words,1728.0,"Model was trained from January 2023 to July 2023, which is six months. However, the training run duration did not take up this whole period. According to a Meta employee interviewed by Epoch, Llama 2 34B and 70B were trained on different clusters, with overlapping training periods. Based on an estimate of 1000 GPUs, it would have taken 72 days.",NVIDIA A100 SXM4 80 GB,Confident,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",United States of America,,,,1000.0,Unreleased,"Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE",Industry
Llama 2-34B,Language,Language modeling,"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom
",Unreleased,"https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
https://arxiv.org/abs/2307.09288",8056.0,Llama 2: Open Foundation and Fine-Tuned Chat Models,2023-07-18,Meta AI,34000000000.0,"Llama has been released in 7B, 13B, 34B, and 70B variants.",4.08e+23,"All models sizes trained on 2.0T tokens, per table 1
2T * 34b * 6 = 4.08e23

Also trained on 1038336 A100-hours, which is 3.5e23 at 30% utilization. So the utilization was probably around 35%.",Llama 2 dataset,"2 trillion tokens of publicly available text, with no text from Meta's products.
""Our training corpus includes a new mix of data from publicly available sources, which does not include data from Meta’s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this
provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.""",,,,,NVIDIA A100 SXM4 80 GB,Confident,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned
large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.
Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our
models outperform open-source chat models on most benchmarks we tested, and based on
our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety
improvements of Llama 2-Chat in order to enable the community to build on our work and
contribute to the responsible development of LLMs.",United States of America,,,,,Unreleased,,Industry
Llama 2-13B,Language,Language modeling,"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom
",Open weights (restricted use),"https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
https://arxiv.org/abs/2307.09288",8056.0,Llama 2: Open Foundation and Fine-Tuned Chat Models,2023-07-18,Meta AI,13000000000.0,"Llama has been released in 7B, 13B, and 70B variants.",1.6e+23,13 billion parameters * 2 trillion tokens * 6 FLOP / token / parameter = 1.6e23 FLOP,Llama 2 dataset,"2 trillion tokens of publicly available text, with no text from Meta's products.
""Our training corpus includes a new mix of data from publicly available sources, which does not include data from Meta’s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations.""",2000000000000.0,2 trillion tokens ~= 1.5 trillion words,,,NVIDIA A100 SXM4 80 GB,Confident,"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.",United States of America,,,,,Unreleased,"Llama 2 license. can't use outputs to train models.

https://github.com/meta-llama/llama/blob/main/LICENSE",Industry
BaiLing,Multimodal,"Language modeling/generation,Question answering,Visual question answering",,Unreleased,https://www.antgroup.com/en/technology/new-tech-Technology-Antfocuses-Tabcomnent-detail/20241028001,,,2023-07-15,Ant Group,,,,,Unspecified unreleased,,,,,,,Unknown,"Ant Group’s BaiLing foundation model has made significant advancements in computing power, security, and knowledge processing. It has established a computing cluster with tens of thousands of heterogeneous accelerator cards, integrated security capabilities ranging from detection to defense, and the ability to process trillions of tokens.",China,,,,,Unreleased,,Industry
ChatRhino,Language,Chat,,,https://jdcorporateblog.com/jd-com-introduces-chatrhino-empowering-industry-innovations-with-an-advanced-large-language-model/,,JD.com Introduces ChatRhino: Empowering Industry Innovations with an Advanced Large Language Model,2023-07-13,JD.com,100000000000.0,"""ChatRhino sets a new benchmark as a 100-billion-parameter model"", could be substantially rounded",,,,"Mix of general and supply chain data: ""By combining 70% generalized data with 30% native intelligent supply chain data, JD’s latest AI model offers targeted solutions for real industry challenges across sectors such as retail, logistics, finance, health, and city""",,,,,,Confident,"JD.com today unveiled its ChatRhino (Yanxi in Chinese) large language model (LLM) on its 2023 JDDiscovery tech summit, tailored to serve various industries. By combining 70% generalized data with 30% native intelligent supply chain data, JD’s latest AI model offers targeted solutions for real industry challenges across sectors such as retail, logistics, finance, health, and city. Building upon the success of the billion-parameter model K-PLUG launched in 2021 and the 10-billion-parameter model Vega introduced in 2022, JD’s ChatRhino sets a new benchmark as a 100-billion-parameter model.",China,,,,,,,Industry
Claude 2,Language,"Language modeling,Chat,Language modeling/generation,Question answering",,API access,"https://www.anthropic.com/index/claude-2, https://www-files.anthropic.com/production/images/Model-Card-Claude-2.pdf",,,2023-07-11,Anthropic,,,3.866e+24,https://colab.research.google.com/drive/1MdPuhS4Emaf23VXYZ-ooExDW-5GXZkw0#scrollTo=Ds0Q5X8aMnOY,Unspecified unreleased,"From model card: ""Claude models are trained on a proprietary mix of publicly available information from the Internet, datasets
that we license from third party businesses, and data that our users affirmatively share or that crowd workers provide. Some of the human feedback data used to finetune Claude was made public [12] alongside our RLHF [2] and red-teaming [4] research.
Claude 2’s training data cuts off in early 2023, and roughly 10 percent of the data included was non-English.""",,,,,,Speculative,,United States of America,,,,,Unreleased,,Industry
TeleChat,Language,Chat,,,https://m.thepaper.cn/baijiahao_23766944,,,2023-07-07,China Telecom,,,,,,"(Google translated from https://m.thepaper.cn/baijiahao_23766944) ""TeleChat uses a large amount of high-quality Chinese and English corpus for pre-training, and uses tens of millions of question and answer data for fine-tuning""",,,,,,Unknown,"(Google translated) At China Telecom's ""Computing and Network Integration·Sunac Future"" sub-forum, China Telecom Digital Intelligence Technology Branch (hereinafter referred to as: Telecom Zhike) officially released China Telecom's large language model TeleChat and demonstrated the large model empowering data Products in three directions: middle platform, intelligent customer service and intelligent government affairs.",China,,,,,,,Industry
Pangu 3.0,"Multimodal,Language,Image generation,Vision","Language modeling/generation,Image generation",,API access,https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html,,"Reshaping Industries with AI: Huawei Cloud Launches Pangu Models 3.0 and Ascend AI Cloud Services
",2023-07-07,Huawei,100000000000.0,"100B? I think the five foundation models are all included in the same system, instead of being five different variants of Pangu, but that's not very clear. I think that's implied by ""All of these capability sets will be made available to customers and partners, and will be consistent regardless of the model size"". 

https://www.huaweicloud.com/intl/en-us/news/20230707180809498.html 
""Huawei Cloud Pangu models were designed to focus on the practical needs of specific industry scenarios. The newly-launched Pangu Models 3.0 use a ""5+N+X"" three-layer architecture.

The L0 layer consists of five foundation models: NLP, CV, multimodal, prediction, and scientific computing, which provide general skills to power an endless possibility of industry-specific applications. Pangu Models 3.0 are available in different sizes: 10 billion parameters, 38 billion parameters, 71 billion parameters, and 100 billion parameters, meeting diverse customer needs and different standards on latency and response times. Brand-new capability sets are also provided, such as knowledge-based Q&A, copywriting, and code generation for the Pangu NLP model; and image generation and understanding for the Pangu multimodal model. All of these capability sets will be made available to customers and partners, and will be consistent regardless of the model size (number of parameters).

The L1 layer consists of N industry-tailored models. Huawei Cloud can provide customers with industry models it has trained on open industry datasets, including Pangu models for government, finance, manufacturing, mining, and meteorology. Alternatively, customers can train their own models using their own datasets based on Huawei's L0 or L1 Pangu models.

The L2 layer provides pre-trained models for specific industry scenarios and tasks, such as intelligent government hotline, intelligent branch assistant, lead compound screening, conveyor belt foreign object detection, and typhoon trajectory prediction. These models can be quickly deployed off-the-shelf.""

",,,,,,,,,,Likely,"Huawei Cloud Pangu models were designed to focus on the practical needs of specific industry scenarios. The newly-launched Pangu Models 3.0 use a ""5+N+X"" three-layer architecture.

The L0 layer consists of five foundation models: NLP, CV, multimodal, prediction, and scientific computing, which provide general skills to power an endless possibility of industry-specific applications. Pangu Models 3.0 are available in different sizes: 10 billion parameters, 38 billion parameters, 71 billion parameters, and 100 billion parameters, meeting diverse customer needs and different standards on latency and response times. Brand-new capability sets are also provided, such as knowledge-based Q&A, copywriting, and code generation for the Pangu NLP model; and image generation and understanding for the Pangu multimodal model. All of these capability sets will be made available to customers and partners, and will be consistent regardless of the model size (number of parameters).",China,,,,,,,Industry
InternLM,Language,Language modeling,,,https://internlm.org/,,,2023-07-06,"Shanghai AI Lab,SenseTime",100000000000.0,Pre-training a bilingual 100B Foundation model on data with over a trillion tokens,6.000001e+23,6 * 100b * 1t = 6e23,,,1000000000.0,"""Pre-training a bilingual 100B Foundation model on data with over a trillion tokens""",,Training performance for the open-source InternLM-7B: https://github.com/InternLM/InternLM/blob/main/doc/en/train_performance.md,NVIDIA A100 SXM4 80 GB,Confident,"Pre-training a bilingual 100B Foundation model on data with over a trillion tokens, the model exhibits excellent performance in scenarios such as Chinese, English, and coding due to the appropriate data ratio. Based on the foundation model, the application of high-quality human annotated dialogue data combined with RLHF technology enables the InternLM large language model to respond to complex commands during human interaction, while also demonstrating responses in line with human morality and values.","China,Hong Kong,China",,,,,,,"Academia,Industry"
xTrimoPGLM -100B,Biology,"Proteins,Protein or nucleotide language model (pLM/nLM)","Bo Chen, Xingyi Cheng, Yangli-ao Geng, Shen Li, Xin Zeng, Boyan Wang, Jing Gong, Chiming Liu, Aohan Zeng, Yuxiao Dong, Jie Tang, Le Song",Unreleased,https://www.biorxiv.org/content/10.1101/2023.07.05.547496v4,65.0,xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein,2023-07-06,"Tsinghua University,BioMap Research",100000000000.0,"Abstract: ""training xTrimoPGLM at an unprecedented scale of 100 billion
parameters and 1 trillion training tokens""",6.2e+23,"""xTrimoPGLM-100B is trained on a cluster of 96 DGX-A100 GPU (8×40G) servers in FP16 precision from January 18 to June 30, 2023. During this time, xTrimoPGLM-100B has consumed 1 trillion tokens from the dataset consisting of Uniref90 and ColAbFoldDB. As of the current date, xTrimoPGLM-100B continues its pre-training process to pass through as many tokens as possible""

6 * 100 billion params * 1T tokens = 6e23

8*96 * 312 trillion * 163 days * 24 * 3600 * 0.3 ~= 1e24

directly given in the paper (Table 9, or Table 4 in new version): 6.2E+23 ",UniRef50,,,~24M protein sequences,3912.0,163 days,NVIDIA A100 SXM4 40 GB,Confident,"Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. This paper proposes a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that xTrimoPGLM significantly outperforms other advanced baselines in diverse protein understanding tasks (13 out of 15 tasks across four categories) and generates novel protein sequences which are structurally similar to natural ones. Furthermore, using the same xTrimoPGLM framework, we train an antibody-specific model (xTrimoPGLM-Ab) using 1 billion parameters. This model set a new record in predicting antibody naturalness and structures, both essential to the field of antibody-based drug design, and demonstrated a significantly faster inference speed than AlphaFold2. These results highlight the substantial capability and versatility of xTrimoPGLM in understanding and generating protein sequences.","China,China",,,,768.0,Unreleased,,"Academia,Industry"
NEC LLM 13B,Language,"Language modeling/generation,Chat",,,https://jpn.nec.com/press/202307/20230706_02.html,,NEC、130億パラメータで世界トップクラスの日本語性能を有する軽量なLLMを開発,2023-07-06,NEC Laboratories,13000000000.0,13B,,"""NEC's LLM relieved this time was also trained using 512 GPUs installed on NEC's AI supercomputer""
from https://jpn.nec.com/rd/technologies/202308/index.html",,,,,,,,Confident,,United States of America,,,,512.0,,,Industry
Stable Diffusion XL (SDXL),Image generation,"Image generation,Text-to-image","Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, Robin Rombach",,https://arxiv.org/abs/2307.01952,1165.0,SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis,2023-07-04,Stability AI,3400000000.0,"""...result in a model size of 2.6B parameters in the UNet, see Tab. 1. The text encoders have a total size of 817M parameters.""",,,Unspecified unreleased,,,,,,,Speculative,"We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at this https URL","Multinational,United Kingdom of Great Britain and Northern Ireland",,,,,,,Industry
ERNIE 3.5,Language,"Language modeling,Language modeling/generation",,,http://research.baidu.com/Blog/index-view?id=185,,Introducing ERNIE 3.5: Baidu’s Knowledge-Enhanced Foundation Model Takes a Giant Leap Forward,2023-06-27,Baidu,,,,,,,,,,,,Unknown,,China,,,,,,,Industry
Inflection-1,Language,Language modeling,,Hosted access (no API),https://inflection.ai/assets/Inflection-1.pdf,,Inflection-1 technical memo,2023-06-23,Inflection AI,,,1.0001e+24,"<= 2.5e24

They define two ""compute classes"", one for models with more compute than PaLM 540B, i.e. GPT-4 and PaLM 2, and one for models with as much compute or less, i.e. GPT-3.5, Chinchilla, LLaMA, and Inflection-1.

PaLM 540B required 2.5e24 FLOP to train (confirmed by Google)",,"""Inflection-1 was trained using thousands of NVIDIA H100 GPUs on a very large dataset.""",,,,,NVIDIA H100 SXM5 80GB,Speculative,"Large language models (LLMs) based on the Transformer architecture have been shown to possess a range of advanced capabilities in language generation and understanding. These capabilities have paved the way for deployment of LLMs in products like OpenAI’s Chat-GPT and Google’s Bard. At Inflection AI, our mission is to create personal AIs for everyone, and in May 2023 we released Pi (pi.ai) – an LLM-based personal AI which is designed to be empathetic, useful, and safe. In this work we introduce the foundation model powering Pi, dubbed Inflection-1, and evaluate its performance characteristics across a variety of benchmarks.",United States of America,,,,,Unreleased,,Industry
MPT-30B,Language,"Language generation,Code generation",,Open weights (unrestricted),https://huggingface.co/mosaicml/mpt-30b,,,2023-06-22,MosaicML,30000000000.0,30B,1.8900000000001e+23,"According to their blog post,
""MPT-30B FLOPs ~= 6 * 30e9 [params] * 1.05e12 [tokens] = 1.89e23 FLOPs""","mC4,C4,RedPajama,The Stack",https://www.databricks.com/sites/default/files/inline-images/open-source-foundations-models-1.png,1050000000000.0,"~4T tokens across sources, but only trained on 1.05T of these",278.4,30B: 512x H100-80gb for 11.6 days,NVIDIA H100 SXM5 80GB,Confident,"MPT-30B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code. This model was trained by MosaicML.

MPT-30B is part of the family of Mosaic Pretrained Transformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.

MPT-30B comes with special features that differentiate it from other LLMs, including an 8k token context window (which can be further extended via finetuning; see MPT-7B-StoryWriter), support for context-length extrapolation via ALiBi, and efficient inference + training via FlashAttention. It also has strong coding abilities thanks to its pretraining mix. MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's FasterTransformer. The size of MPT-30B was also specifically chosen to make it easy to deploy on a single GPU—either 1xA100-80GB in 16-bit precision or 1xA100-40GB in 8-bit precision.",United States of America,,,,512.0,Open source,"apache 2.0 for weights.

pretrain code here: https://github.com/mosaicml/llm-foundry/tree/main/scripts/train/yamls/pretrain ",Industry
Wu Dao Aquila-7B,Language,"Chat,Code generation",,Open weights (restricted use),"https://spectrum.ieee.org/china-chatgpt-wu-dao

https://huggingface.co/BAAI/Aquila-7B

https://github.com/FlagAI-Open/FlagAI/blob/master/examples/Aquila/README_en.md",,,2023-06-10,Beijing Academy of Artificial Intelligence / BAAI,7000000000.0,,,,,,,,,,NVIDIA A100,Confident,"Who said all large-language models (LLMs) necessarily need to be large? In China’s case, LLMs are currently downsizing in their size and number of parameters. According to sources, this is because the country is now focusing on enabling Chinese startups and smaller entities to build their own generative AI applications. As part of this downscaling trend, in June the Beijing Academy of Artificial Intelligence (BAAI) introduced Wu Dao 3.0, a series of open-source LLMs.

Based on interviews with high-ranking, anonymous sources involved in the project, IEEE Spectrum can report that Wu Dao 3.0 builds on the academy’s work with Wu Dao 2.0, a sparse, multimodal generative AI model—as has been widely reported about version 2.0—with 1.75 trillion parameters. Although there is no single set of parameters for Wu Dao 3.0 (it’s a range of models with a variety of parameter counts) all are well below the 1.75 trillion high-water mark that version 2.0 set.",China,,,,,Open source,"Apache 2.0 for code: https://huggingface.co/BAAI/Aquila-7B
BAAI license for weights, commercial but restrictions around rights/PRC laws: https://huggingface.co/BAAI/Aquila-7B/resolve/main/BAAI%20Aquila%20Model%20License%20Agreement.pdf",Academia
Wu Dao Aquila-33B,Language,"Chat,Code generation",,Unreleased,"https://spectrum.ieee.org/china-chatgpt-wu-dao

https://huggingface.co/BAAI/Aquila-7B",,,2023-06-10,Beijing Academy of Artificial Intelligence / BAAI,33000000000.0,33B for largest model: https://huggingface.co/BAAI/Aquila-7B,,,,,,,,,NVIDIA A100,Confident,"Who said all large-language models (LLMs) necessarily need to be large? In China’s case, LLMs are currently downsizing in their size and number of parameters. According to sources, this is because the country is now focusing on enabling Chinese startups and smaller entities to build their own generative AI applications. As part of this downscaling trend, in June the Beijing Academy of Artificial Intelligence (BAAI) introduced Wu Dao 3.0, a series of open-source LLMs.

Based on interviews with high-ranking, anonymous sources involved in the project, IEEE Spectrum can report that Wu Dao 3.0 builds on the academy’s work with Wu Dao 2.0, a sparse, multimodal generative AI model—as has been widely reported about version 2.0—with 1.75 trillion parameters. Although there is no single set of parameters for Wu Dao 3.0 (it’s a range of models with a variety of parameter counts) all are well below the 1.75 trillion high-water mark that version 2.0 set.",China,,,,,,"""coming soon"" per https://huggingface.co/BAAI/Aquila-7B. This page was published a year ago so seems unlikely it will ever be released.",Academia
MusicGen,Audio,Audio generation,"Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Défossez",,https://arxiv.org/abs/2306.05284,240.0,Simple and Controllable Music Generation,2023-06-08,Meta AI,3359000000.0,"""We train autoregressive transformer models at different sizes: 300M, 1.5B, 3.3B parameters""

Uses EnCodec 32kHz (HF version has 59M params) for audio tokenization.",,"We train the 300M, 1.5B and 3.3B parameter models, using respectively 32, 64 and 96 GPUs, with mixed precision.

Unclear how many epochs used so FLOP calculation is not feasible.",ShutterStock and Pond5 music data collections,"""We use 20K hours of licensed music to train MUSICGEN. Specifically, we rely on an internal dataset of 10K high-quality music tracks, and on the ShutterStock and Pond5 music data collections with respectively 25K and 365K instrument-only music tracks. All datasets consist of full-length music sampled at 32 kHz with metadata composed of a textual description and additional information such as the genre, BPM, and tags.""",,"""We train on 30-second audio crops sampled at random from the full track... We use 20K hours of licensed music""

20000 hours * 60 min/hour * 2 inputs/min = 2400000 input sequences

EnCodec is run at 32kHz but after convolutions has a frame rate of 50 Hz, suggesting 2400000 * 30s * 50/s = 3,600,000,000 audio tokens.

Not confident enough in this calculation to add to database.",,,,Likely,"We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at this https URL.",United States of America,,,,,,,Industry
PolySphere-1,Language,"Chat,Language modeling/generation",AI Inside,,"https://inside.ai/en/news/2023/06/08/aiinside-xresearch/
https://inside.ai/en/news/2023/06/29/polysphere-token/",,"AI inside Establishes “XResearch” for R&D and Social Implementation of Generative AI and LLM, Providing the Alpha Version of a 14 Billion-Parameter Japanese LLM Service",2023-06-08,AI inside,14000000000.0,"14B from https://inside.ai/en/news/2023/06/08/aiinside-xresearch/
",,,,,,,,,,Likely,,Japan,,,,,,,Industry
PaLM 2,Language,"Language modeling,Language modeling/generation","Andrew M. Dai, David R. So, Dmitry Lepikhin, Jonathan H. Clark, Maxim Krikun, Melvin Johnson, Nan Du, Rohan Anil, Siamak Shakeri, Xavier Garcia, Yanping Huang, Yi Tay, Yong Cheng, Yonghui Wu, Yuanzhong Xu, Yujing Zhang, Zachary Nado, Bryan Richter, Alex Polozov, Andrew Nystrom, Fangxiaoyu Feng, Hanzhao Lin, Jacob Austin, Jacob Devlin, Kefan Xiao, Orhan Firat, Parker Riley, Steven Zheng, Yuhuai Wu, Zhongtao Liu, Jiahui Yu, Guy Gur-Ari, Weikang Zhou, Sneha Kudugunta, Sunipa Dev, Frederick Liu, Gustavo Hernandez Abrego, Kelvin Xu, Abe Ittycheriah, Daniel Sohn, John Nham, Le Hou, Siyuan Qiao, Pidong Wang, Zirui Wang, Laurent El Shafey, Hyeontaek Lim, Marcello Maggioni, Michael Isard, Paul Barham, Qiao Zhang, Tao Wang, Yash Katariya, Aurko Roy, Benjamin Lee, Brennan Saeta, Ce Zheng, Hadi Hashemi, Junwhan Ahn, Rajkumar Samuel, Steven Hand, Zhifeng Chen, Kiran Vodrahalli, Aakanksha Chowdhery, Ethan Dyer, Emanuel Taropa, Vlad Feinberg, James Bradbury, Reiner Pope, Wei Li, YaGuang Li, Eric Chu, Jeffrey Hui, Joshua Howland, Vlad Fienber, Aroma Mahendru, Michele Catasta, Vedant Misra, Kevin Robinson, Maysam Moussalem, Sebastian Ruder, Erica Moreira, Eric Ni, Paige Bailey, Lucas Gonzalez, Alexandre Passos, Slav Petrov, Gaurav Mishra, Mark Omernick, Ambrose Slone, Andrea Hu, Colin Cherry, Denny Zhou, Jan Botha, John Wieting, Joshua Maynez, Kathleen Kenealy, Kevin Brooks, Linting Xue, Markus Freitag, Martin Polacek, Pengcheng Yin, Sebastian Gehrmann, Xuezhi Wang, Kathy Meier-Hellstern, Christopher A. Choquette-Choo, Daniel Smilkov, Emily Reif, Alicia Parrish, Alex Castro Ros, Clément Crepy, Dasha Valter, Jeremy Hurwitz, Katherine Lee, Mark Díaz, Marie Pellat, Matthew Jagielski, Renee Shelby, Shachi Dave",API access,https://arxiv.org/abs/2305.10403,1734.0,PaLM 2 Technical Report,2023-05-10,Google,340000000000.0,"Model Architecture: ""PaLM-2 is a new state-of-the-art language model. We have small, medium, and large variants that use stacked layers based on the Transformer architecture, with varying parameters depending on model size. Further details of model size and architecture are withheld from external publication.""
However, the parameter count was leaked to CNBC: https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html",7.34e+24,"Compute Requirements ""Not reported.""
Paper suggests heuristic of  C=6ND. Based on 340B parameters and 3.6T tokens, training compute would be around 7.3*10^24 FLOP.",,"""The PaLM 2 pre-training corpus is composed of a diverse set of sources: web documents, books, code, mathematics, and conversational data. The pre-training corpus is significantly larger than the corpus used to train PaLM (Chowdhery et al., 2022). PaLM 2 is trained on a dataset that includes a higher percentage of non-English data than previous large language models, which is beneficial for multilingual tasks"" (page 9)",2700000000000.0,"""The pre-training corpus is significantly larger than the corpus used to train PaLM"" so greater than 6e+11. According to the leaked documents viewed by CNBC, the corpus was 3.6 trillion tokens or around 2.7*10^12 words.

https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.html",,,Google TPU v4,Likely,"We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM (Chowdhery et al., 2022). PaLM 2 is a Transformer-based model trained using a mixture of objectives similar to UL2 (Tay et al., 2023). Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities.",United States of America,,,,,Unreleased,,Industry
CodeGen2,Language,Code generation,"Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, Yingbo Zhou",Open weights (unrestricted),https://arxiv.org/abs/2305.02309,132.0,CodeGen2: Lessons for Training LLMs on Programming and Natural Languages,2023-05-03,Salesforce,16000000000.0,16B for largest CodeGen2 model,,,Stack v1.1,"""We examine our recipe on four model sizes: 1B, 3.7B, 7B, and 16B, and
refer to them as CodeGen2.
1 For training a subset of the Stack v1.1 (Kocetkov et al., 2022), filtered
with a stronger permissive license guideline, is used""",,,,,,Confident,"Large language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly.
In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a ""free lunch"" hypothesis. For data distributions, the effect of a mixture distribution and multi-epoch training of programming and natural languages on model performance is explored.
We conduct a comprehensive series of empirical experiments on 1B LLMs, for which failures and successes of this exploration are distilled into five lessons. We will provide a final recipe for training and release CodeGen2 models in size 1B, 3.7B, 7B, and, 16B parameters, along with the training framework as open-source: this https URL.",United States of America,,,,,,apache 2.0,Industry
ruGPT-3.5 13B,Language,"Chat,Language modeling/generation",,Open weights (unrestricted),https://huggingface.co/ai-forever/ruGPT-3.5-13B,,ruGPT-3.5 13B,2023-04-24,Sber,13000000000.0,13B,1.0699776e+23,"""Model was trained using Deepspeed and Megatron libraries, on 300B tokens dataset for 3 epochs, around 45 days on 512 V100. After that model was finetuned 1 epoch with sequence length 2048 around 20 days on 200 GPU A100 on additional data""

512 GPUs * 125000000000000 FLOPs/s [peak] * 45 days * 24 hours * 3600 s * 0.3 + 200 GPUs * 312000000000000 FLOPs/s [peak for fp16] * 20 days * 24 hours * 3600 s * 0.3 = 1.0699776e+23

they probably used fp16 as in their similar project: https://habr.com/ru/companies/sberdevices/articles/780334/

6ND = 6*13B*300B*3 = 70200*10^18 = 7*10^24",,,300000000000.0,,1080.0,,"NVIDIA A100,NVIDIA Tesla V100 SXM2",Confident,,Russia,,,,512.0,Unreleased,"MIT license
https://huggingface.co/ai-forever/ruGPT-3.5-13B/discussions","Industry,Government"
Claude 1.3,Language,"Language modeling,Chat",,,https://twitter.com/AnthropicAI/status/1648353600350060545?lang=en,,,2023-04-18,Anthropic,,,,,Unspecified unreleased,,,,,,,Unknown,,United States of America,,,,,,,Industry
Anthropic LM 52B,Language,"Language modeling/generation,Question answering","Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, Jared Kaplan",Unreleased,https://arxiv.org/abs/2204.05862,1695.0,Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback,2023-04-12,Anthropic,52000000000.0,,,,,"The 52B preference model was trained on a mixture of helpfulness and harmlessness (red teaming) datasets collected by Anthropic using crowdworkers conversing with language models in a feedback interface.
The helpfulness dataset contains around 44k comparisons and the harmlessness dataset contains around 42k comparisons. The data consists of multi-turn dialogues where crowdworkers choose the more helpful or more harmful response at each turn.
The model was trained using a technique called preference model pretraining on additional datasets before finetuning on Anthropic's human feedback data.",,,,,,Confident,"We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.",United States of America,,,,,Unreleased,,Industry
SenseChat,Language,Chat,,,https://www.sensetime.com/en/news-detail/51166397?categoryId=1072,,"SenseTime Launches “SenseNova” Foundation Model Sets and AI Computing Systems, Advancing AGI Development",2023-04-10,SenseTime,180000000000.0,"https://www.thepaper.cn/newsDetail_forward_22639611
Translation:
""SenseTime launched the ""SenseNova"" large model system, which includes natural language generation, image generation services, pre-labeling for perception models, and model development. The ""SenseChat"" application platform, powered by a 180-billion parameter Chinese language model, supports ultra-long text comprehension and offers capabilities such as question answering, understanding, and generation in Chinese.""

Link says ""hundreds of billions"" but the more precise number above seems more credible.",3.89e+24,"“Over the course of five years, SenseTime has built SenseCore, a leading AI infrastructure with 27,000 GPUs, capable of delivering a total computational power of 5,000 petaflops”

Assuming they used this entire cluster with 30 days of training (rough average of frontier model training times since 2016), 30% utilization rate: 5000e15 * 0.3 * 30 * 24 * 60 * 60 = 3.89e24 FLOP.

Assuming the model is dense and trained Chinchilla-optimal: 20 tokens/parameter * (180e9 parameters)**2 * 6 = 3.89e24 FLOP. (The two estimates match by coincidence.)

The model seems more likely than not to be dense, given that news of SenseChat 5.0 makes a point of stating its MoE architecture, whereas SenseChat 1.0 does not mention architecture.

Given uncertainties (e.g. the model is possibly MoE, could have been overtrained or undertrained, could have trained longer or shorter), likely between 1e23 and 3e25 FLOP.",,,,,,,,Speculative,"SenseTime hosted a Tech Day event, sharing their strategic plan for advancing AGI (Artificial General Intelligence) development through the combination of “foundation models + large-scale computing” systems. Under this strategy, SenseTime unveiled the “SenseNova” foundation model set, introducing a variety of foundation models and capabilities in natural language processing, content generation, automated data annotation, and custom model training. At the event, SenseTime not only showcased their large language model’s capabilities, but also demonstrated a series of generative AI models and applications, such as text-to-image creation, 2D/3D digital human generation, and complex scenario/detailed object generation. Additionally, they introduced their AGI research and development platform facilitated by the integration of “foundation models + large-scale computing” systems.","Hong Kong,China",,,,,,,Industry
BloombergGPT,Language,Language modeling,"Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, Gideon Mann",Unreleased,https://arxiv.org/abs/2303.17564,556.0,BloombergGPT: A Large Language Model for Finance,2023-03-30,"Bloomberg,Johns Hopkins University",50558868480.0,,2.36e+23,"2.36e23 per Table 4

(using our usual hardware method, 512 A100s over 53 days would be 512 * 312 teraFLOP/s * 53 * 24 * 3600 * 0.3 = 2.19e23)",,"""To train BloombergGPT, we construct “FinPile”, a comprehensive dataset consisting of a range of English financial documents including news, filings, press releases, web-scraped financial documents, and social media drawn from the Bloomberg archives. These documents have been acquired through our business process over the past two decades. We augment FinPile with public data widely used to train LLMs. The result is a training corpus that is roughly half domain-specific text and half general-purpose text.""",532000000000.0,"708.9 billion tokens. At 0.75 English words per token, that's 532B words",1270.0,"""~53 days""",NVIDIA A100,Confident,"The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general LLM benchmarks. Additionally, we explain our modeling choices, training process, and evaluation methodology. We release Training Chronicles (Appendix C) detailing our experience in training BloombergGPT.","United States of America,United States of America",,,,512.0,Unreleased,,"Industry,Academia"
LightOn Mini,Language,"Language modeling/generation,Chat",,Hosted access (no API),https://www.lighton.ai/blog/lighton-s-blog-4/lighton-s-large-language-model-of-40-billion-parameters-mini-19,,LightOn's Large Language Model of 40 billion parameters: MINI,2023-03-21,LightOn,40000000000.0,"""Boasting an impressive 40 billion parameters, Mini is a formidable addition to the growing array of language models available in the market today.""",2.4e+23,6ND aproximation: 6*40B*1T = 2.4e23,,"""The amount of data in Mini corpus is 1 trillion tokens. We mainly used data from the public web to pre-train our model, with strong filtering, toxicity reduction, and deduplication to ensure that only high-quality data is retained.""",1000000000000.0,"""The amount of data in Mini corpus is 1 trillion tokens. We mainly used data from the public web to pre-train our model, with strong filtering, toxicity reduction, and deduplication to ensure that only high-quality data is retained.""  assuming 0.75 words per token - 750000000000.0 words",,,,Confident,,France,,,,,Unreleased,,Industry
Firefly,Image generation,Image generation,,,https://news.adobe.com/news/news-details/2023/Adobe-Unveils-Firefly-a-Family-of-new-Creative-Generative-AI/default.aspx,,"Adobe Unveils Firefly, a Family of new Creative Generative AI",2023-03-21,Adobe,,,,,Adobe Stock,"""The current Firefly generative AI model is trained on a dataset of licensed content, such as Adobe Stock, and public domain content where copyright has expired.""

https://www.adobe.com/products/firefly.html",,,,,,Unknown,"Today, Adobe (Nasdaq:ADBE) introduced Adobe Firefly, a new family of creative generative AI models, first focused on the generation of images and text effects. Adobe Firefly will bring even more precision, power, speed and ease directly into Creative Cloud, Document Cloud, Experience Cloud and Adobe Express workflows where content is created and modified. Adobe Firefly will be part of a series of new Adobe Sensei generative AI services across Adobe’s clouds.",United States of America,,,,,,,Industry
Gen-2,Video,Video generation,Gen-2 authors,,https://research.runwayml.com/gen2,,,2023-03-20,Runway,,,,,,,,,,,,Unknown,,United States of America,,,,,,,Industry
PanGu-Σ,Language,"Code generation,Language modeling,Translation,Question answering","Xiaozhe Ren, Pingyi Zhou, Xinfan Meng, Xinjing Huang, Yadao Wang, Weichao Wang, Pengfei Li, Xiaoda Zhang, Alexander Podolskiy, Grigory Arshinov, Andrey Bout, Irina Piontkovskaya, Jiansheng Wei, Xin Jiang, Teng Su, Qun Liu, Jun Yao",Unreleased,https://arxiv.org/abs/2303.10845,48.0,PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing,2023-03-20,Huawei Noah's Ark Lab,1085000000000.0,"""In this work, we present PanGu-Σ , a large language model with sparse architecture containing 1.085 trillion parameters.""",4.67e+23,"It has sparse architecture, so we can't use C=6ND.
""We develop PanGu-Σ model under the framework of MindSpore and train it on a cluster with only 512 Ascend 910 AI Accelerators with 329 billion tokens over 100 days.""
100 days * 512 processors * 320 teraFLOPS/processor * 33% utilization = 4.67e+23 FLOP
https://www.wolframalpha.com/input?i=100+days+*+512+*+320+terahertz+*+0.33",,"""329B tokens in more than 40 natural and programming languages""",246750000000.0,329B tokens ~= 247B words,2400.0,"We develop PanGu-Σ model under the framework of MindSpore 5
and train it on a cluster with only 512 Ascend 910 AI Accelerators [28] with 329 billion tokens over 100 days.",Huawei Ascend 910,Confident,"The scaling of large language models has greatly improved natural language understanding, generation, and reasoning. In this work, we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 AI processors and MindSpore framework, and present the language model with 1.085T parameters named PanGu-{\Sigma}. With parameter inherent from PanGu-{\alpha}, we extend the dense Transformer model to sparse one with Random Routed Experts (RRE), and efficiently train the model over 329B tokens by using Expert Computation and Storage Separation(ECSS). This resulted in a 6.3x increase in training throughput through heterogeneous computing. Our experimental findings show that PanGu-{\Sigma} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks. Moreover, it demonstrates strong abilities when fine-tuned in application data of open-domain dialogue, question answering, machine translation and code generation.",China,,,,512.0,Unreleased,,Industry
GPT-4,"Multimodal,Language,Vision","Language modeling,Language modeling/generation,Question answering,Visual question answering","OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain et al. (181 additional authors not shown)",API access,https://arxiv.org/abs/2303.08774,8281.0,GPT-4 Technical Report,2023-03-15,OpenAI,,,2.1e+25,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",Unspecified unreleased,,4900000000000.0,"Speculative. Reported secondhand by online sources such as Semianalysis, but not verified by OpenAI. If total number of tokens seen was 13T, text was repeated for 2 epochs, and text was the majority of tokens, then dataset size roughly is 13T*0.75/2 = 4.9T words.

Note this examines only the text dataset, since GPT-4 was first and foremost a language model. However, the vision component had its own vision dataset, which we believe accounted for a much smaller part of the compute budget.",2280.0,(Speculative) SemiAnalysis conjectures that GPT-4 training took 90-100 days with utilization of 32-36%.,NVIDIA A100 SXM4 40 GB,Speculative,"We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.",United States of America,,,,25000.0,Unreleased,,Industry
Falcon-40B,Language,Language modeling,,Open weights (unrestricted),https://arxiv.org/abs/2311.16867; https://www.tii.ae/news/abu-dhabi-based-technology-innovation-institute-introduces-falcon-llm-foundational-large,0.0,Abu Dhabi-based Technology Innovation Institute Introduces Falcon LLM: Foundational Large Language Model (LLM) outperforms GPT-3 with 40 Billion Parameters,2023-03-15,Technology Innovation Institute,40000000000.0,Model comes in 7B and 40B variants.,2.4e+23,"C = 6ND = 6 * 40B * 1000B = 2.4e+23 FLOP (assuming one epoch)

Table 1 from https://arxiv.org/pdf/2311.16867 Falcon paper

2,800 petaflop-days * 1e15 * 24 * 3600 = 2.4192e+23 FLOPs",RefinedWeb,"Falcon-40B was trained on 1,000B tokens of RefinedWeb, a high-quality filtered and deduplicated web dataset which we enhanced with curated corpora. Significant components from our curated copora were inspired by The Pile (Gao et al., 2020).",1000000000000.0,1000B tokens ~= 750B words,1440.0,"""Falcon-40B was trained on AWS SageMaker, on 384 A100 40GB GPUs in P4d instances.""
""Training started in December 2022 and took two months.""",NVIDIA A100,Confident,,United Arab Emirates,,,,384.0,Unreleased,apache 2.0,Government
Claude,Language,"Language modeling,Chat",,,https://www.anthropic.com/index/introducing-claude,,Introducing Claude,2023-03-14,Anthropic,,,,,Unspecified unreleased,,,,,,,Unknown,"Claude is a next-generation AI assistant based on Anthropic’s research into training helpful, honest, and harmless AI systems. Accessible through chat interface and API in our developer console, Claude is capable of a wide variety of conversational and text processing tasks while maintaining a high degree of reliability and predictability.",United States of America,,,,,,,Industry
Cohere Command,Language,Language modeling/generation,,API access,https://cohere.com/models/command,,"World-class AI, at your command",2023-03-01,Cohere,52000000000.0,"52B for larger version

https://aws.amazon.com/bedrock/cohere-command-embed/

Cohere Command has had a few different sizes over time and is continuously updated, but there's been a 52B version since at least March 2023: https://twitter.com/percyliang/status/1638236921754443776",,,,,,,,"https://docs.cohere.com/docs/environmental-impact

2696.16 kg carbon for base-light and 6689.76 kg carbon for embed-english. Nothing listed for the large model. 

It's possible to back out GPU-hours using this calculator, though it varies by region and Cohere doesn't specify the region.

https://mlco2.github.io/impact/",Google TPU v4,Speculative,,Canada,,,,,Unreleased,,Industry
Jurassic-2 Jumbo,Language,"Language modeling,Translation",,,https://www.ai21.com/blog/introducing-j2,,Announcing Jurassic-2 and Task-Specific APIs,2023-03-01,AI21 Labs,178000000000.0,Source is https://crfm.stanford.edu/helm/latest/#/leaderboard as viewed on 2023-12-06,,,,,,,,,,,"Announcing the launch of Jurassic-2, the latest generation of AI21 Studio’s foundation models, a game-changer in the field of AI, with top-tier quality and new capabilities. And that's not all - we're also releasing our task-specific APIs, with plug-and-play reading and writing capabilities that outperform competitors.",Israel,,,,,,,Industry
CodeGen-Mono 16.1B,Language,"Code generation,Code autocompletion","Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong",Open weights (unrestricted),https://arxiv.org/abs/2203.13474,764.0,CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis,2023-02-27,Salesforce,16100000000.0,16.1B parameters,,,"The Pile,Big Query,BigPython","""The family of CODEGEN models is trained sequentially on three datasets: The Pile, BigQuery, and BigPython.""",568200000000.0,Table 5,,,Google TPU v4,Likely,"""Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: this https URL.""",United States of America,,,,,,apache 2.0,Industry
LLaMA-33B,Language,"Language modeling,Code generation","Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",Open weights (non-commercial),https://arxiv.org/abs/2302.13971,8872.0,LLaMA: Open and Efficient Foundation Language Models,2023-02-27,Meta AI,32500000000.0,Table 2 in the paper,2.7300000000001e+23,1.4T tokens * 32.5B params * 6 FLOP/token/param = 2.73e+23 FLOP,"CCNet,GitHub,Wikipedia,books,arXiv,Stack Exchange",See Table 1,1340000000000.0,"Table 1 indicates that 1.4T tokens involved sampling sub-datasets at more or less than one epoch. Correcting for this:

(1.1 epoch * 3.3TB) + (1.06 epoch * 0.783TB) + ... = 1.4T tokens
5.24 epoch-TBs = 1.4T tokens
5.24 epoch-TB * 1000 GB/TB * 200M token/GB = 1.4T tokens
1.05T epoch*token = 1.4T tokens
1 epoch = 1.34T tokens",,,,Confident,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",United States of America,,,,,Unreleased,"""we are releasing our model under a noncommercial license focused on research use cases"" https://ai.meta.com/blog/large-language-model-llama-meta-ai/",Industry
LLaMA-65B,Language,"Language modeling,Code generation","Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",Open weights (non-commercial),https://arxiv.org/abs/2302.13971,8872.0,LLaMA: Open and Efficient Foundation Language Models,2023-02-24,Meta AI,65200000000.0,"Model card, table 1: https://github.com/facebookresearch/llama/blob/53011c3d7946dadb8274a4c5c7586ab54edf792d/MODEL_CARD.md",5.5e+23,"1.4e12 tokens * 6.52e10 parameters * 6 FLOP/token/parameter = 5.5e23 FLOP

Compared to 2048 A100 GPUs each with 311.84 TFLOPS maximum performance for 21 days, this implies 47% utilization.
https://www.wolframalpha.com/input?i=5.5*10%5E23+FLOP+%2F+%282048+*+311.84+teraFLOPS+*+21+days%29","CCNet,GitHub,Wikipedia,books,arXiv,Stack Exchange","The model was trained using the following source of data: CCNet [67%], C4 [15%], GitHub [4.5%], Wikipedia [4.5%], Books [4.5%], ArXiv [2.5%], Stack Exchange[2%]. The Wikipedia and Books domains include data in the following languages: bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk. See the paper for more details about the training set and corresponding preprocessing.",1340000000000.0,"Table 1 indicates that 1.4T tokens involved sampling sub-datasets at more or less than one epoch. Correcting for this:

(1.1 epoch * 3.3TB) + (1.06 epoch * 0.783TB) + ... = 1.4T tokens
5.24 epoch-TBs = 1.4T tokens
5.24 epoch-TB * 1000 GB/TB * 200M token/GB = 1.4T tokens
1.05T epoch*token = 1.4T tokens
1 epoch = 1.34T tokens
",500.0,"""When training a 65B-parameter model, our code processes around 380 tokens/sec/GPU on 2048 A100 GPU with 80GB of RAM. This means that training over our dataset containing 1.4T tokens takes approximately 21 days.""",NVIDIA A100,Confident,"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",United States of America,,,,2048.0,Unreleased,"""we are releasing our model under a noncommercial license focused on research use cases"" https://ai.meta.com/blog/large-language-model-llama-meta-ai/",Industry
Anthropic LM 175B,Language,"Language modeling/generation,Question answering","Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I. Liao, Kamilė Lukošiūtė, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, Dawn Drain, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau, Kamal Ndousse, Karina Nguyen, Liane Lovitt, Michael Sellitto, Nelson Elhage, Noemi Mercado, Nova DasSarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kadavath, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, Christopher Olah, Jack Clark, Samuel R. Bowman, Jared Kaplan",Unreleased,https://arxiv.org/abs/2302.07459,132.0,The Capacity for Moral Self-Correction in Large Language Models,2023-02-15,Anthropic,175000000000.0,175B,,,,,,,,,,Confident,"We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to ""morally self-correct"" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.",United States of America,,,,,Unreleased,,Industry
ViT-22B,Vision,"Object detection,Image classification","Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Patrick Collier, Alexey Gritsenko, Vighnesh Birodkar, Cristina Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetić, Dustin Tran, Thomas Kipf, Mario Lučić, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, Neil Houlsby",Unreleased,https://arxiv.org/abs/2302.05442v1,428.0,Scaling Vision Transformers to 22 Billion Parameters,2023-02-10,Google,21743000000.0,"21.743B, Table 1",1.93248e+23,"""ViT-22B was trained using 256 visual tokens per image, where each token represents a 14 × 14 patch extracted from 224 × 224 sized images. ViT-22B is trained for 177k steps with batch size of 65k: approximately 3 epochs""

""ViT-22B was trained on 1024 TPU V4 chips for 177K steps""

""Using these techniques, ViT-22B processes 1.15k tokens per second per core during training (forward and backward pass) on TPUv4 [...] ViT-22B’s model flops utilization (MFU) is 54.9%""

256 * 177k * 65k = 2.945T tokens

So training time is 2.945T tokens / (1.15k * 2 * 1024) tokens/s = 1.25M seconds = 347.4 hours

So 1024 TPUv4 chips for 1.25M seconds at 54.9% MFU:
1024 * 2.75e14 * 1.25M * 0.549 = 1.93248e23",JFT-4B,"""Dataset. ViT-22B is trained on a version of JFT (Sun et al., 2017), extended to around 4B images (Zhai et al.,
2022a). These images have been semi-automatically annotated with a class-hierarchy of 30k labels""",4000000000.0,"""Dataset. ViT-22B is trained on a version of JFT (Sun et al., 2017), extended to around 4B images (Zhai et al.,
2022a). These images have been semi-automatically annotated with a class-hierarchy of 30k labels""",347.4,"""Using these techniques, ViT-22B processes 1.15k tokens per second per core during training (forward and backward pass)""
From model card we know they trained with 1024 TPUv4 chips, and there are 2 cores per chip. Total number of tokens was 177K steps * 65k images/step * 256 tokens/image = 2.945T tokens

So training time is 2.945T tokens / (1.15k * 2 * 1024) tokens/s = 1.25M seconds = 347.4 hours",Google TPU v4,Confident,"The scaling of Transformers has driven breakthrough capabilities for language models. At present, the largest large language models (LLMs) contain upwards of 100B parameters. Vision Transformers (ViT) have introduced the same architecture to image and video modelling, but these have not yet been successfully scaled to nearly the same degree; the largest dense ViT contains 4B parameters (Chen et al., 2022). We present a recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of experiments on the resulting model. When evaluated on downstream tasks (often with a lightweight linear model on frozen features), ViT-22B demonstrates increasing performance with scale. We further observe other interesting benefits of scale, including an improved tradeoff between fairness and performance, state-of-the-art alignment to human visual perception in terms of shape/texture bias, and improved robustness. ViT-22B demonstrates the potential for ""LLM-like"" scaling in vision, and provides key steps towards getting there.",United States of America,,,,1024.0,Unreleased,don't see it here: https://github.com/google-research/vision_transformer?tab=readme-ov-file#available-vit-models ,Industry
Gen-1,Video,Video generation,"Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, Anastasis Germanidis",,https://arxiv.org/abs/2302.03011,377.0,Structure and Content-Guided Video Synthesis with Diffusion Models,2023-02-06,Runway,,,,,,,,,,,,Unknown,,United States of America,,,,,,,Industry
Whisper v2,Speech,Speech recognition,"Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever",Open weights (unrestricted),"https://huggingface.co/openai/whisper-large-v2

https://arxiv.org/abs/2212.04356",2240.0,Robust Speech Recognition via Large-Scale Weak Supervision,2022-12-05,OpenAI,1550000000.0,1550M,1.1e+23,"""Compared to the Whisper large model, the large-v2 model is trained for 2.5x more epochs with added regularization for improved performance.""

We (roughly) estimated Whisper v1 as 4.65e22. 2.5x that is 1.16e23 or ~1.1e23",Unspecified unreleased,"""The models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages.""",9302400000.0,"""When scaled to 680,000 hours of multilingual and multitask
supervision, the resulting models generalize well
to standard benchmarks and are often competitive
with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning.""


13,680 words/h (estimate) * 680,000h = 9,302,400,000 words",,,,Likely,"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning.

Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision by Alec Radford et al. from OpenAI. The original code repository can be found here.

Compared to the Whisper large model, the large-v2 model is trained for 2.5x more epochs with added regularization for improved performance.",United States of America,,,,,Unreleased,"Apache 2.0 for weights

code for v1 is MIT: https://github.com/openai/whisper",Industry
GPT-3.5 Turbo,Language,"Language modeling/generation,Question answering,Chat","John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, Rapha Gontijo Lopes, Shengjia Zhao, Arun Vijayvergiya, Eric Sigler, Adam Perelman, Chelsea Voss, Mike Heaton, Joel Parish, Dave Cummings, Rajeev Nayak, Valerie Balcom, David Schnurr, Tomer Kaftan, Chris Hallacy, Nicholas Turley, Noah Deutsch, Vik Goel, Jonathan Ward, Aris Konstantinidis, Wojciech Zaremba, Long Ouyang, Leonard Bogdonoff, Joshua Gross, David Medina, Sarah Yoo, Teddy Lee, Ryan Lowe, Dan Mossing, Joost Huizinga, Roger Jiang, Carroll Wainwright, Diogo Almeida, Steph Lin, Marvin Zhang, Kai Xiao, Katarina Slama, Steven Bills, Alex Gray, Jan Leike, Jakub Pachocki, Phil Tillet, Shantanu Jain, Greg Brockman, Nick Ryder, Alex Paino, Qiming Yuan, Clemens Winter, Ben Wang, Mo Bavarian, Igor Babuschkin, Szymon Sidor, Ingmar Kanitscheider, Mikhail Pavlov, Matthias Plappert, Nik Tezak, Heewoo Jun, William Zhuk, Vitchyr Pong, Lukasz Kaiser, Jerry Tworek, Andrew Carr, Lilian Weng, Sandhini Agarwal, Karl Cobbe, Vineet Kosaraju, Alethea Power, Stanislas Polu, Jesse Han, Raul Puri, Shawn Jain, Benjamin Chess, Christian Gibson, Oleg Boiko, Emy Parparita, Amin Tootoonchian, Kyle Kosic, Christopher Hesse",API access,https://platform.openai.com/docs/models/gpt-3.5-turbo,,"A fast, inexpensive model for simple tasks",2022-11-30,OpenAI,20000000000.0,20B parameters according to Table 1 in Microsoft's CODEFUSION paper: https://arxiv.org/pdf/2310.17680.pdf,,,Unspecified unreleased,,,,,,,Speculative,"GPT-3.5 Turbo models can understand and generate natural language or code and have been optimized for chat using the Chat Completions API but work well for non-chat tasks as well. As of July 2024, use gpt-4o-mini in place of GPT-3.5 Turbo, as it is cheaper, more capable, multimodal, and just as fast. GPT-3.5 Turbo is still available for use in the API.",United States of America,,,,,Unreleased,available on API: https://platform.openai.com/docs/models/gpt-3-5-turbo ,Industry
GPT-3.5,Language,Language modeling,,API access,https://platform.openai.com/docs/models/gpt-3-5,,,2022-11-28,OpenAI,,"Parameter count may be 175B based on OpenAI's statements that text-davinci-003 is in the GPT-3.5 series of models. It was also stated to be 175B in the Microsoft CODEFUSION paper, but the paper was reportedly retracted because the authors did not know the parameter count.",2.578e+24,https://colab.research.google.com/drive/1QSxa8YCWjEBQU7mrXLhw6TP1VX5oqgdW#scrollTo=Gt6Z6oZ26clI,,,,,,,NVIDIA A100 SXM4 40 GB,Speculative,,United States of America,,,,,Unreleased,,Industry
Galactica,"Language,Biology","Language modeling,Question answering","Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic",Open weights (non-commercial),https://arxiv.org/abs/2211.09085,599.0,Galactica: A Large Language Model for Science,2022-11-16,Meta AI,120000000000.0,"""The largest 120B model we train runs on a single NVIDIA A100 node""",3.24e+23,"Authors state the model is trained on 450b tokens. Using 6 FLOP/token/parameter, this is 6*120b*450b = 3.24e23",Galactica Corpus,"""Our corpus consists of 106 billion tokens from papers, reference material, encyclopedias and other scientific sources. We combine natural language sources, such as papers and textbooks, and natural sequences, such as protein sequences and chemical formulae. We process LATEX where we can capture it, and also include academic code to capture computational science""",106000000000.0,"""Total dataset size = 106 billion tokens""",,,NVIDIA A100 SXM4 80 GB,Likely,"Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.",United States of America,,,,128.0,Unreleased,"cc-by-nc (non-commercial): https://huggingface.co/facebook/galactica-120b 

repo but no training code: https://github.com/paperswithcode/galai/blob/main/README.md ",Industry
Make-A-Video,Video,Video generation,"Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, Yaniv Taigman",,https://arxiv.org/abs/2209.14792,962.0,Make-A-Video: Text-to-Video Generation without Text-Video Data,2022-09-29,Meta AI,,,,,"LAION,WebVid-10M,HD-VILA-100M",,,,,,,Unknown,"We propose Make-A-Video -- an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. Make-A-Video has three advantages: (1) it accelerates training of the T2V model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today's image generation models. We design a simple yet effective way to build on T2I models with novel and effective spatial-temporal modules. First, we decompose the full temporal U-Net and attention tensors and approximate them in space and time. Second, we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models that can enable various applications besides T2V. In all aspects, spatial and temporal resolution, faithfulness to text, and quality, Make-A-Video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures.",United States of America,,,,,,,Industry
PaLI,"Language,Vision,Multimodal","Visual question answering,Language modeling/generation","Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut",Unreleased,https://arxiv.org/abs/2209.06794v4,567.0,PaLI: A Jointly-Scaled Multilingual Language-Image Model,2022-09-14,Google,16900000000.0,"3.9b Image Encoder, 
14b Multimodal Encoder-Decoder",1.69e+23,"Pre-training the ViT component involved 1.1 million steps (they train over 1M steps but run the last 100k twice and then average the two resulting models). Batch size is 16384 and the inputs are 224x224. Table 8 indicates a forward pass with ViT-e/14 on a 224 image takes 1980 GFLOPs, so total training compute for the ViT-e/14 model is:
1980e9 * 16384 * 1.1 million * 3 (account for backward passes) = 1.07e23

In the ""overal model"" section, they then say: ""The largest model, PaLI-17B, is pretrained using 1,024 GCP-TPUv4 chips for 7 days"". It is then trained for another 3 days on 512 chips at higher resolution. 

I assume the stated TPUv4 training does not include the ViT pretraining, since it amounts to fewer FLOPs than we estimate above for the ViT.

275 teraFLOP/s * ((1024 * 7) + (512 * 3)) * 24 * 3600 * 0.3 (utilization assumption) = 6.2e22

Total: 1.07e23 + 6.2e22 = 1.69e23",WebLI,"""we introduce WebLI, a multilingual imagelanguage dataset built from images and texts available on the public web... Due to the abundance of multilingual content on the internet, the collection process for the WebLI dataset can be scaled to cover 10 billion images and 12 billion alt-texts. In addition to annotation with web text, we use publicly available automatic service to extract OCR annotations on all images, resulting in 29 billion image-OCR pairs. To balance quality and retain scale, we filter the dataset to the highest quality subset retaining only the top 10% scoring of the original WebLI image-text pairs (about 1B examples), which we use to train PaLI""",1600000000.0,"""During training, the model passes over 1.6B images, one epoch over the entire pretraining dataset""",240.0,10,Google TPU v4,Likely,"Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI (Pathways Language and Image model), a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pre-trained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.",United States of America,,,,1024.0,Unreleased,,Industry
Luminous-supreme,Language,Language generation,,API access,https://docs.aleph-alpha.com/docs/introduction/model-card/,,Model Card Luminous,2022-08-15,Aleph Alpha,70000000000.0,"""~70B""",3.5461e+23,"""~839000h"" GPU-hours on A100s, per Environmental Impact section of model card.

312 trillion * 839000 * 3600 * 0.3 = 2.8e23

6ND = 6*70B*1069.30B = 4.49106e+23

sqrt(2.8e23*4.49106e+23) = 3.54612... × 10^23

reported here:
167TFLOPS
https://docs.aleph-alpha.com/docs/Deprecated%20Luminous/Deprecated-Luminous/model-card/",,"""The Luminous family has been trained on a dataset compiled of sources in English, German, French, Spanish and Italian...""

more details in model card 
https://docs.aleph-alpha.com/docs/introduction/model-card/",1069300000000.0,"from the table

Total Size:
2.77 + 0.79 + 0.18 + 0.07 + 0.06 + 0.02 = 3.89 TB
Tokens:
761.41B + 217.15B + 49.47B + 19.29B + 16.49B + 5.49B = 1069.30B tokens",2016.0,Approximately 12 weeks = 2016 hours,"NVIDIA A100 SXM4 40 GB,NVIDIA A100 SXM4 80 GB",Confident,"The Luminous series is a family of large language models. Large language models are powerful technological tools that can process and produce text. These capabilities emerge during model “training” where the model is exposed to significant amounts of human text data. Similar to a person who deliberately absorbs information while reading a whole library and half of the internet, large language models acquire structural understanding (and not necessarily also knowledge) of language and accumulated information about the world.

The Luminous family currently consists of three vanilla models, which differ in complexity and ability. They are, from the smallest to the largest, luminous-base, luminous-extended and luminous-supreme. All Luminous models are trained in the five most commonly spoken European languages: English, German, French, Italian and Spanish.",Germany,,,,512.0,Unreleased,,Industry
Luminous-extended,Language,Language modeling/generation,,API access,https://docs.aleph-alpha.com/docs/Deprecated%20Luminous/Deprecated-Luminous/model-card/,,,2022-08-15,Aleph Alpha,30000000000.0,~30B (~42B with multi-modality),1.0019457e+23,"311840000000000*360000*3600*0.3 = 1.2124339e+23

6ND = 6*30*10^9*460000000000 = 8.28e+22

sqrt(8.28e+22*1.2124339e+23) = 1.0019457e+23",,"""""""The Luminous family has been trained on a dataset compiled of sources in English, German, French, Spanish and Italian...""""

more details in model card 
https://docs.aleph-alpha.com/docs/introduction/model-card/""",460000000000.0,"~460B tokens
230000 iterations",1344.0,~ 8 weeks = 56*24 = 1344 hours,NVIDIA A100 SXM4 40 GB,Confident,"Aleph Alpha luminous-extended is the second largest model which is faster and cheaper than Luminous-supreme. the model can perform information extraction, language simplification and has multi-capable image description capability. You can try Aleph Alpha models with predefined examples for free. Go to at the Jumpstart page on their site and click through the examples on Classification and Labelling, Generation, Information Extraction, Translation and Conversion and Multimodal. Aleph Alpha are based in Europe, which allows customers with sensitive data to process their information in compliance with European regulations for data protection and security on a sovereign, European computing infrastructure.",Germany,,,,512.0,Unreleased,,Industry
GLM-130B,Language,"Language modeling/generation,Translation","Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, Jie Tang",Open weights (non-commercial),https://keg.cs.tsinghua.edu.cn/glm-130b/posts/glm-130b/,989.0,GLM-130B: An Open Bilingual Pre-trained Model,2022-08-04,Tsinghua University,130000000000.0,Dense model,3.5490054945e+23,"""96 NVIDIA A100 (40G * 8) servers for 2 months""

312 TFLOPS/GPU * 96 servers * 8 GPU/server * 2 months * 32.5% utilization = 4.037e23

utilization rate - citation from the paper: ""we report hardware FLOPs utilization (HFU) of 43.3% and model FLOPs utilization (MFU) of 32.5% due to re-materialization.""

Aligns pretty well with 6ND:
6 * 400B * 130B = 3.12E23

Geometric mean: sqrt(4.037e23 * 3.12e23) = 3.549e23","The Pile,WuDao Corpora","""The pre-training data includes 1.2T Pile (train split) (Gao et al., 2020) English, 1.0T Chinese WudaoCorpora (Yuan et al., 2021), and 250G Chinese corpora (including online forums, encyclopedia, and
QA) we crawl from the web, which form a balanced composition of English and Chinese contents""",400000000000.0,"400B ""We completed the 400B-token training and evaluation of GLM-130B in July, and subsequently released the model and pre-training details in August 2022. ""  from https://arxiv.org/pdf/2406.12793

""As of July 3rd, 2022, GLM-130B has been trained on over 400 billion text tokens (200B each for Chinese and English)""",1440.0,"""During the 60-day access to the cluster, we manage to train GLM-130B for 400 billion tokens""
60 days * 24 = 1,440 hours",NVIDIA A100 SXM4 40 GB,Confident,"GLM-130B (ICLR 2023) is an open bilingual (English & Chinese) bidirectional dense model with 130 billion parameters, pre-trained using the General Language Model (GLM) algorithm1. It is designed to support inference tasks with the 130B parameters on a single A100 (40G * 8) or V100 (32G * 8) server. As of July 3rd, 2022, GLM-130B has been trained on over 400 billion text tokens (200B each for Chinese and English) ",China,,,,768.0,Unreleased,non commercial license. looks like inference but not training code: https://github.com/THUDM/GLM-130B/blob/main/MODEL_LICENSE,Academia
AlexaTM 20B,Language,"Language modeling,Translation,Question answering","Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, Chandana Satya Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv Verma, Gokhan Tur, Prem Natarajan",API access,https://arxiv.org/abs/2208.01448,73.0,AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model,2022-08-02,Amazon,19750000000.0,See Table 1 on p.3 of the paper,2.04374016e+23,"Training throughput is reported as 154 TFLOP/s - see p.5 of the paper.
""We relied on an internal and optimized version of DeepSpeed that we have since open-sourced (Chiu & Zheng, 2022) to obtain training throughput of up to 154 TFLOPS/GPU on 16 AWS p4d.24xlarge compute instances.""

Accelerator compute days are reported as 15,360 days - see Table 17 on p.18 of the paper.","mC4,Wikipedia",,1319000000000.0,"See Table 2 on p.3 of the paper.

119B Wikipedia tokens + 1.2T mC4 tokens = 1319000000000 tokens",2880.0,"See p.5 of the paper: ""We trained AlexaTM 20B for 120 days on 128 A100 GPUs...""",NVIDIA A100,Confident,"In this work, we demonstrate that multilingual large-scale sequence-to-sequence (seq2seq) models, pre-trained on a mixture of denoising and Causal Language Modeling (CLM) tasks, are more efficient few-shot learners than decoder-only models on various tasks. In particular, we train a 20 billion parameter multilingual seq2seq model called Alexa Teacher Model (AlexaTM 20B) and show that it achieves state-of-the-art (SOTA) performance on 1-shot summarization tasks, outperforming a much larger 540B PaLM decoder model. AlexaTM 20B also achieves SOTA in 1-shot machine translation, especially for low-resource languages, across almost all language pairs supported by the model (Arabic, English, French, German, Hindi, Italian, Japanese, Marathi, Portuguese, Spanish, Tamil, and Telugu) on Flores-101 dataset. We also show in zero-shot setting, AlexaTM 20B outperforms GPT3 (175B) on SuperGLUE and SQuADv2 datasets and provides SOTA performance on multilingual tasks such as XNLI, XCOPA, Paws-X, and XWinograd. Overall, our results present a compelling case for seq2seq models as a powerful alternative to decoder-only models for Large-scale Language Model (LLM) training. ",United States of America,,,,128.0,,https://aws.amazon.com/about-aws/whats-new/2022/11/alexatm-20b-model-available-sagemaker-jumpstart/?nc1=h_ls,Industry
YuYan 11B,Language,Language modeling/generation,"Gongzheng Li, Yadong Xi, Jingzhen Ding, Duan Wang, Ziyang Luo, Rongsheng Zhang, Bai Liu, Changjie Fan, Xiaoxi Mao, Zeng Zhao",,https://arxiv.org/abs/2104.12470,8.0,Easy and Efficient Transformer: Scalable Inference Solution For Large NLP Model,2022-07-15,"Hong Kong Baptist University,NetEase",11000000000.0,https://huggingface.co/FUXI/yuyan-11b,,,,,,,,,"NVIDIA A100 PCIe,NVIDIA GeForce RTX 2080 Ti 11GB",Confident,"Recently, large-scale transformer-based models have been proven to be effective over various tasks across many domains. Nevertheless, applying them in industrial production requires tedious and heavy works to reduce inference costs. To fill such a gap, we introduce a scalable inference solution: Easy and Efficient Transformer (EET), including a series of transformer inference optimization at the algorithm and implementation levels. First, we design highly optimized kernels for long inputs and large hidden sizes. Second, we propose a flexible CUDA memory manager to reduce the memory footprint when deploying a large model. Compared with the state-of-the-art transformer inference library (Faster Transformer v4.0), EET can achieve an average of 1.40-4.20x speedup on the transformer decoder layer with an A100 GPU","Hong Kong,China,China",,,,,,,"Academia,Industry"
BLOOM-176B,Language,"Language modeling,Translation,Code generation","Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",Open weights (restricted use),https://arxiv.org/abs/2211.05100,1984.0,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model,2022-07-11,"Hugging Face,BigScience",176247271424.0,"See ""Technical Specifications"" on Hugging Face:
https://huggingface.co/bigscience/bloom",3.65664e+23,"https://bigscience.huggingface.co/blog/bloom Blog post says 117 days.

384 A100 GPUs * 314 TFLOPS throughput per GPU * 117 days * 0.3 (utilization assumption) = 3.65664e23
https://www.wolframalpha.com/input?i=384+*+314+TFLOPS+*+117+days+*+0.3",BigScience ROOTS Corpus,"In total, 1.6 terabytes of pre-processed text was converted into 350 billion unique tokens as BLOOM's training datasets.
arXiv:2210.15424

""BLOOM was trained on the ROOTS corpus (Lauren¸con et al., 2022), a composite collection
of 498 Hugging Face datasets (Lhoest et al., 2021) amounting to 1.61 terabytes of text that
span 46 natural languages and 13 programming languages. A high-level overview of this
dataset can be seen in Figure 3, while a detailed itemized list of every language along with
its linguistic genus, family and macroarea is presented in Table 1""",379000000000.0,"Table 3.5 https://arxiv.org/pdf/2211.05100

366B (pretrain) + 13B (finetune) = 379B  tokens total ",2808.0,117 days * 24 hours/day,NVIDIA A100 SXM4 80 GB,Confident,"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.","Multinational,United States of America,Multinational,France",,,,384.0,Unreleased,responsible use restrictions: https://bigscience.huggingface.co/blog/the-bigscience-rail-license,"Industry,Research collective"
CodeWhisperer,Language,Code generation,,Hosted access (no API),https://aws.amazon.com/blogs/machine-learning/introducing-amazon-codewhisperer-the-ml-powered-coding-companion/,,"Introducing Amazon CodeWhisperer, the ML-powered coding companion",2022-06-24,Amazon,,,,,Unspecified unreleased,,,,,,,Unknown,"We are excited to announce Amazon CodeWhisperer, a machine learning (ML)-powered service that helps improve developer productivity by providing code recommendations based on developers’ natural comments and prior code. With CodeWhisperer, developers can simply write a comment that outlines a specific task in plain English, such as “upload a file to S3.” Based on this, CodeWhisperer automatically determines which cloud services and public libraries are best suited for the specified task, builds the specific code on the fly, and recommends the generated code snippets directly in the IDE.",United States of America,,,,,Unreleased,,Industry
YaLM,Language,"Language modeling,Chat","Mikhail Khrushchev, Ruslan Vasilev, Alexey Petrov, Nikolay Zinov",Open weights (unrestricted),https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6,,Yandex Publishes YaLM 100B. It’s the Largest GPT-Like Neural Network in Open Source,2022-06-23,Yandex,100000000000.0,100B,2.2e+23,"""It took us 65 days to train the model on a pool of 800 A100 graphics cards and 1.7 TB of online texts, books, and countless other sources.""","The Pile,YaLM Russian Dataset","""25% The Pile — open English dataset by Eleuther AI team

75% Texts in Russian collected by our team (percentages of the whole dataset are given)""

https://github.com/yandex/YaLM-100B?tab=readme-ov-file",300000000000.0,"1.7TB of data 300B tokens – from github https://github.com/yandex/YaLM-100B
I've assumed that 1 token correspond to 1 word in russian language.",1560.0,65 days,NVIDIA A100,Likely,,Russia,,,,800.0,Unreleased,"Apache 2.0 for weights.

training details, but no code: https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6 ",Industry
Parti,Image generation,"Text-to-image,Image generation","Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, Yonghui Wu",Unreleased,https://arxiv.org/abs/2206.10789v1,880.0,Scaling Autoregressive Models for Content-Rich Text-to-Image Generation,2022-06-22,Google Research,20000000000.0,"Abstract: ""we achieve consistent quality improvements
by scaling the encoder-decoder Transformer model up to 20B parameters""",3.962895376192635e+23,"Calculated from architecture. Does not take into account the encoding and decoding of text and images, only the transformer stack.

Table 1 shows for the 20B model
16 encoder layers
64 decoder layers
Dmodel = 4096
Dhidden = 16384
Num heads = 64

Just below table 1:
""We use a maximum length of text tokens of 128, and the length of image tokens are fixed to 1024""

I take the length of the sequence to be 100 for the encoder stack and 1024 for the decoder stack.

Section 3, Training: ""a total
of 450,000 steps and final ratio of 0.025. We use a global batch size of 8192 during training.""","LAION-400M,FIT400M,JFT-4B",,4800000000.0,,,,Google TPU v4,,"We present the Pathways Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P2), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See https://parti.research.google/ for high-resolution images.","Multinational,United States of America,Canada,Switzerland",,,,,Unreleased,"""For these reasons, we have decided not to release our Parti models, code, or data for public use without further safeguards in place""
https://sites.research.google/parti/",Industry
OPT-66B,Language,"Language modeling,Chat,Language modeling/generation,Question answering","Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",Open weights (non-commercial),https://arxiv.org/abs/2205.01068,2932.0,OPT: Open Pre-trained Transformer Language Models,2022-06-21,Meta AI,66000000000.0,,1.100000000001e+23,"OPT-66B was trained for 140k steps, using a batch size of 2M tokens (see the OPT baselines logbook and Table 1 in Zhang et al. (2022), respectively), so training took 140e3 ∗ 2e6 ∗ 66e9 ∗ 6 = 1.1e23 FLOP","The Pile,BookCorpus (BooksCorpus, Toronto Book Corpus),CC-Stories,Pushshift Reddit","C.2 Composition section:
– BookCorpus (Zhu et al., 2015) consists of more than 10K unpublished books
– CC-Stories (Trinh and Le, 2018) contains a subset of CommonCrawl data filtered to match the story-like style of Winograd schemas
– The Pile (Gao et al., 2021a) from which the following was included:
* Pile-CC
* OpenWebText2
* USPTO
* Project Gutenberg
* OpenSubtitles
* Wikipedia
* DM Mathematics
* HackerNews
– Pushshift.io Reddit dataset that was developed in Baumgartner et al. (2020) and processed in Roller et al. (2021).
– CCNewsV2 containing an updated version of the English portion of the CommonCrawl News dataset that was used in RoBERTa (Liu et al., 2019b)",180000000000.0,"""Our final corpus contains roughly 180B tokens.""",,,NVIDIA A100 SXM4 80 GB,Confident,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",United States of America,,,,,Open source,"non-commercial for weights:
https://ai.meta.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/

training code (MIT) https://github.com/facebookresearch/metaseq/blob/main/docs/training.md ",Industry
MetaLM,"Multimodal,Language,Vision","Language modeling,Visual question answering,Language modeling/generation","Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, Furu Wei",,https://arxiv.org/abs/2206.06336v1,90.0,Language Models are General-Purpose Interfaces,2022-06-13,Microsoft Research,,,,,The Pile,,,,,,,Unknown,"Foundation models have received much attention due to their effectiveness across a broad range of downstream applications. Though there is a big convergence in terms of architecture, most pretrained models are typically still developed for specific tasks or modalities. In this work, we propose to use language models as a general-purpose interface to various foundation models. A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer. We propose a semi-causal language modeling objective to jointly pretrain the interface and the modular encoders. We subsume the advantages and capabilities from both causal and non-causal modeling, thereby combining the best of two worlds. Specifically, the proposed method not only inherits the capabilities of in-context learning and open-ended generation from causal language modeling, but also is conducive to finetuning because of the bidirectional encoders. More importantly, our approach seamlessly unlocks the combinations of the above capabilities, e.g., enabling in-context learning or instruction following with finetuned encoders. Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.","United States of America,United Kingdom of Great Britain and Northern Ireland",,,,,,,Industry
EGRU (PTB),Language,Language modeling,"Anand Subramoney, Khaleelulla Khan Nazeer, Mark Schöne, Christian Mayr, David Kappel",Unreleased,https://arxiv.org/abs/2206.06178v3,13.0,Efficient recurrent architectures through activity sparsity and sparse back-propagation through time,2022-06-13,"Ruhr University Bochum,Technische Universität Dresden,University of London",55000000.0,"Table 3
",,,Penn TreeBank (PTB),,,,,,NVIDIA A100,Confident,"Recurrent neural networks (RNNs) are well suited for solving sequence tasks in resource-constrained systems due to their expressivity and low computational requirements. However, there is still a need to bridge the gap between what RNNs are capable of in terms of efficiency and performance and real-world application requirements. The memory and computational requirements arising from propagating the activations of all the neurons at every time step to every connected neuron, together with the sequential dependence of activations, contribute to the inefficiency of training and using RNNs. We propose a solution inspired by biological neuron dynamics that makes the communication between RNN units sparse and discrete. This makes the backward pass with backpropagation through time (BPTT) computationally sparse and efficient as well. We base our model on the gated recurrent unit (GRU), extending it with units that emit discrete events for communication triggered by a threshold so that no information is communicated to other units in the absence of events. We show theoretically that the communication between units, and hence the computation required for both the forward and backward passes, scales with the number of events in the network. Our model achieves efficiency without compromising task performance, demonstrating competitive performance compared to state-of-the-art recurrent network models in real-world tasks, including language modeling. The dynamic activity sparsity mechanism also makes our model well suited for novel energy-efficient neuromorphic hardware. Code is available at this https URL.","Germany,Germany,United Kingdom of Great Britain and Northern Ireland",,,,1.0,Open source,Apache 2.0: https://github.com/Efficient-Scalable-Machine-Learning/EvNN,"Academia,Academia,Academia"
BIG-G 137B,Language,Language modeling/generation,"Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito et al. (351 additional authors not shown)",Unreleased,https://arxiv.org/abs/2206.04615,1394.0,Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models,2022-06-09,Google,137000000000.0,"137B. Table App.1
",5.6e+23,"""BIG-G models were trained at Google. We use 13 dense decoder-only Transformer models (Vaswani
et al., 2017) with gated activation layers (Dauphin et al., 2017) and GELU activations based on the LaMDA
architectures (Thoppilan et al., 2022). These models were trained on a dataset consisting of a mixture of web
documents, code, dialog, and Wikipedia data, with approximately three billion documents tokenized to 2.8
trillion BPE tokens using a 32k-token SentencePiece vocabulary""

Appendix:

""We use a pre-training batch size of 262k tokens for all models...""

2.6M steps for 137B, per Table App.1. So trained on 2.6M * 262k = 681B tokens (~0.25 epochs)
681B * 137B * 6 = 5.6e23",GLaM dataset,"""These models were trained on a dataset consisting of a mixture of web
documents, code, dialog, and Wikipedia data, with approximately three billion documents tokenized to 2.8
trillion BPE tokens using a 32k-token SentencePiece vocabulary""",681000000000.0,"Full dataset is comprised of 2.8 trillion tokens, but calculation based on batch size and steps suggests model was trained on only 681 billion tokens.",,,,Confident,"Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit ""breakthrough"" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.",United States of America,,,,,Unreleased,,Industry
UL2,Language,Language modeling/generation,"Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler",Open weights (unrestricted),https://arxiv.org/abs/2205.05131v1,253.0,Unifying Language Learning Paradigms,2022-05-10,"Google Research,Google Brain",20000000000.0,Taken from Directory of LLMs,1.2e+23,"Trained on 1T tokens
20B * 1T * 6 = 1.2e23 

Second source: Section 5.1 says model was trained on 512 TPUv4 chips, and took slightly over 1 month
512 * 2.75e14 * 31 * 24 * 3600 * 0.3 = 1.13e23",C4,'The model is trained on a total of 1 trillion tokens on C4 (2 million steps).',1000000000000.0,1T tokens,744.0,"around 31 days from 'Pre-training took approximately slight more than one month for about 1 trillion
tokens.' from section 5.1
so around 31*24 = 744
",Google TPU v4,Confident,"Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. We release Flax-based T5X model checkpoints for the 20B model at \url{this https URL}.","Multinational,United States of America,Canada,Switzerland,United States of America",,,,512.0,,Apache 2.0,"Industry,Industry"
OPT-175B,Language,"Language modeling,Chat,Language modeling/generation,Question answering","Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",Open weights (non-commercial),https://arxiv.org/abs/2205.01068,2932.0,OPT: Open Pre-trained Transformer Language Models,2022-05-02,Meta AI,175000000000.0,"""In line with Meta AI’s commitment to open science, we are sharing Open Pretrained Transformer (OPT-175B), a language model with 175 billion parameters trained on publicly available data sets""",4.3e+23,"https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/final_update.md

""As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute""","The Pile,BookCorpus (BooksCorpus, Toronto Book Corpus),CC-Stories,Pushshift Reddit","""The pre-training corpus contains a concatenation
of datasets used in RoBERTa (Liu et al., 2019b),
the Pile (Gao et al., 2021a), and PushShift.io Reddit (Baumgartner et al., 2020; Roller et al., 2021)""
...
""RoBERTa We included the BookCorpus (Zhu et al., 2015) and Stories (Trinh and Le, 2018) subsets of the RoBERTa corpus and utilized an updated version of CCNews, containing news stories crawled through September 28, 2021. This CCNews v2 corpus was preprocessed the same way as the original RoBERTa CCNews (Liu et al., 2019b).

The Pile We included a subset of the Pile (Gao et al., 2021a), including: CommonCrawl, DM Mathematics, Project Gutenberg, HackerNews, OpenSubtitles, OpenWebText2, USPTO and Wikipedia. Other subsets of the Pile were eliminated
...

PushShift.io Reddit We included a subset of the Pushshift.io corpus produced by Baumgartner et al. (2020) and previously used by Roller et al. (2021). To convert the conversational trees
into language-model-accessible documents, we extracted the longest chain of comments in each thread and discarded all other paths in the tree. This reduced the corpus by about 66%.",180000000000.0,"""The training data contains 180B tokens corresponding to 800 GB of data""

1 token ~ 0.75 words",793.5,"4.3*10^23 FLOP / (147 TFLOPS) = 813000 A100-hours
https://www.wolframalpha.com/input?i=4.3*10%5E23+FLOP+%2F+%28147+TFLOPS%29

""As of yesterday, at 12:46pm PST on January 6, our 175B model finally completed its training run on 300B tokens. This required ~4.30E+23 FLOPs of compute, or roughly ~33 days of continuous training on 1024 80GB A100s (assuming no hardware issues, no numerical instabilities, etc.).""",NVIDIA A100 SXM4 80 GB,Confident,"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3,1 while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models",United States of America,,,,1024.0,Open source,"non-commercial for weights:
https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/MODEL_LICENSE.md

training code (MIT) https://github.com/facebookresearch/metaseq/blob/main/docs/training.md ",Industry
VLM-4,Language,"Language modeling/generation,Translation,Text summarization,Text classification",,API access,https://web.archive.org/web/20220506095925/https://lighton.ai/blog/lighton-publicly-launches-muse-an-api-to-vlm-4-large-language-models-trained-natively-in-five-european-languages/,,"LightOn publicly launches Muse, an API to VLM-4 Large Language Models trained natively in five European Languages",2022-04-12,LightOn,,,,,,,,,,,,Unverified,,France,,,,,Unreleased,,Industry
PaLM (540B),Language,"Language modeling,Code generation,Translation","Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev,, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta ,Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel",Unreleased,https://arxiv.org/abs/2204.02311,5064.0,PaLM: Scaling Language Modeling with Pathways,2022-04-04,Google Research,540350000000.0,"""To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM).""",2.5272e+24,"See Table 20.

6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.
Equivalent to 6144 TPUv4 for 1368 hours.

46.2% model FLOPs utilization

""The 540B-parameter PaLM model sustained a remarkable 57.8% of the peak hardware floating point performance over 50 days while training on TPU v4 supercomputers. "" https://cloud.google.com/blog/topics/systems/tpu-v4-enables-performance-energy-and-co2e-efficiency-gains","Wikipedia,GLaM dataset,LaMBDA dataset,GitHub",,585000000000.0,"""The PaLM pretraining dataset consists of a high-quality corpus of 780 billion tokens that represent a wide range of natural language use cases.""

1 token ~ 0.75 words",1536.0,"6144 TPUv4 for 1200 hours + 3072 TPUv4 for 336 hours.

Equivalent to 6144 TPUv4 for 1368 hours.",Google TPU v4,Confident,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.","Multinational,United States of America,Canada,Switzerland",,,,6144.0,Unreleased,,Industry
Chinchilla,Language,Language modeling,"Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre",Unreleased,https://arxiv.org/abs/2203.15556,1486.0,Training Compute-Optimal Large Language Models,2022-03-29,DeepMind,70000000000.0,"""We test this hypothesis by training a predicted compute-optimal model, \chinchilla, that uses the same compute budget as \gopher but with 70B parameters and 4× more more data. \chinchilla uniformly and significantly outperforms \Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks.""",5.76e+23,"""Both Chinchilla and Gopher have been trained for the same number of FLOPs but differ in the size of the model and the number of training tokens.""

We see the number of flops in table 3","MassiveWeb,C4","MassiveWeb, Books, C4, News, Github, Wikipedia (Table A1)",1050000000000.0,"Table 1 shows Chinchilla was training on 1.4 trillion tokens

1 token ~ 0.75 words",,,"Google TPU v4,Google TPU v3",Confident,"We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over \nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, \chinchilla, that uses the same compute budget as \gopher but with 70B parameters and 4× more more data. \chinchilla uniformly and significantly outperforms \Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that \chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, \chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over \gopher.",United Kingdom of Great Britain and Northern Ireland,,,,,Unreleased,,Industry
ST-MoE,Language,Language modeling/generation,"Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, William Fedus",Unreleased,https://arxiv.org/abs/2202.08906v2,117.0,ST-MoE: Designing Stable and Transferable Sparse Expert Models,2022-02-17,"Google,Google Brain,Google Research",269000000000.0,269B. it's called ST-MoE-32B because it's equivalent to a 32B dense model.,2.9e+23,"The paper claims ""scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder"". If this is true for training cost, then 6*32e9*1.5e12 = 2.9e23",C4,"""The pre-training dataset used to train our Sparse 32B model is a mix of C4 (Raffel et al., 2019) and the dataset introduced in GLaM (Du et al., 2021).""",1500000000000.0,"""We pre-train for 1.5T tokens on a mixture of English-only C4 dataset (Raffel et al., 2019) and the dataset from GLaM (Du et al., 2021) summarized in Appendix E""
",,,,Likely,"Scale has opened new frontiers in natural language processing -- but at a high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have been proposed as an energy efficient path to even larger and more capable language models. But advancing the state-of-the-art across a broad set of natural language tasks has been hindered by training instabilities and uncertain quality during fine-tuning. Our work focuses on these issues and acts as a design guide. We conclude by scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time, a sparse model achieves state-of-the-art performance in transfer learning, across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC Challenge), summarization (XSum, CNN-DM), closed book question answering (WebQA, Natural Questions), and adversarially constructed tasks (Winogrande, ANLI R3).","United States of America,United States of America,Multinational,United States of America,Canada,Switzerland",,,,,Open source,"Apache License 2.0
Code for our models is available at https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py","Industry,Industry,Industry"
Midjourney V1,Image generation,Image generation,,Hosted access (no API),,,,2022-02-15,Midjourney,,,,,Unspecified unreleased,,,,,,,Unknown,,United States of America,,,,,Unreleased,,Industry
LaMDA,Language,Language modeling,"Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, Quoc Le",Unreleased,https://arxiv.org/abs/2201.08239,1375.0,LaMDA: Language Models for Dialog Applications,2022-02-10,Google,137000000000.0,"""LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters""",3.55e+23,"""The total FLOPS is 56.5% * 123 TFLOPS/s * 1024 chips * 57.7 days
= 3.55E+23""
From https://arxiv.org/pdf/2201.08239.pdf p.18
",Infiniset,"LaMDA's underlying dataset is called 'Infiniset', and besides the dialogue also involves common crawl, wikipedia, a mixture of english and non-english web documents, and data from programming-related sites (so LaMDA models can also dabble in code).",1560000000000.0,"""and are pre-trained on 1.56T words of public dialog data and web text""",1385.0,57.7 days * 24,Google TPU v3,Confident,"We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.",United States of America,,,,1024.0,Unreleased,,Industry
AlphaCode,Language,Code generation,"Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, Oriol Vinyals",Unreleased,https://arxiv.org/abs/2203.07814,1013.0,Competition-Level Code Generation with AlphaCode,2022-02-02,DeepMind,41100000000.0,41.1B. Table 3,1.63944e+23,"Figure 7 (a) shows a maximum training compute budget of approx 23000 TPU-days per model.
23000 days * 24 h/day * 3600 sec/h * 2.75e14 FLOP/s * 0.3 utilization = 1.64e23 FLOP","CodeContests,Unspecified unreleased","Looks like evaluation data is released but not pretraining data:

""We use large transformer language models to generate code, pre-training them
on selected GitHub code and fine-tuning on our curated set of competitive programming problems...
A core part of developing our system was ensuring that submissions are rigorously evaluated and
that evaluation problems are truly unseen during training, so difficult problems cannot be solved
by copying from the training set. Towards this goal, we release a new training and evaluation
competitive programming dataset, CodeContests""",,Appendix part A has answers for pretraining.,147.2,"Figure 7 (a) shows that the models were trained for around 23000 TPU-days. We know they trained on TPUv4s, and in appendix D.1 they say they have 3750 TPUv4 and TPUv4i. Assuming they trained only on the 3750 TPUv4s, that suggests 23000 / 3750 = 6.13 days, or 147.2 hours.",Google TPU v4,,"Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and
accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete
simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into
code. For example, competitive programming problems which require an understanding of algorithms
and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper
reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform,
AlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance:
(1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and
efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the
search space, followed by filtering based on program behavior to a small set of submissions.
",United Kingdom of Great Britain and Northern Ireland,,,,3750.0,Unreleased,,Industry
ERNIE 3.0 Titan,Language,"Language modeling,Language modeling/generation","Shuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng",Hosted access (no API),https://arxiv.org/abs/2112.12731,70.0,ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation,2021-12-23,"Baidu,Peng Cheng Laboratory",260000000000.0,"""[We] developed... distributed training technology, including fine-grained parallelism, heterogeneous hardware-aware training, and fault tolerance mechanism to train the 260B model on both Nvidia V100 GPU and Ascend 910 NPU clusters.""
See also:
https://twitter.com/BaiduResearch/status/1468633977242243078?t=6q4zuLNdTSc4GUBe9OM5Aw&s=19",1.0421e+24,"The paper suggests that ERNIE 3.0 Titan uses more compute than GPT-3. This is consistent with the 6ND approximation.

C = 6ND = 6 (FLOP/param/token) * (260B params) * (668B tokens) = 1.0421*10^24 FLOP",ERNIE 3.0 Corpus,,668000000000.0,"""To ensure the success of the pre-training of ERNIE 3.0 Titan, we utilize the ERNIE 3.0 Corpus [ 2 ], a large-scale, wide-variety, and high-quality Chinese text corpora amounting to 4TB""

Assuming 167M words/tokens per GB",,,"NVIDIA Tesla V100 DGXS 32 GB,Huawei Ascend 910",Confident,"Pre-trained language models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. GPT-3 has shown that scaling up pre-trained language models can further exploit their enormous potential. A unified framework named ERNIE 3.0 was recently proposed for pre-training large-scale knowledge enhanced models and trained a model with 10 billion parameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP tasks. In order to explore the performance of scaling up ERNIE 3.0, we train a hundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion parameters on the PaddlePaddle platform. Furthermore, we design a self-supervised adversarial loss and a controllable language modeling loss to make ERNIE 3.0 Titan generate credible and controllable texts. To reduce the computation overhead and carbon emission, we propose an online distillation framework for ERNIE 3.0 Titan, where the teacher model will teach students and train itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense pre-trained model so far. Empirical results show that the ERNIE 3.0 Titan outperforms the state-of-the-art models on 68 NLP datasets.","China,China",,,,1920.0,Unreleased,"The Ernie 3.0 Titan model was used in Ernie bot. Today, ERNIE has been widely deployed across finance, healthcare, insurance, equity, Internet, logistics, and other fields.

http://research.baidu.com/Blog/index-view?id=165","Industry,Academia"
EXAONE 1.0,"Multimodal,Language,Vision","Translation,Language modeling/generation,Visual question answering",,Unreleased,"https://www.lgcorp.com/media/release/27387#:~:text=LG%20AI%20Research%20proposes%20EXAONE,performance%20while%20learning%20fewer%20parameters.",,,2021-12-14,LG,300000000000.0,,1.6956e+24,"No indication of how images are processed. Supposing they used something like ViT-H/14, and training images were 512x512 (they state ""EXAONE shows remarkable performance such as [...] offering 1024x1024 sized image output"", but typically this size of image training would only be done during a relatively short, final stage of pre-training), there would be 37x37 = 1,369 patches per image
1,369 * 250 million = around 342 billion image patch embeddings.

300M parameters * (342 billion + 600 billion) image tokens * 6 = 1.6956e24",Unspecified unreleased,"""To create multi-modal AI, LG AI Research Institute learned from 600 billion corpora, the world's largest, and more than 250 million high-resolution images combining language and images. It is also differentiated in that it is a bilingual AI that understands and speaks Korean and English at the level of a native speaker.""

600000000000+250000000=600250000000",600250000000.0,,,,,Speculative,"[Dec 2021]  EXAONE is a bilingual artificial intelligence that has learned the characteristics of both Korean and English languages at the same time. Since the initial development last June, it has completed learning of 1.3 billion, 13 billion, 39 billion, and 175 billion parameter models, and it is currently learning 300 billion parametric models. EXAONE shows remarkable performance such as obtaining the highest FID score, offering 1024x1024 sized image output, and achieving purpose conversation in the language domain as well as the highest level in the emotional classification domain. ",Korea (Republic of),,,,,,,Industry
GLaM,Language,"Language modeling/generation,Question answering","Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, Claire Cui",Unreleased,https://arxiv.org/abs/2112.06905,597.0,GLaM: Efficient Scaling of Language Models with Mixture-of-Experts,2021-12-13,Google,1200000000000.0,1.2 trillion parameters,3.6363112434e+23,"The network activates 96.6 billion parameters per token and trained for 600B tokens.

6 * 600B * 96.6B = 3.478e23

Digitizing figure 4 (d) indicates 139.67 TPU-years of training. 
2.75e14 * 139.67 * 365.25 * 24 * 3600 * 0.3 = 3.636e23

Since these are close, we will use the 6NC estimate and derive hardware utilization from the training time information.

Later they say they measured 326W power usage per chip, which could maybe be used to estimate utilization.","Wikipedia,GLaM dataset","""To train our model, we build a high-quality dataset of 1.6 trillion tokens that are representative of a wide range of natural language use cases. Web pages constitute the vast quantity of data in our unlabeled dataset. However, their quality ranges from professional writing to low-quality comment and forum pages.""",600000000000.0,"The dataset is made of 1.6 trillion tokens, but later in the paper they say they only train the largest model for 600b tokens. 600b / 0.75 words/token = 800b words.

""The complete GLaM training using 600B tokens consumes only 456 MWh and emits 40.2 net tCO2e.""",1366.0,"Note that they give several energy estimates. Use the complete training figures for 600B tokens, not the GPT-3 comparison values with 280B tokens.

""326W measured system power per TPU-v4 chip""
""The complete GLaM training using 600B tokens consumes only
456 MWh""
1024 TPU v4 chips
(456 MWh) / (326W/chip * 1024 chips) = 1366 hours",Google TPU v4,Confident,"Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.",United States of America,,,,1024.0,Unreleased,,Industry
Gopher (280B),Language,"Language modeling,Question answering","Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, Geoffrey Irving",Unreleased,https://arxiv.org/abs/2112.11446,1122.0,"""Scaling Language Models: Methods, Analysis & Insights from Training Gopher""",2021-12-08,DeepMind,280000000000.0,"Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.",6.31e+23,"Table A26
6.31E+08 Train PFLOPs",MassiveTex,,300000000000.0,"""We train all models for 300 billion tokens with a 2048 token context window, using the Adam (Kingma and Ba, 2014) optimiser.""

1 token ~ 0.75 words",920.0,"""We trained Gopher for 920 hours in November and December 2020 in Google’s Georgia datacentre. The PUE of the datacenter at this time was 1.08; the net tCO2e per MWh in October 2020 was 0.33. Using an estimate of 283W drawn per chip, this leads to a total of 380 net tCO2e""",Google TPU v3,Confident,"We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25× fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",United Kingdom of Great Britain and Northern Ireland,,,,4096.0,Unreleased,,Industry
Yuan 1.0,Language,Language modeling,"Shaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang, Chong Shen, Hongli Liu, Feng Li, Hong Zhu, Jiangang Luo, Liang Xu, Xuanwei Zhang, Jun Liu",API access,https://arxiv.org/abs/2110.04725,51.0,Yuan 1.0: Large-Scale Pre-trained Language Model in Zero-Shot and Few-Shot Learning,2021-10-12,Inspur,245730000000.0,"Table 2: Parameters of Yuan models.
""Parameters (billion)""",3.5380000000001e+23,"Table 9: 4095 petaFLOPS-days which equals 3.538*10^23 FLOP

https://www.wolframalpha.com/input?i=4095+petaFLOPS+*+1+day
","Common Crawl,Wikipedia,Sogue News","""A Chinese corpus with 5TB high-quality text is built, which is sufficient to train Yuan 245B model without sampling the dataset twice.""

In order to obtain the high-quality dataset, we develop a Massive Data Filtering System (MDFS) built on Spark to clean and filter the raw data, and train a Bert-based model to select high quality samples. MDFS is consisted of three parts, data collection, coarse filtering and fine filtering (Fig. 5). The raw data is collected from Common Crawl, Sogou News, SogouT, Encyclopedia, and Books (Table 3). To process these raw data, we run MDFS system on a high performance cluster with 36 nodes.",1000000000000.0,"""Yuan 1.0 was trained on a new Chinese dataset of 5TB high-quality text that was built on 850TB raw data from Internet.""

1 GB ~ 167M words in English or 333M words in Chinese. For a mixed dataset of mostly Chinese, 5TB may be equivalent to around 1T words.

Table 2: 180B training tokens",,,,Confident,"Recent work like GPT-3 has demonstrated excellent performance of Zero-Shot and Few-Shot learning on many natural language processing (NLP) tasks by scaling up model size, dataset size and the amount of computation. However, training a model like GPT-3 requires huge amount of computational resources which makes it challengeable to researchers. In this work, we propose a method that incorporates large-scale distributed training performance into model architecture design. With this method, Yuan 1.0, the current largest singleton language model with 245B parameters, achieves excellent performance on thousands GPUs during training, and the state-of-the-art results on NLP tasks. A data processing method is designed to efficiently filter massive amount of raw data. The current largest high-quality Chinese corpus with 5TB high quality texts is built based on this method. In addition, a calibration and label expansion method is proposed to improve the Zero-Shot and Few-Shot performance, and steady improvement is observed on the accuracy of various tasks. Yuan 1.0 presents strong capacity of natural language generation, and the generated articles are difficult to distinguish from the human-written ones.",China,,,,2128.0,Unreleased,https://github.com/Shawn-IEITSystems/Yuan-1.0,Industry
Megatron-Turing NLG 530B,Language,Language modeling,"Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro",Unreleased,https://arxiv.org/abs/2201.11990,657.0,"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model",2021-10-11,"Microsoft,NVIDIA",530000000000.0,,1.17e+24,"https://www.lesswrong.com/posts/bGuMrzhJdENCo8BxX/nvidia-and-microsoft-releases-530b-parameter-transformer?commentId=HSJSNspKp94tFcSCx

source: https://lair.lighton.ai/akronomicon/
9938 PF-days * 3600 * 24 * 10^15  = 8.586432e+23","Common Crawl,The Pile,CC-Stories,Realnews"," In addition to Common Crawl data, we leveraged a number of other previously generated datasets. From The Pile, we selected Books3, OpenWebText2, Stack Exchange, PubMed Abstracts,
Wikipedia, Gutenberg (PG-19), BookCorpus2, NIH ExPorter, and Pile-CC datasets. We also included the
CC-Stories and RealNews datasets used to train Megatron",270000000000.0,"""Our training dataset consists of 339 billion tokens and we
trained MT-NLG on 270 billions tokens by blending the 15 training datasets as described above. We also set aside 2% of our data for validation.""

1 token ~ 0.75 words",770.0,"Total compute was 1.17*10^24 FLOP.
They don't directly report the utilization and training speed when using the full Selene supercomputer with 560 DGX * 8 A100/DGX = 4480 GPUs. See section 2.3 Hardware Setup.

At 280 DGX, the utilization is 126/312 = 40% and a batch takes 60 seconds; at 350, it is 39% for 50 seconds; at 420, it is 36% for 44 seconds.

The overall utilization was 30.2% and the full cluster has 560 DGX. Dividing the total compute by the total performance of 4480 A100 at 30.2% utilization gives 770 hours.",NVIDIA A100 SXM4 80 GB,,"Pretrained general-purpose language models can achieve state-of-the-art accuracies in various natural language processing domains by adapting to downstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of their success, the size of these models has increased rapidly, requiring high-performance hardware, software, and algorithmic techniques to enable training such large models. As the result of a joint effort between Microsoft and NVIDIA, we present details on the training of the largest monolithic transformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530 billion parameters. In this paper, we first focus on the infrastructure as well as the 3D parallelism methodology used to train this model using DeepSpeed and Megatron. Next, we detail the training process, the design of our training corpus, and our data curation techniques, which we believe is a key ingredient to the success of the model. Finally, we discuss various evaluation results, as well as other interesting observations and new properties exhibited by MT-NLG. We demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning accuracies on several NLP benchmarks and establishes new state-of-the-art results. We believe that our contributions will help further the development of large-scale training infrastructures, large-scale language models, and natural language generations.","United States of America,United States of America",,,,4480.0,Unreleased,,"Industry,Industry"
HyperCLOVA 82B,Language,"Language modeling/generation,Chat,Translation,Text classification","Boseop Kim, HyoungSeok Kim, Sang-Woo Lee, Gichang Lee, Donghyun Kwak, Dong Hyeon Jeon, Sunghyun Park, Sungju Kim, Seonhoon Kim, Dongpil Seo, Heungsub Lee, Minyoung Jeong, Sungjae Lee, Minsub Kim, Suk Hyun Ko, Seokhun Kim, Taeyong Park, Jinuk Kim, Soyoung Kang, Na-Hyeon Ryu, Kang Min Yoo, Minsuk Chang, Soobin Suh, Sookyo In, Jinseong Park, Kyungduk Kim, Hiun Kim, Jisu Jeong, Yong Goo Yeo, Donghoon Ham, Dongju Park, Min Young Lee, Jaewook Kang, Inho Kang, Jung-Woo Ha, Woomyoung Park, Nako Sung",API access,https://arxiv.org/abs/2109.04650,92.0,What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers,2021-09-10,"NAVER,Search Solutions",82000000000.0,"""We introduce a Korean in-context large-scale LM with 82B parameters, i.e., HyperCLOVA. This is the first discovery on near
100B-scale non-English LM.""

According to media reports, HyperCLOVA has 204B parameters (i.e. a different version than in the paper)
https://m.koreaherald.com/view.php?ud=20210525000824 ",1.476e+23,"""For experiments in Section 4, the model trained with 150B is used for fair comparison, because not all models are finished training at the same iteration. However, experiments in Section 5.2 use the model trained with 300B tokens, as HyperCLOVA Studio provided the 39B and 82B models trained with 300B tokens.""

82e9 connections * 2 FLOP/connection * 300e9 tokens * 3 backward pass = 1.476e23 FLOP

Calculation using GPU time corroborates this:
- ""Our model is based on megatron-LM (Shoeybi et al., 2019) and trained on the NVIDIA Superpod, which includes 128 strongly clustered DGX servers with 1,024 A100 GPUs.""
- ""It takes 13.4 days to train a model with 82B parameters with 150B tokens."" Assume 300B tokens takes twice as long, 26.8 days.
- Assume the default of 30% utilization rate for large language models.

1024 A100 GPUs * 312e12 FLOP/second * 0.3 utilization * 26.8 days * 24 * 60 * 60 seconds/day = 2.219e+23 FLOP",Unspecified unreleased,"Blog corpus: 273.6 billion tokens
Cafe corpus (online community): 83.3 billion tokens
News corpus: 73.8 billion tokens
Comments (crawled from various platforms): 41.1 billion tokens
KiN (Korean QnA website): 27.3 billion tokens
Modu (collection of five datasets): 6.0 billion tokens
WikiEn, WikiJp (Foreign Wikipedia): 5.2 billion tokens
Other unspecified sources: 51.5 billion tokens",300000000000.0,"""However, experiments in Section 5.2 use the model trained with 300B tokens, as HyperCLOVA Studio provided the 39B and 82B models trained with 300B tokens.""

""We introduce HyperCLOVA, a large-scale
Korean in-context learning-based LM with
nearly 100B parameters, by constructing a
large Korean-centric corpus of 560B tokens.""

Based on tokenizing the Hyperclova article itself using OpenAI's tiktoken BPE tokenizer (https://github.com/openai/tiktoken), there are 3285 tokens for 1069 words - about 3 tokens per word.

This work uses a special tokenizer, but based on Figure 5 in Appendix E, the number of tokens seems similar between different tokenization methods.

Based on that, 5.6e11 Korean tokens ~= 1.9e11 words",643.2,see compute notes,NVIDIA A100,Confident,"GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications.","Korea (Republic of),Korea (Republic of)",,,,1024.0,Unreleased,"""We introduce HyperCLOVA Studio, an interactive prompt engineering interface which provides GUI and API interfaces like the OpenAI
playground1""","Industry,Industry"
HyperCLOVA 204B,Language,Language modeling/generation,,,,92.0,,2021-09-10,NAVER,204000000000.0,https://www.navercorp.com/navercorp_/ir/announce/2023/NAVER_CEO%20letter%20to%20shareholders_Aug%202023_Eng.pdf,,"Estimations for 82B model (marked as lower bound estimations)

""For experiments in Section 4, the model trained with 150B is used for fair comparison, because not all models are finished training at the same iteration. However, experiments in Section 5.2 use the model trained with 300B tokens, as HyperCLOVA Studio provided the 39B and 82B models trained with 300B tokens.""

82e9 connections * 2 FLOP/connection * 300e9 tokens * 3 backward pass = 1.476e23 FLOP

Calculation using GPU time corroborates this:
- ""Our model is based on megatron-LM (Shoeybi et al., 2019) and trained on the NVIDIA Superpod, which includes 128 strongly clustered DGX servers with 1,024 A100 GPUs.""
- ""It takes 13.4 days to train a model with 82B parameters with 150B tokens."" Assume 300B tokens takes twice as long, 26.8 days.
- Assume the default of 30% utilization rate for large language models.

1024 A100 GPUs * 312e12 FLOP/second * 0.3 utilization * 26.8 days * 24 * 60 * 60 seconds/day = 2.219e+23 FLOP",Unspecified unreleased,,560000000000.0,"https://twitter.com/arankomatsuzaki/status/1397583304610783238
https://venturebeat.com/ai/naver-trained-a-gpt-3-like-korean-language-model/",,,NVIDIA A100,Speculative,,Korea (Republic of),,,,,Unreleased,,Industry
Jurassic-1-Jumbo,Language,"Language modeling/generation,Chat","Opher Lieber, Or Sharir, Barak Lenz, Yoav Shoham",API access,https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf,55.0,Jurassic-1: Technical Details and Evaluation,2021-08-11,AI21 Labs,178000000000.0,"""Jurassic-1 models come in two sizes, where the Jumbo version, at 178B parameters, is the largest and most sophisticated language model ever released for general use by developers.""",3.7e+23,see here https://docs.google.com/document/d/1B8x6XYcmB1u6Tmq3VcbAtj5bzhDaj2TcIPyK6Wpupx4/edit,,,225000000000.0,"""Our model was trained with the conventional self-supervised auto-regressive training objective on 300B tokens drawn from publicly available resources""

1 token ~ 0.75 words",,,NVIDIA A100,,"Jurassic-1 is a pair of auto-regressive language models recently released by AI21 Labs, consisting of J1-Jumbo, a 178B-parameter model, and J1-Large, a 7B-parameter model. We describe their architecture and training, and evaluate their performance relative to GPT-3. The evaluation is in terms of perplexity, as well as zero-shot and few-shot learning. To that end, we developed a zeroshot and few-shot test suite, which we made publicly available (https://github.com/ai21labs/ lm-evaluation) as a shared resource for the evaluation of mega language models.",Israel,,,,,Unreleased,,Industry
GPT-3 175B (davinci),Language,"Text autocompletion,Language modeling/generation","Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",API access,https://arxiv.org/abs/2005.14165,32643.0,Language Models are Few-Shot Learners,2020-05-28,OpenAI,175000000000.0,"""we train GPT-3, an autoregressive language model with 175 billion parameters""",3.14e+23,"Table D.1
https://arxiv.org/abs/2005.14165","Common Crawl,WebText2,Wikipedia,Books1,Books2",Table 2.2 (other datasets also used),374000000000.0,"From table 2.2, we determine that there are 410 + 19 + 12 + 55 + 3 = 499 billion tokens. 

We multiply this by 0.75 to give 374B words. 

3.74e11

========================
[Anson: I think the calculation below doesn't look at all the data, the CommonCrawl data only constitutes 60% of the data. Multiplying by 5/3 gives 4.75e11]

""The CommonCrawl data was downloaded from 41 shards of monthly CommonCrawl covering 2016 to 2019, constituting 45TB of compressed plaintext before filtering and 570GB after filtering, roughly equivalent to 400 billion byte-pair-encoded tokens. ""

Converted to words using 
http://extraconversion.com/data-storage/gigabits/gigabits-to-words.html

2.85e11",355.2,14.8 days according to https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf,NVIDIA Tesla V100 DGXS 32 GB,Confident,"Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",United States of America,,,,10000.0,Unreleased,"https://openai.com/blog/openai-api
",Industry
Meena,Language,Text autocompletion,"Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",Unreleased,https://arxiv.org/abs/2001.09977,879.0,Towards a Human-like Open-Domain Chatbot,2020-01-28,Google Brain,2600000000.0,"""We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token.""",1.12e+23,"https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf
Table 4

In the paper: ""We trained our best model for 30 days on a TPUv3 Pod (2,048 TPU cores) on the Meena dataset containing 40B words (or 61B BPE tokens) [...] by the end of training, the model had traversed the full
training set 164 times (or epochs) and observed a total of about 10T tokens""

Hardware: 30 * 24 * 3600 * (2048/2) * 1.23e14 * 0.3 = 9.794e22
Ops counting: 6 * 10T * 2.6B = 1.56E23
Geometric mean: sqrt(9.79e22*1.56E23) = 1.24e23, very close to the figure in the link above.",,,40000000000.0,"""The final Meena dataset contains 341GB of text
(40B words)""

Converting from GB to words yields 6.8e10, which is in the same OOM",720.0,"We trained our best model for 30 days on a TPUv3 Pod (2,048 TPU cores)",Google TPU v3,Confident,"We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated.",United States of America,,,,1024.0,Unreleased,,Industry
AlphaGo Master,Games,Go,"D Silver, J Schrittwieser, K Simonyan, I Antonoglou",Unreleased,https://www.nature.com/articles/nature24270,8795.0,Mastering the game of Go without human knowledge,2017-10-19,DeepMind,,,2.00010000000001e+23,"This is a guess. There was no single journal publication that accompanied this model, that gave information about architecture/model training time etc. All I could find was that it has the same architecture as AlphaGo Zero, and that it had roughly the same power consumption as AGZ. See for instance: 
https://deepmind.com/blog/article/alphago-zero-starting-scratch

Since AGZ reaches the ELO of AlphaGo Master in about 25-30 days (60-75% of the total training time), I estimate the compute to be around 60-75% that of AGZ. I round this to 2e23, and I expect this to only be accurate within an OOM.",,,,,72.0,"""Training started from completely random behaviour and continued without human intervention for approximately three days.""",Google TPU v1,,"A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.",United Kingdom of Great Britain and Northern Ireland,,,,,Unreleased,,Industry
AlphaGo Zero,Games,Go,"D Silver, J Schrittwieser, K Simonyan, I Antonoglou",Unreleased,https://www.nature.com/articles/nature24270,8795.0,Mastering the game of Go without human knowledge,2017-10-18,DeepMind,46400244.0,Quick calculation,3.41e+23,"source: https://docs.google.com/spreadsheets/d/1Kj4Q5WADcDXtUJLIOfGTCE3tGvxNczEMwyy8QtgSkHk/edit#gid=54587040&fvid=1361937389


AGZ had two models, one of which was small and another of which was large. The compute for AGZ is for the large model, which has 40 residual blocks instead of 20.

A second way of looking at this... we believe multiple TPUs were used for training. 29 million games * 211 moves per game on average * 0.8 seconds per move = 4.8952E+09 seconds of player-time across all TPUs.

4.8952E+09 seconds of player-time / (40 days * 24 * 60 * 60 seconds of real time) ~= 1,416 players

4 TPUs per player => 4.8952E+09 * 4 = 1.95808E+10 TPU-seconds
Total compute = 1.95808E+10 TPU-seconds * 92E+12 FLOP/(TPU-second) * 0.4 = 7.2e23 FLOP

So similar to the Cotra and Davidson estimate (within a factor of 2).",,,5800000000.0,"""Over the course of training, 29 million games of self-play were generated""

Approx 200 moves per Go game on average

https://homepages.cwi.nl/~aeb/go/misc/gostat.html

Thus 200 * 29e6 = 5.8e9",480.0,,Google TPU v1,,"A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.",United Kingdom of Great Britain and Northern Ireland,,,,,Unreleased,,Industry
